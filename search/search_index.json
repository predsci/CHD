{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Coronal Hole Detection Project For Python files visit the CHD Git repository . Project Outline Data Collection Image Pre-Processing: PSF Deconvolution Limb-Brightening Correction Inter-Instrument Transformation Coronal Hole Detection Mapping Project Pipeline Some sort of graphic here...","title":"Home"},{"location":"#welcome-to-the-coronal-hole-detection-project","text":"For Python files visit the CHD Git repository .","title":"Welcome to the Coronal Hole Detection Project"},{"location":"#project-outline","text":"Data Collection Image Pre-Processing: PSF Deconvolution Limb-Brightening Correction Inter-Instrument Transformation Coronal Hole Detection Mapping","title":"Project Outline"},{"location":"#project-pipeline","text":"Some sort of graphic here...","title":"Project Pipeline"},{"location":"about/","text":"About the Project Contact Cooper Downs: cdowns@predsci.com James Turtle: jturtle@predsci.com Jon Linker: linkerj@predsci.com Tamar Ervin (Intern): tamar@predsci.com Opal Issan (Intern): oissan@predsci.com","title":"About the Project"},{"location":"about/#about-the-project","text":"","title":"About the Project"},{"location":"about/#contact","text":"Cooper Downs: cdowns@predsci.com James Turtle: jturtle@predsci.com Jon Linker: linkerj@predsci.com Tamar Ervin (Intern): tamar@predsci.com Opal Issan (Intern): oissan@predsci.com","title":"Contact"},{"location":"chd/chd/","text":"Coronal Hole Detection Coronal Hole Detection is carried out using a two-threshold region growing algorithm. Algorithm 1 2 3 4 5 6 7 8 9 10 11 12 13 \"\"\" python wrapper function for fortran algorithm @param image_data: EUV Image data for detection @param use_chd: matrix of size (nx,ny) which contains 1's where there is valid image data, and non-zero values for areas with invalid/no IMG data. @param nx, ny: image dimensions @param t1, t2: threshold values @param nc: pixel connectivity parameter - number of consecutive pixels needed for connectivity @param iters: maximum number of iterations allowed @return ezseg_output: segmentation map where 0 marks a detection @return iters_used: number of iterations preformed \"\"\" ezseg_output , iters_used = ezsegwrapper . ezseg ( np . log10 ( image_data ), use_chd , nx , ny , t1 , t2 , nc , iters ) 1.) viable pixels are checked to see if the intensity level is below threshold 1 if so, pixel is marked 2.) in each iteration, pixels are checked if their intensity is between threshold 1 and 2, and if they have the required number of connected pixels if so, pixel is marked 3.) continues until no more pixels are marked Example Maps Example minimum intensity merge maps with and without Coronal Hole Detection overlaid. You can click on image titles to enlarge images. Maps with both methods of minimum intensity merge are shown. Minimum Intensity Merge with Two Threshold Mu Cutoff Values April 11, 2011 EUV Map Combined EUV/CHD Map May 15, 2011 EUV Map Combined EUV/CHD Map July 21, 2011 EUV Map Combined EUV/CHD Map August 18, 2011 EUV Map Combined EUV/CHD Map October 1, 2011 EUV Map Combined EUV/CHD Map Minimum Intensity Merge based on Maximum Mu Value April 11, 2011 EUV Map Combined EUV/CHD Map May 15, 2011 EUV Map Combined EUV/CHD Map July 21, 2011 EUV Map Combined EUV/CHD Map August 18, 2011 EUV Map Combined EUV/CHD Map October 1, 2011 EUV Map Combined EUV/CHD Map","title":"CH Detection"},{"location":"chd/chd/#coronal-hole-detection","text":"Coronal Hole Detection is carried out using a two-threshold region growing algorithm.","title":"Coronal Hole Detection"},{"location":"chd/chd/#algorithm","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 \"\"\" python wrapper function for fortran algorithm @param image_data: EUV Image data for detection @param use_chd: matrix of size (nx,ny) which contains 1's where there is valid image data, and non-zero values for areas with invalid/no IMG data. @param nx, ny: image dimensions @param t1, t2: threshold values @param nc: pixel connectivity parameter - number of consecutive pixels needed for connectivity @param iters: maximum number of iterations allowed @return ezseg_output: segmentation map where 0 marks a detection @return iters_used: number of iterations preformed \"\"\" ezseg_output , iters_used = ezsegwrapper . ezseg ( np . log10 ( image_data ), use_chd , nx , ny , t1 , t2 , nc , iters ) 1.) viable pixels are checked to see if the intensity level is below threshold 1 if so, pixel is marked 2.) in each iteration, pixels are checked if their intensity is between threshold 1 and 2, and if they have the required number of connected pixels if so, pixel is marked 3.) continues until no more pixels are marked","title":"Algorithm"},{"location":"chd/chd/#example-maps","text":"Example minimum intensity merge maps with and without Coronal Hole Detection overlaid. You can click on image titles to enlarge images. Maps with both methods of minimum intensity merge are shown.","title":"Example Maps"},{"location":"chd/chd/#minimum-intensity-merge-with-two-threshold-mu-cutoff-values","text":"April 11, 2011 EUV Map Combined EUV/CHD Map May 15, 2011 EUV Map Combined EUV/CHD Map July 21, 2011 EUV Map Combined EUV/CHD Map August 18, 2011 EUV Map Combined EUV/CHD Map October 1, 2011 EUV Map Combined EUV/CHD Map","title":"Minimum Intensity Merge with Two Threshold Mu Cutoff Values"},{"location":"chd/chd/#minimum-intensity-merge-based-on-maximum-mu-value","text":"April 11, 2011 EUV Map Combined EUV/CHD Map May 15, 2011 EUV Map Combined EUV/CHD Map July 21, 2011 EUV Map Combined EUV/CHD Map August 18, 2011 EUV Map Combined EUV/CHD Map October 1, 2011 EUV Map Combined EUV/CHD Map","title":"Minimum Intensity Merge based on Maximum Mu Value"},{"location":"chd/f2py/","text":"Creating Wrapper Function for Ezseg Algorithm How to create a wrapper function for Fortran code: more information can be found here 1.) python -m numpy.f2py ezseg.f -m ezsegwrapper -h ezseg.pyf creates ezseg.pyf file with ezsegwrapper function 2.) cp ezseg.pyf ezsegwrapper.pyf copy file and update to be python compatible 3.) python -m numpy.f2py -c ezsegwrapper.pyf ezseg.f creates shared module ezsegwrapper.so","title":"Fortran to Python"},{"location":"chd/f2py/#creating-wrapper-function-for-ezseg-algorithm","text":"How to create a wrapper function for Fortran code: more information can be found here 1.) python -m numpy.f2py ezseg.f -m ezsegwrapper -h ezseg.pyf creates ezseg.pyf file with ezsegwrapper function 2.) cp ezseg.pyf ezsegwrapper.pyf copy file and update to be python compatible 3.) python -m numpy.f2py -c ezsegwrapper.pyf ezseg.f creates shared module ezsegwrapper.so","title":"Creating Wrapper Function for Ezseg Algorithm"},{"location":"db/db/","text":"Database Information The Image Pre-Processing Pipeline is built upon a database to store images, histograms, and fit parameter values. Updating the Database The original database is now quite different than the database needed to query and save calculated parameters. In order to generate the necessary updates to run the code, do the following: 1.) install the python package Alembic in your python environment conda install -c conda-forge alembic additional installation information can be found here 2.) in the (CHD) project folder, run the script to update the database alembic upgrade head this will run the latest updates to the database 3.) to return to the original database, run the downgrade script alembic downgrade base this will return the database to it's original form 4.) to only run certain upgrades/downgrades run a specific number of revisions (n is the number of revisions to either upgrade/downgrade) alembic upgrade +n alembic downgrade -n run a specific upgrade/downgrade: scripts are found here alembic upgrade \"revision\" alembic downgrade \"revision\" \"revision\" refers to the first 3+ identifying characters from the specific script 5.) to create new update scripts alembic revision -m \"revision name\" edit this script with the upgrade/downgrade necessary Database Schema","title":"Database Basics"},{"location":"db/db/#database-information","text":"The Image Pre-Processing Pipeline is built upon a database to store images, histograms, and fit parameter values.","title":"Database Information"},{"location":"db/db/#updating-the-database","text":"The original database is now quite different than the database needed to query and save calculated parameters. In order to generate the necessary updates to run the code, do the following: 1.) install the python package Alembic in your python environment conda install -c conda-forge alembic additional installation information can be found here 2.) in the (CHD) project folder, run the script to update the database alembic upgrade head this will run the latest updates to the database 3.) to return to the original database, run the downgrade script alembic downgrade base this will return the database to it's original form 4.) to only run certain upgrades/downgrades run a specific number of revisions (n is the number of revisions to either upgrade/downgrade) alembic upgrade +n alembic downgrade -n run a specific upgrade/downgrade: scripts are found here alembic upgrade \"revision\" alembic downgrade \"revision\" \"revision\" refers to the first 3+ identifying characters from the specific script 5.) to create new update scripts alembic revision -m \"revision name\" edit this script with the upgrade/downgrade necessary","title":"Updating the Database"},{"location":"db/db/#database-schema","text":"","title":"Database Schema"},{"location":"db/iit/","text":"Database for Inter-Instrument Transformation For the Inter-Instrument Transformation, the database is used to query EUV Images and LBC Fit Parameters to apply the correction. 1D Intensity Histograms are created and stored in the database. After calculation, fit parameters are stored in the database then queried to apply the IIT Correction. Tables Histogram This table stores histogram and information associated with IIT Histograms. The histograms are created in Step One of Inter-Instrument Transformation Correction. Columns: hist_id: auto-incremented integer id associated with the histogram (Primary Key, Integer) image_id: integer id associated with image (Foreign Key: EUV Images, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) date_obs: time of image observation (DateTime) wavelength: observation wavelength (Integer) n_mu_bins: number of mu bins (Integer) n_intensity_bins: number of intensity bins (Integer) lat_band: latitude band (Blob) mu_bin_edges: array of mu bin edges from number of mu bins (Blob) intensity_bin_edges: array of intensity bin edges from number of intensity bins (Blob) hist: histogram associated with image (Blob) Image Combos This table stores information regarding the combination of images used to calculate the fit parameter. Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) n_images: number of images in combination (Integer) date_mean: mean date of images in image combination (DateTime) date_max: maximum date of images in image combination (DateTime) date_min: minimum date of images in image combination (DateTime) Image Combo Assoc This table stores specific image ids with the associated combo id. Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Foreign Key: Image Combos, Integer) image_id: integer id associated with image (Primary Key, Foreign Key: EUV Images, Integer) Method Defs This table stores information about a correction method and an associated integer method id. Columns: meth_id: auto-incremented integer id associated with the specific method (Primary Key, Integer) meth_name: method name (String) meth_description: description of method (String) Var Defs This table stores information about a variable and an associated integer variable id. Columns: var_id: auto-incremented integer id associated with the specific variable (Primary Key, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) var_name: variable name (String) var_description: description of variable (String) Var Vals This table stores variable values with the associated variable, method, and image combination. These values are calculated from the IIT fit analysis ( IIT Step Two ). These values are queried during the application of the correction ( IIT Step Three ) and during the creation of histogram plots ( IIT Step Four ). Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Foreign Key: Image Combos, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) var_id: auto-incremented integer id associated with the specific variable (Primary Key, Foreign Key: Var Defs, Integer) var_val: variable value (Float)","title":"Database for IIT"},{"location":"db/iit/#database-for-inter-instrument-transformation","text":"For the Inter-Instrument Transformation, the database is used to query EUV Images and LBC Fit Parameters to apply the correction. 1D Intensity Histograms are created and stored in the database. After calculation, fit parameters are stored in the database then queried to apply the IIT Correction.","title":"Database for Inter-Instrument Transformation"},{"location":"db/iit/#tables","text":"","title":"Tables"},{"location":"db/iit/#histogram","text":"This table stores histogram and information associated with IIT Histograms. The histograms are created in Step One of Inter-Instrument Transformation Correction. Columns: hist_id: auto-incremented integer id associated with the histogram (Primary Key, Integer) image_id: integer id associated with image (Foreign Key: EUV Images, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) date_obs: time of image observation (DateTime) wavelength: observation wavelength (Integer) n_mu_bins: number of mu bins (Integer) n_intensity_bins: number of intensity bins (Integer) lat_band: latitude band (Blob) mu_bin_edges: array of mu bin edges from number of mu bins (Blob) intensity_bin_edges: array of intensity bin edges from number of intensity bins (Blob) hist: histogram associated with image (Blob)","title":"Histogram"},{"location":"db/iit/#image-combos","text":"This table stores information regarding the combination of images used to calculate the fit parameter. Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) n_images: number of images in combination (Integer) date_mean: mean date of images in image combination (DateTime) date_max: maximum date of images in image combination (DateTime) date_min: minimum date of images in image combination (DateTime)","title":"Image Combos"},{"location":"db/iit/#image-combo-assoc","text":"This table stores specific image ids with the associated combo id. Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Foreign Key: Image Combos, Integer) image_id: integer id associated with image (Primary Key, Foreign Key: EUV Images, Integer)","title":"Image Combo Assoc"},{"location":"db/iit/#method-defs","text":"This table stores information about a correction method and an associated integer method id. Columns: meth_id: auto-incremented integer id associated with the specific method (Primary Key, Integer) meth_name: method name (String) meth_description: description of method (String)","title":"Method Defs"},{"location":"db/iit/#var-defs","text":"This table stores information about a variable and an associated integer variable id. Columns: var_id: auto-incremented integer id associated with the specific variable (Primary Key, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) var_name: variable name (String) var_description: description of variable (String)","title":"Var Defs"},{"location":"db/iit/#var-vals","text":"This table stores variable values with the associated variable, method, and image combination. These values are calculated from the IIT fit analysis ( IIT Step Two ). These values are queried during the application of the correction ( IIT Step Three ) and during the creation of histogram plots ( IIT Step Four ). Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Foreign Key: Image Combos, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) var_id: auto-incremented integer id associated with the specific variable (Primary Key, Foreign Key: Var Defs, Integer) var_val: variable value (Float)","title":"Var Vals"},{"location":"db/lbc/","text":"Database for Limb-Brightening Correction For the Limb-Brightening Correction, the database is used to query for images, store histograms, and store fit parameter values. These fit parameter values can then be queried in order to apply the Limb-Brightening correction. Tables EUV Images This table stores files and information associated with EUV Images. Columns: image_id: auto-incremented integer id associated with the image (Primary Key, Integer) date_obs: time of image observation (DateTime) instrument: observation instrument (String) wavelength: observation wavelength (Integer) fname_raw: associated fits file (String) fname_hdf: associated hdf5 file (String) distance: associated distance (Float) cr_lon: Carrington Longitude (Float) cr_lat: Carrington Latitude (Float) cr_rot: Carrington Rotation (Float) flag: default 0 (Integer) time_of_download: time of image download to database (DateTime) Histogram This table stores histogram and information associated with LBC Histograms. The histograms are created in Step One of Limb Brightening. Columns: hist_id: auto-incremented integer id associated with the histogram (Primary Key, Integer) image_id: integer id associated with image (Foreign Key: EUV Images, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) date_obs: time of image observation (DateTime) wavelength: observation wavelength (Integer) n_mu_bins: number of mu bins (Integer) n_intensity_bins: number of intensity bins (Integer) lat_band: latitude band (Blob) mu_bin_edges: array of mu bin edges from number of mu bins (Blob) intensity_bin_edges: array of intensity bin edges from number of intensity bins (Blob) hist: histogram associated with image (Blob) Image Combos This table stores information regarding the combination of images used to calculate the fit parameter. Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) n_images: number of images in combination (Integer) date_mean: mean date of images in image combination (DateTime) date_max: maximum date of images in image combination (DateTime) date_min: minimum date of images in image combination (DateTime) Image Combo Assoc This table stores specific image ids with the associated combo id. Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Foreign Key: Image Combos, Integer) image_id: integer id associated with image (Primary Key, Foreign Key: EUV Images, Integer) Method Defs This table stores information about a correction method and an associated integer method id. Columns: meth_id: auto-incremented integer id associated with the specific method (Primary Key, Integer) meth_name: method name (String) meth_description: description of method (String) Var Defs This table stores information about a variable and an associated integer variable id. Columns: var_id: auto-incremented integer id associated with the specific variable (Primary Key, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) var_name: variable name (String) var_description: description of variable (String) Var Vals This table stores variable values with the associated variable, method, and image combination. These values are calculated from the theoretical fit analysis ( LBC Step Two ). These values are queried during the application of the correction ( LBC Step Three ) and during the creation of beta and y plots ( LBC Step Four ). Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Foreign Key: Image Combos, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) var_id: auto-incremented integer id associated with the specific variable (Primary Key, Foreign Key: Var Defs, Integer) var_val: variable value (Float)","title":"Database for LBC"},{"location":"db/lbc/#database-for-limb-brightening-correction","text":"For the Limb-Brightening Correction, the database is used to query for images, store histograms, and store fit parameter values. These fit parameter values can then be queried in order to apply the Limb-Brightening correction.","title":"Database for Limb-Brightening Correction"},{"location":"db/lbc/#tables","text":"","title":"Tables"},{"location":"db/lbc/#euv-images","text":"This table stores files and information associated with EUV Images. Columns: image_id: auto-incremented integer id associated with the image (Primary Key, Integer) date_obs: time of image observation (DateTime) instrument: observation instrument (String) wavelength: observation wavelength (Integer) fname_raw: associated fits file (String) fname_hdf: associated hdf5 file (String) distance: associated distance (Float) cr_lon: Carrington Longitude (Float) cr_lat: Carrington Latitude (Float) cr_rot: Carrington Rotation (Float) flag: default 0 (Integer) time_of_download: time of image download to database (DateTime)","title":"EUV Images"},{"location":"db/lbc/#histogram","text":"This table stores histogram and information associated with LBC Histograms. The histograms are created in Step One of Limb Brightening. Columns: hist_id: auto-incremented integer id associated with the histogram (Primary Key, Integer) image_id: integer id associated with image (Foreign Key: EUV Images, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) date_obs: time of image observation (DateTime) wavelength: observation wavelength (Integer) n_mu_bins: number of mu bins (Integer) n_intensity_bins: number of intensity bins (Integer) lat_band: latitude band (Blob) mu_bin_edges: array of mu bin edges from number of mu bins (Blob) intensity_bin_edges: array of intensity bin edges from number of intensity bins (Blob) hist: histogram associated with image (Blob)","title":"Histogram"},{"location":"db/lbc/#image-combos","text":"This table stores information regarding the combination of images used to calculate the fit parameter. Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) n_images: number of images in combination (Integer) date_mean: mean date of images in image combination (DateTime) date_max: maximum date of images in image combination (DateTime) date_min: minimum date of images in image combination (DateTime)","title":"Image Combos"},{"location":"db/lbc/#image-combo-assoc","text":"This table stores specific image ids with the associated combo id. Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Foreign Key: Image Combos, Integer) image_id: integer id associated with image (Primary Key, Foreign Key: EUV Images, Integer)","title":"Image Combo Assoc"},{"location":"db/lbc/#method-defs","text":"This table stores information about a correction method and an associated integer method id. Columns: meth_id: auto-incremented integer id associated with the specific method (Primary Key, Integer) meth_name: method name (String) meth_description: description of method (String)","title":"Method Defs"},{"location":"db/lbc/#var-defs","text":"This table stores information about a variable and an associated integer variable id. Columns: var_id: auto-incremented integer id associated with the specific variable (Primary Key, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) var_name: variable name (String) var_description: description of variable (String)","title":"Var Defs"},{"location":"db/lbc/#var-vals","text":"This table stores variable values with the associated variable, method, and image combination. These values are calculated from the theoretical fit analysis ( LBC Step Two ). These values are queried during the application of the correction ( LBC Step Three ) and during the creation of beta and y plots ( LBC Step Four ). Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Foreign Key: Image Combos, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) var_id: auto-incremented integer id associated with the specific variable (Primary Key, Foreign Key: Var Defs, Integer) var_val: variable value (Float)","title":"Var Vals"},{"location":"db/map/","text":"Database for Coronal Hole Detection and Mapping For creation of EUV and CHD maps, the database is used to query EUV Images, LBC/IIT Correction Coefficients, and save mapping methods and resulting maps to the database. Tables EUV Images This table stores files and information associated with EUV Images. It is queried to get the original EUV Images before applying Image Pre-Processing (LBC and IIT) Corrections. Columns: image_id: auto-incremented integer id associated with the image (Primary Key, Integer) date_obs: time of image observation (DateTime) instrument: observation instrument (String) wavelength: observation wavelength (Integer) fname_raw: associated fits file (String) fname_hdf: associated hdf5 file (String) distance: associated distance (Float) cr_lon: Carrington Longitude (Float) cr_lat: Carrington Latitude (Float) cr_rot: Carrington Rotation (Float) flag: default 0 (Integer) time_of_download: time of image download to database (DateTime) Image Combos This table stores information regarding the combination of images used to calculate the fit parameter. It is used to determine what combo id corresponds to the date in question. Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) n_images: number of images in combination (Integer) date_mean: mean date of images in image combination (DateTime) date_max: maximum date of images in image combination (DateTime) date_min: minimum date of images in image combination (DateTime) Image Combo Assoc This table stores specific image ids with the associated combo id. It is used when querying for the correct combo id. Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Foreign Key: Image Combos, Integer) image_id: integer id associated with image (Primary Key, Foreign Key: EUV Images, Integer) Var Vals This table stores variable values with the associated variable, method, and image combination. It is queried for the correction parameters used for Limb-Brightening and Inter-Instrument Transformation corrections. Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Foreign Key: Image Combos, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) var_id: auto-incremented integer id associated with the specific variable (Primary Key, Foreign Key: Var Defs, Integer) var_val: variable value (Float) Method Defs This table stores information about a correction method and an associated integer method id, used when querying for correction parameters. Columns: meth_id: auto-incremented integer id associated with the specific method (Primary Key, Integer) meth_name: method name (String) meth_description: description of method (String) Var Defs This table stores information about a variable and an associated integer variable id. It is used when querying for correction parameters to apply LBC/IIT. Columns: var_id: auto-incremented integer id associated with the specific variable (Primary Key, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) var_name: variable name (String) var_description: description of variable (String) Var Vals Map This table stores variable values with the associated map, variable, method, and image combination. Values are saved here when the map is saved to the database. Columns: map_id: auto-incremented interger id associated with specific map (Primary Key, Integer) combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Foreign Key: Image Combos, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) var_id: auto-incremented integer id associated with the specific variable (Primary Key, Foreign Key: Var Defs, Integer) var_val: variable value (Float) Method Combos This table stores information about associated correction methods used in the creation of a map. A new method combination is created when a map is saved to the database. Columns: meth_combo_id: auto-incremented integer id associated with the specific method combination (Primary Key, Integer) n_methods: number of associated methods (Integer) Method Combo Assoc This table associates method combo ids with the method id. Columns: meth_combo_id: auto-incremented integer id associated with the specific method combination (Primary Key, Foreign Key: Method Combos, Integer) meth_id: auto-incremented integer id associated with the specific method (Primary Key, Foreign Key: Method Defs, Integer) EUV Maps This table stores files and information associated with EUV Maps. Columns: map_id: auto-incremented integer id associated with the map (Primary Key, Integer) combo_id: auto-incremented integer id associated with that specific combination of images (Foreign Key: Image Combos, Integer) meth_combo_id: auto-incremented integer id associated with the specific method combination (Primary Key, Foreign Key: Method Combos, Integer) fname: associated hdf5 file, saved either as a 'single' or 'synoptic' map (String) time_of_compute: time of map computation (DateTime)","title":"Database for CHD Mapping"},{"location":"db/map/#database-for-coronal-hole-detection-and-mapping","text":"For creation of EUV and CHD maps, the database is used to query EUV Images, LBC/IIT Correction Coefficients, and save mapping methods and resulting maps to the database.","title":"Database for Coronal Hole Detection and Mapping"},{"location":"db/map/#tables","text":"","title":"Tables"},{"location":"db/map/#euv-images","text":"This table stores files and information associated with EUV Images. It is queried to get the original EUV Images before applying Image Pre-Processing (LBC and IIT) Corrections. Columns: image_id: auto-incremented integer id associated with the image (Primary Key, Integer) date_obs: time of image observation (DateTime) instrument: observation instrument (String) wavelength: observation wavelength (Integer) fname_raw: associated fits file (String) fname_hdf: associated hdf5 file (String) distance: associated distance (Float) cr_lon: Carrington Longitude (Float) cr_lat: Carrington Latitude (Float) cr_rot: Carrington Rotation (Float) flag: default 0 (Integer) time_of_download: time of image download to database (DateTime)","title":"EUV Images"},{"location":"db/map/#image-combos","text":"This table stores information regarding the combination of images used to calculate the fit parameter. It is used to determine what combo id corresponds to the date in question. Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) n_images: number of images in combination (Integer) date_mean: mean date of images in image combination (DateTime) date_max: maximum date of images in image combination (DateTime) date_min: minimum date of images in image combination (DateTime)","title":"Image Combos"},{"location":"db/map/#image-combo-assoc","text":"This table stores specific image ids with the associated combo id. It is used when querying for the correct combo id. Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Foreign Key: Image Combos, Integer) image_id: integer id associated with image (Primary Key, Foreign Key: EUV Images, Integer)","title":"Image Combo Assoc"},{"location":"db/map/#var-vals","text":"This table stores variable values with the associated variable, method, and image combination. It is queried for the correction parameters used for Limb-Brightening and Inter-Instrument Transformation corrections. Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Foreign Key: Image Combos, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) var_id: auto-incremented integer id associated with the specific variable (Primary Key, Foreign Key: Var Defs, Integer) var_val: variable value (Float)","title":"Var Vals"},{"location":"db/map/#method-defs","text":"This table stores information about a correction method and an associated integer method id, used when querying for correction parameters. Columns: meth_id: auto-incremented integer id associated with the specific method (Primary Key, Integer) meth_name: method name (String) meth_description: description of method (String)","title":"Method Defs"},{"location":"db/map/#var-defs","text":"This table stores information about a variable and an associated integer variable id. It is used when querying for correction parameters to apply LBC/IIT. Columns: var_id: auto-incremented integer id associated with the specific variable (Primary Key, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) var_name: variable name (String) var_description: description of variable (String)","title":"Var Defs"},{"location":"db/map/#var-vals-map","text":"This table stores variable values with the associated map, variable, method, and image combination. Values are saved here when the map is saved to the database. Columns: map_id: auto-incremented interger id associated with specific map (Primary Key, Integer) combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Foreign Key: Image Combos, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) var_id: auto-incremented integer id associated with the specific variable (Primary Key, Foreign Key: Var Defs, Integer) var_val: variable value (Float)","title":"Var Vals Map"},{"location":"db/map/#method-combos","text":"This table stores information about associated correction methods used in the creation of a map. A new method combination is created when a map is saved to the database. Columns: meth_combo_id: auto-incremented integer id associated with the specific method combination (Primary Key, Integer) n_methods: number of associated methods (Integer)","title":"Method Combos"},{"location":"db/map/#method-combo-assoc","text":"This table associates method combo ids with the method id. Columns: meth_combo_id: auto-incremented integer id associated with the specific method combination (Primary Key, Foreign Key: Method Combos, Integer) meth_id: auto-incremented integer id associated with the specific method (Primary Key, Foreign Key: Method Defs, Integer)","title":"Method Combo Assoc"},{"location":"db/map/#euv-maps","text":"This table stores files and information associated with EUV Maps. Columns: map_id: auto-incremented integer id associated with the map (Primary Key, Integer) combo_id: auto-incremented integer id associated with that specific combination of images (Foreign Key: Image Combos, Integer) meth_combo_id: auto-incremented integer id associated with the specific method combination (Primary Key, Foreign Key: Method Combos, Integer) fname: associated hdf5 file, saved either as a 'single' or 'synoptic' map (String) time_of_compute: time of map computation (DateTime)","title":"EUV Maps"},{"location":"dp/cr/","text":"Synoptic Maps Example Maps for the Month of June 2011 These example maps used data from June 1 to July 1 2011. The functions to create basic full CR EUV/CHD Maps can be found here while the example is here . The example to create mu-dependent CHD maps is found here . CR EUV Map CR CHD Map Mu-Dependent CR CHD Map","title":"Synoptic Maps"},{"location":"dp/cr/#synoptic-maps","text":"","title":"Synoptic Maps"},{"location":"dp/cr/#example-maps-for-the-month-of-june-2011","text":"These example maps used data from June 1 to July 1 2011. The functions to create basic full CR EUV/CHD Maps can be found here while the example is here . The example to create mu-dependent CHD maps is found here .","title":"Example Maps for the Month of June 2011"},{"location":"dp/cr/#cr-euv-map","text":"","title":"CR EUV Map"},{"location":"dp/cr/#cr-chd-map","text":"","title":"CR CHD Map"},{"location":"dp/cr/#mu-dependent-cr-chd-map","text":"","title":"Mu-Dependent CR CHD Map"},{"location":"dp/ed/","text":"Time-Varying Ensemble Detection There are currently two methods of creating time-varying maps. The first creates maps of varying timescale based on user entered intervals around a specific user inputted center date. These maps are then combined with a weighted average - either user inputted, or evenly weighted. The second method uses a Gaussian distribution and a user inputted center date. The closer an image is to the center date, the more weight it has. Images are added one at a time to create the resultant maps . Running Average Maps EUV Map CHD Map Gaussian Time Varying Maps EUV Map CHD Map Code Outline Running Average Maps This outline creates individual combined maps based on user defined timescales (1 day, 1 week, 2 weeks, etc.), then combines the maps based on user defined weighting. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 if timescale_weight is None : timescale_weight = [ 1.0 / len ( timescales )] * len ( timescales ) weight_sum = sum ( timescale_weight ) if weight_sum != 1 : raise ValueError ( \"The timescale weights do not sum to 1. Please reenter weights or change to None for even weighting of maps.\" ) lbc_combo_query , iit_combo_query = chd_funcs . get_inst_combos ( db_session , inst_list , time_min = max_time_min , time_max = max_time_max ) query_pd = db_funcs . query_euv_images ( db_session = db_session , time_min = query_time_min , time_max = query_time_max ) los_image , iit_image , methods_list , use_indices = cr_funcs . apply_ipp ( db_session , hdf_data_dir , inst_list , row , methods_list , lbc_combo_query , iit_combo_query , n_intensity_bins = n_intensity_bins , R0 = R0 ) chd_image = cr_funcs . chd ( db_session , inst_list , los_image , iit_image , use_indices , iit_combo_query , thresh1 = thresh1 , thresh2 = thresh2 , nc = nc , iters = iters ) euv_map , chd_map = cr_funcs . create_map ( iit_image , chd_image , methods_list , row , map_x = map_x , map_y = map_y , R0 = R0 ) euv_timescale [ time_ind ], chd_timescale [ time_ind ], combined_method , chd_combined_method = cr_funcs . cr_map ( euv_map , chd_map , euv_timescale [ time_ind ], chd_timescale [ time_ind ], image_info , map_info , mu_cutoff = mu_cutoff , mu_merge_cutoff = mu_merge_cutoff ) euv_combined , chd_combined , timescale_method = dp_funcs . create_timescale_maps ( euv_timescale , chd_timescale , timescale_weight , image_info_timescale , map_info_timescale ) dp_funcs . save_timescale_maps ( db_session , map_data_dir , euv_combined , chd_combined , image_info_timescale , map_info_timescale , methods_list , combined_method , chd_combined_method , timescale_method ) 1.) timescale_weight = [1.0 / len(timescales)] * len(timescales) if the timescale weights are undefined, create an array of even weights 2.) raise ValueError check that the weights in the array add up to 1 3.) chd_funcs.get_inst_combos query the appropriate combo ids for each instrument based off the maximum time range the user inputted 4.) db_funcs.query_euv_images query the database for images based off the time range and center date 5.) cr_funcs.apply_ipp apply image pre-processing corrections 6.) cr_funcs.chd apply the Coronal Hole Detection algorithm to the image 7.) cr_funcs.create_map convert the image and detection to a map 8.) cr_funcs.cr_map create combined maps using the method for synoptic mapping 9.) dp_funcs.create_timescale_maps combine the timescale maps to create a running average map, based off the timescale combination function 10.) dp_funcs.save_timescale_maps plot and save timescale maps to the database, including the methods combination Gaussian Time Varying Maps This method first creates a gaussian distribution and then weights images based off their closeness to a center date. 1 2 3 4 5 6 7 8 lbc_combo_query , iit_combo_query = chd_funcs . get_inst_combos ( db_session , inst_list , time_min = query_time_min , time_max = query_time_max ) query_pd = db_funcs . query_euv_images ( db_session = db_session , time_min = query_time_min , time_max = query_time_max ) norm_dist = dp_funcs . gauss_time ( query_pd , sigma ) los_image , iit_image , methods_list , use_indices = cr_funcs . apply_ipp ( db_session , hdf_data_dir , inst_list , row , methods_list , lbc_combo_query , iit_combo_query , n_intensity_bins = n_intensity_bins , R0 = R0 ) chd_image = cr_funcs . chd ( db_session , inst_list , los_image , iit_image , use_indices , iit_combo_query , thresh1 = thresh1 , thresh2 = thresh2 , nc = nc , iters = iters ) euv_map , chd_map = cr_funcs . create_map ( iit_image , chd_image , methods_list , row , map_x = map_x , map_y = map_y , R0 = R0 ) euv_combined , chd_combined , sum_wgt , combined_method = dp_funcs . time_wgt_map ( euv_map , chd_map , euv_combined , chd_combined , image_info , weight , sum_wgt , sigma , mu_cutoff ) dp_funcs . save_gauss_time_maps ( db_session , map_data_dir , euv_combined , chd_combined , image_info , map_info , methods_list , combined_method ) 1.) chd_funcs.get_inst_combos query the appropriate combo ids for each instrument based off the maximum time range the user inputted 2.) db_funcs.query_euv_images query the database for images based off the time range and center date 3.) dp_funcs.gauss_time generate a gaussian distribution based off the number of images queried and user defined sigma balue 4.) cr_funcs.apply_ipp apply image pre-processing corrections 5.) cr_funcs.chd apply the Coronal Hole Detection algorithm to the image 6.) cr_funcs.create_map convert the image and detection to a map 6.) dp_funcs.time_wgt_map create combined maps based off the weighting of the Gaussian distribution 7.) dp_funcs.save_gauss_time_maps plot and save time weighted maps to the database, including the methods combination","title":"Ensemble Detection"},{"location":"dp/ed/#time-varying-ensemble-detection","text":"There are currently two methods of creating time-varying maps. The first creates maps of varying timescale based on user entered intervals around a specific user inputted center date. These maps are then combined with a weighted average - either user inputted, or evenly weighted. The second method uses a Gaussian distribution and a user inputted center date. The closer an image is to the center date, the more weight it has. Images are added one at a time to create the resultant maps .","title":"Time-Varying Ensemble Detection"},{"location":"dp/ed/#running-average-maps","text":"EUV Map CHD Map","title":"Running Average Maps"},{"location":"dp/ed/#gaussian-time-varying-maps","text":"EUV Map CHD Map","title":"Gaussian Time Varying Maps"},{"location":"dp/ed/#code-outline","text":"","title":"Code Outline"},{"location":"dp/ed/#running-average-maps_1","text":"This outline creates individual combined maps based on user defined timescales (1 day, 1 week, 2 weeks, etc.), then combines the maps based on user defined weighting. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 if timescale_weight is None : timescale_weight = [ 1.0 / len ( timescales )] * len ( timescales ) weight_sum = sum ( timescale_weight ) if weight_sum != 1 : raise ValueError ( \"The timescale weights do not sum to 1. Please reenter weights or change to None for even weighting of maps.\" ) lbc_combo_query , iit_combo_query = chd_funcs . get_inst_combos ( db_session , inst_list , time_min = max_time_min , time_max = max_time_max ) query_pd = db_funcs . query_euv_images ( db_session = db_session , time_min = query_time_min , time_max = query_time_max ) los_image , iit_image , methods_list , use_indices = cr_funcs . apply_ipp ( db_session , hdf_data_dir , inst_list , row , methods_list , lbc_combo_query , iit_combo_query , n_intensity_bins = n_intensity_bins , R0 = R0 ) chd_image = cr_funcs . chd ( db_session , inst_list , los_image , iit_image , use_indices , iit_combo_query , thresh1 = thresh1 , thresh2 = thresh2 , nc = nc , iters = iters ) euv_map , chd_map = cr_funcs . create_map ( iit_image , chd_image , methods_list , row , map_x = map_x , map_y = map_y , R0 = R0 ) euv_timescale [ time_ind ], chd_timescale [ time_ind ], combined_method , chd_combined_method = cr_funcs . cr_map ( euv_map , chd_map , euv_timescale [ time_ind ], chd_timescale [ time_ind ], image_info , map_info , mu_cutoff = mu_cutoff , mu_merge_cutoff = mu_merge_cutoff ) euv_combined , chd_combined , timescale_method = dp_funcs . create_timescale_maps ( euv_timescale , chd_timescale , timescale_weight , image_info_timescale , map_info_timescale ) dp_funcs . save_timescale_maps ( db_session , map_data_dir , euv_combined , chd_combined , image_info_timescale , map_info_timescale , methods_list , combined_method , chd_combined_method , timescale_method ) 1.) timescale_weight = [1.0 / len(timescales)] * len(timescales) if the timescale weights are undefined, create an array of even weights 2.) raise ValueError check that the weights in the array add up to 1 3.) chd_funcs.get_inst_combos query the appropriate combo ids for each instrument based off the maximum time range the user inputted 4.) db_funcs.query_euv_images query the database for images based off the time range and center date 5.) cr_funcs.apply_ipp apply image pre-processing corrections 6.) cr_funcs.chd apply the Coronal Hole Detection algorithm to the image 7.) cr_funcs.create_map convert the image and detection to a map 8.) cr_funcs.cr_map create combined maps using the method for synoptic mapping 9.) dp_funcs.create_timescale_maps combine the timescale maps to create a running average map, based off the timescale combination function 10.) dp_funcs.save_timescale_maps plot and save timescale maps to the database, including the methods combination","title":"Running Average Maps"},{"location":"dp/ed/#gaussian-time-varying-maps_1","text":"This method first creates a gaussian distribution and then weights images based off their closeness to a center date. 1 2 3 4 5 6 7 8 lbc_combo_query , iit_combo_query = chd_funcs . get_inst_combos ( db_session , inst_list , time_min = query_time_min , time_max = query_time_max ) query_pd = db_funcs . query_euv_images ( db_session = db_session , time_min = query_time_min , time_max = query_time_max ) norm_dist = dp_funcs . gauss_time ( query_pd , sigma ) los_image , iit_image , methods_list , use_indices = cr_funcs . apply_ipp ( db_session , hdf_data_dir , inst_list , row , methods_list , lbc_combo_query , iit_combo_query , n_intensity_bins = n_intensity_bins , R0 = R0 ) chd_image = cr_funcs . chd ( db_session , inst_list , los_image , iit_image , use_indices , iit_combo_query , thresh1 = thresh1 , thresh2 = thresh2 , nc = nc , iters = iters ) euv_map , chd_map = cr_funcs . create_map ( iit_image , chd_image , methods_list , row , map_x = map_x , map_y = map_y , R0 = R0 ) euv_combined , chd_combined , sum_wgt , combined_method = dp_funcs . time_wgt_map ( euv_map , chd_map , euv_combined , chd_combined , image_info , weight , sum_wgt , sigma , mu_cutoff ) dp_funcs . save_gauss_time_maps ( db_session , map_data_dir , euv_combined , chd_combined , image_info , map_info , methods_list , combined_method ) 1.) chd_funcs.get_inst_combos query the appropriate combo ids for each instrument based off the maximum time range the user inputted 2.) db_funcs.query_euv_images query the database for images based off the time range and center date 3.) dp_funcs.gauss_time generate a gaussian distribution based off the number of images queried and user defined sigma balue 4.) cr_funcs.apply_ipp apply image pre-processing corrections 5.) cr_funcs.chd apply the Coronal Hole Detection algorithm to the image 6.) cr_funcs.create_map convert the image and detection to a map 6.) dp_funcs.time_wgt_map create combined maps based off the weighting of the Gaussian distribution 7.) dp_funcs.save_gauss_time_maps plot and save time weighted maps to the database, including the methods combination","title":"Gaussian Time Varying Maps"},{"location":"dp/qm/","text":"Quality Assurance Maps The goal of these maps is to determine where data at each pixel came from, and the mu value of the origin image at that point. Example Maps Synchronic Maps on June 18, 2011 EUV Map Quality EUV Map CHD Map Quality CHD Map Full CR EUV Map Quality CR EUV Map Code Outline 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def quality_map ( db_session , map_data_dir , inst_list , query_pd , euv_combined , chd_combined = None , color_list = None ): euv_origin_image = euv_combined . origin_image euv_origins = np . unique ( euv_origin_image ) euv_image = np . empty ( euv_origin_image . shape , dtype = object ) for euv_id in euv_origins : query_ind = np . where ( query_pd [ 'data_id' ] == euv_id ) instrument = query_pd [ 'instrument' ] . iloc [ query_ind [ 0 ]] if len ( instrument ) != 0 : euv_image = np . where ( euv_origin_image != euv_id , euv_image , instrument . iloc [ 0 ]) Plotting . PlotQualityMap ( euv_combined , euv_image , inst_list , color_list , nfig = 'EUV Quality Map ' + str ( euv_combined . image_info . date_obs [ 0 ]), title = 'EUV Quality Map: Mu Dependent \\n ' + str ( euv_combined . image_info . date_obs [ 0 ])) if chd_combined is not None : chd_origin_image = chd_combined . origin_image chd_origins = np . unique ( chd_origin_image ) chd_image = np . empty ( chd_origin_image . shape , dtype = object ) for chd_id in chd_origins : query_ind = np . where ( query_pd [ 'data_id' ] == chd_id ) instrument = query_pd [ 'instrument' ] . iloc [ query_ind [ 0 ]] if len ( instrument ) != 0 : chd_image = np . where ( euv_origin_image != chd_id , chd_image , instrument . iloc [ 0 ]) Plotting . PlotQualityMap ( chd_combined , chd_image , inst_list , color_list , nfig = 'CHD Quality Map ' + str ( chd_combined . image_info . date_obs [ 0 ]), title = 'CHD Quality Map: Mu Dependent \\n ' + str ( chd_combined . image_info . date_obs [ 0 ]), map_type = 'CHD' ) # save these maps to database return None 1.) query_ind = np.where(query_pd['image_id'] == euv_id) loop through the unique list of image ids and determine at what indices they are present 2.) euv_image = np.where(euv_origin_image != euv_id, euv_image, instrument.iloc[0]) add instrument name to array in correct pixel position 3.) Plotting.PlotQualityMap plot a quality map based off instrument and mu value of the final map","title":"Quality Maps"},{"location":"dp/qm/#quality-assurance-maps","text":"The goal of these maps is to determine where data at each pixel came from, and the mu value of the origin image at that point.","title":"Quality Assurance Maps"},{"location":"dp/qm/#example-maps","text":"","title":"Example Maps"},{"location":"dp/qm/#synchronic-maps-on-june-18-2011","text":"EUV Map Quality EUV Map CHD Map Quality CHD Map Full CR EUV Map Quality CR EUV Map","title":"Synchronic Maps on June 18, 2011"},{"location":"dp/qm/#code-outline","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def quality_map ( db_session , map_data_dir , inst_list , query_pd , euv_combined , chd_combined = None , color_list = None ): euv_origin_image = euv_combined . origin_image euv_origins = np . unique ( euv_origin_image ) euv_image = np . empty ( euv_origin_image . shape , dtype = object ) for euv_id in euv_origins : query_ind = np . where ( query_pd [ 'data_id' ] == euv_id ) instrument = query_pd [ 'instrument' ] . iloc [ query_ind [ 0 ]] if len ( instrument ) != 0 : euv_image = np . where ( euv_origin_image != euv_id , euv_image , instrument . iloc [ 0 ]) Plotting . PlotQualityMap ( euv_combined , euv_image , inst_list , color_list , nfig = 'EUV Quality Map ' + str ( euv_combined . image_info . date_obs [ 0 ]), title = 'EUV Quality Map: Mu Dependent \\n ' + str ( euv_combined . image_info . date_obs [ 0 ])) if chd_combined is not None : chd_origin_image = chd_combined . origin_image chd_origins = np . unique ( chd_origin_image ) chd_image = np . empty ( chd_origin_image . shape , dtype = object ) for chd_id in chd_origins : query_ind = np . where ( query_pd [ 'data_id' ] == chd_id ) instrument = query_pd [ 'instrument' ] . iloc [ query_ind [ 0 ]] if len ( instrument ) != 0 : chd_image = np . where ( euv_origin_image != chd_id , chd_image , instrument . iloc [ 0 ]) Plotting . PlotQualityMap ( chd_combined , chd_image , inst_list , color_list , nfig = 'CHD Quality Map ' + str ( chd_combined . image_info . date_obs [ 0 ]), title = 'CHD Quality Map: Mu Dependent \\n ' + str ( chd_combined . image_info . date_obs [ 0 ]), map_type = 'CHD' ) # save these maps to database return None 1.) query_ind = np.where(query_pd['image_id'] == euv_id) loop through the unique list of image ids and determine at what indices they are present 2.) euv_image = np.where(euv_origin_image != euv_id, euv_image, instrument.iloc[0]) add instrument name to array in correct pixel position 3.) Plotting.PlotQualityMap plot a quality map based off instrument and mu value of the final map","title":"Code Outline"},{"location":"dp/vt/","text":"Gaussian Varying Threshold Parameters These maps are created using varying CHD Threshold parameters. The threshold is varied based off a Gaussian distribution with a user inputted value of sigma (sigma=0.15) and thresholds for each image are randomly selected. The only different data product produced here is the Coronal Hole Detection map, the EUV map is the same as the synoptic EUV maps created. In these Gaussian weighted CHD Maps, coronal hole detections are weighted to produce a probability type map just as in the synoptic CHD Maps . Example Maps EUV Map CHD Map Code Outline 1 2 3 4 5 6 7 query_pd = db_funcs . query_euv_images ( db_session = db_session , time_min = query_time_min , time_max = query_time_max ) lbc_combo_query , iit_combo_query = chd_funcs . get_inst_combos ( db_session , inst_list , time_min = query_time_min , time_max = query_time_max ) los_image , iit_image , methods_list , use_indices = cr_funcs . apply_ipp ( db_session , hdf_data_dir , inst_list , row , methods_list , lbc_combo_query , iit_combo_query , n_intensity_bins = n_intensity_bins , R0 = R0 ) chd_image = dp_funcs . gauss_chd ( db_session , inst_list , los_image , iit_image , use_indices , iit_combo_query , thresh1 = thresh1 , thresh2 = thresh2 , nc = nc , iters = iters , sigma = sigma ) euv_map , chd_map = cr_funcs . create_map ( iit_image , chd_image , methods_list , row , map_x = map_x , map_y = map_y , R0 = R0 ) euv_combined , chd_combined , euv_combined_method , chd_combined_method = cr_funcs . cr_map ( euv_map , chd_map , euv_combined , chd_combined , image_info , map_info , mu_cutoff = mu_cutoff , mu_merge_cutoff = mu_merge_cutoff ) dp_funcs . save_threshold_maps ( db_session , map_data_dir , euv_combined , chd_combined , image_info , map_info , methods_list , euv_combined_method , chd_combined_method , sigma ) 1.) db_funcs.query_euv_images query the database for images based off the time range and center date 2.) chd_funcs.get_inst_combos query the appropriate combo ids for each instrument based off the maximum time range the user inputted 3.) cr_funcs.apply_ipp apply image pre-processing corrections 4.) dp_funcs.gauss_chd apply Coronal Hole Detection algorithm based on gaussian weighted threshold parameters 5.) cr_funcs.create_map convert the image and detection to a map 6.) cr_funcs.cr_map add image to synoptic map 7.) dp_funcs.save_threshold_maps plot and save varying threshold map to database with included methods","title":"Varying Threshold Maps"},{"location":"dp/vt/#gaussian-varying-threshold-parameters","text":"These maps are created using varying CHD Threshold parameters. The threshold is varied based off a Gaussian distribution with a user inputted value of sigma (sigma=0.15) and thresholds for each image are randomly selected. The only different data product produced here is the Coronal Hole Detection map, the EUV map is the same as the synoptic EUV maps created. In these Gaussian weighted CHD Maps, coronal hole detections are weighted to produce a probability type map just as in the synoptic CHD Maps .","title":"Gaussian Varying Threshold Parameters"},{"location":"dp/vt/#example-maps","text":"","title":"Example Maps"},{"location":"dp/vt/#euv-map","text":"","title":"EUV Map"},{"location":"dp/vt/#chd-map","text":"","title":"CHD Map"},{"location":"dp/vt/#code-outline","text":"1 2 3 4 5 6 7 query_pd = db_funcs . query_euv_images ( db_session = db_session , time_min = query_time_min , time_max = query_time_max ) lbc_combo_query , iit_combo_query = chd_funcs . get_inst_combos ( db_session , inst_list , time_min = query_time_min , time_max = query_time_max ) los_image , iit_image , methods_list , use_indices = cr_funcs . apply_ipp ( db_session , hdf_data_dir , inst_list , row , methods_list , lbc_combo_query , iit_combo_query , n_intensity_bins = n_intensity_bins , R0 = R0 ) chd_image = dp_funcs . gauss_chd ( db_session , inst_list , los_image , iit_image , use_indices , iit_combo_query , thresh1 = thresh1 , thresh2 = thresh2 , nc = nc , iters = iters , sigma = sigma ) euv_map , chd_map = cr_funcs . create_map ( iit_image , chd_image , methods_list , row , map_x = map_x , map_y = map_y , R0 = R0 ) euv_combined , chd_combined , euv_combined_method , chd_combined_method = cr_funcs . cr_map ( euv_map , chd_map , euv_combined , chd_combined , image_info , map_info , mu_cutoff = mu_cutoff , mu_merge_cutoff = mu_merge_cutoff ) dp_funcs . save_threshold_maps ( db_session , map_data_dir , euv_combined , chd_combined , image_info , map_info , methods_list , euv_combined_method , chd_combined_method , sigma ) 1.) db_funcs.query_euv_images query the database for images based off the time range and center date 2.) chd_funcs.get_inst_combos query the appropriate combo ids for each instrument based off the maximum time range the user inputted 3.) cr_funcs.apply_ipp apply image pre-processing corrections 4.) dp_funcs.gauss_chd apply Coronal Hole Detection algorithm based on gaussian weighted threshold parameters 5.) cr_funcs.create_map convert the image and detection to a map 6.) cr_funcs.cr_map add image to synoptic map 7.) dp_funcs.save_threshold_maps plot and save varying threshold map to database with included methods","title":"Code Outline"},{"location":"ipp/iit/","text":"Inter-Instrument Transformation The goal of the inter-instrument correction is to equate the intensities from one instrument to the intensities of another. The choice of which instrument to use as the \"reference instrument\" is an updatable parameter. Examples of Corrected Images These images of before and after applying IIT are from the different instruments at Carrington Rotation 2108.59. These can be enlarged by clicking image titles. AIA Images Original AIA Image Corrected AIA Image Difference AIA Image EUVI-A Images Original STA Image Corrected STA Image Difference STA Image EUVI-B Images Original STB Image Corrected STB Image Difference STB Image Examples of Histograms 200 Intensity Bin Histograms before and after IIT Correction. LBC Corrected Histogram IIT Corrected Histogram Analysis Pipeline Compute Histograms and Save to Database This function applies the limb-brightening correction, calculates the associated IIT histogram, and saves these histograms to the database. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def create_histograms ( db_session , inst_list , lbc_query_time_min , lbc_query_time_max , hdf_data_dir , n_mu_bins = 18 , n_intensity_bins = 200 , lat_band = [ - np . pi / 64. , np . pi / 64. ], log10 = True , R0 = 1.01 ): \"\"\" function to apply LBC, create and save histograms to the database \"\"\" image_pd = db_funcs . query_euv_images ( db_session = db_session , time_min = lbc_query_time_min , time_max = lbc_query_time_max , instrument = query_instrument ) combo_query = db_funcs . query_inst_combo ( db_session , lbc_query_time_min , lbc_query_time_max , meth_name = \"LBCC Theoretic\" , instrument = instrument ) original_los , lbcc_image , mu_indices , use_indices , theoretic_query = lbcc_funcs . apply_lbc ( db_session , hdf_data_dir , combo_query , image_row = row , n_intensity_bins = n_intensity_bins , R0 = R0 ) hist = psi_d_types . LBCCImage . iit_hist ( lbcc_image , lat_band , log10 ) iit_hist = psi_d_types . create_iit_hist ( lbcc_image , method_id [ 1 ], lat_band , hist ) db_funcs . add_hist ( db_session , iit_hist ) 1.) db_funcs.query_euv_images queries database for images (from EUV_Images table) in specified date range 2.) db_funcs.query_inst_combo queries database for closest image combinations to date observed 3.) lbcc_funcs.apply_lbc applies Limb-Brightening Correction to images and creates LBCCImage datatype 4.) psi_d_types.LBCCImage.iit_hist calculates IIT histogram from LBC corrected data 5.) psi_d_types.create_iit_hist creates IIT histogram datatype 6.) db_funcs.add_hist saves histograms to database (table Histogram) associating an image_id, meth_id, and basic information with histogram Calculate and Save Correction Coefficients This function queries the database for IIT histograms, calculates correction coefficients, and saves them to the database. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def calc_iit_coefficients ( db_session , inst_list , ref_inst , calc_query_time_min , calc_query_time_max , weekday = 0 , number_of_days = 180 , n_intensity_bins = 200 , lat_band = [ - np . pi / 2.4 , np . pi / 2.4 ], create = False ): \"\"\" function to query IIT histograms, calculate IIT coefficients, and save to database \"\"\" euv_images = db_funcs . query_euv_images ( db_session , time_min = calc_query_time_min , time_max = calc_query_time_max , instrument = ref_instrument ) ref_hist_pd = db_funcs . query_hist ( db_session = db_session , meth_id = method_id [ 1 ], n_intensity_bins = n_intensity_bins , lat_band = np . array ( lat_band ) . tobytes (), time_min = calc_query_time_min - datetime . timedelta ( days = number_of_days ), time_max = calc_query_time_max + datetime . timedelta ( days = number_of_days ), instrument = ref_instrument ) rot_images = db_funcs . query_euv_images_rot ( db_session , rot_min = rot_min , rot_max = rot_max , instrument = query_instrument ) alpha_x_parameters = iit . optim_iit_linear ( norm_hist_ref , norm_hist_fit , intensity_bin_edges , init_pars = init_pars ) db_funcs . store_iit_values ( db_session , pd_hist , meth_name , meth_desc , alpha_x_parameters . x , create ) 1.) db_funcs.query_euv_images queries database for euv images for the reference instrument used to get a range of Carrington rotation for which to calculate fit coefficients 2.) db_funcs.query_hist queries database for histograms (from Histogram table) in specified date range 3.) db_funcs.query_euv_images_rot queries database for euv images by Carrington rotation range 4.) iit.optim_iit_linear use linear optimization method to calculate fit parameters hist_ref and hist_fit are the reference histogram and the instrument histogram for the fit these are determined using boolean indexing 5.) db_funcs.store_iit_values save the two fit coefficients to database using function store_iit_values creates image combination combo_id of image_ids and dates in Images_Combos table creates association between each image_id and combo_id in Image_Combo_Assoc table creates new method \u201cIIT\u201d with an associated meth_id in Meth_Defs table creates new variable definitions \"alpha and \"x\"\" with an associated var_id in Var_Defs table store variable value as float in Var_Vals table with associated combo_id, meth_id, and var_id Apply Inter-Instrument Transformation and Plot New Images This function queries the database for IIT coefficients, applies the correction, and plots resulting images. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def apply_iit_correction ( db_session , hdf_data_dir , iit_query_time_min , iit_query_time_max , inst_list , n_mu_bins , n_intensity_bins , n_images_plot = 1 , plot = False ): \"\"\" function to query IIT correction coefficients, apply correction, and plot resulting images \"\"\" euv_images = db_funcs . query_euv_images ( db_session , time_min = iit_query_time_min , time_max = iit_query_time_max , instrument = ref_instrument ) rot_images = db_funcs . query_euv_images_rot ( db_session , rot_min = rot_min , rot_max = rot_max , instrument = query_instrument ) combo_query_lbc = db_funcs . query_inst_combo ( db_session , iit_query_time_min , iit_query_time_max , lbc_meth_name , instrument ) combo_query_iit = db_funcs . query_inst_combo ( db_session , iit_query_time_min , iit_query_time_max , iit_meth_name , instrument ) original_los , lbcc_image , mu_indices , use_indices , theoretic_query = lbcc_funcs . apply_lbc ( db_session , hdf_data_dir , combo_query_lbc , image_row = row , n_intensity_bins = n_intensity_bins , R0 = R0 ) lbcc_image , iit_image , use_indices , alpha , x = apply_iit ( db_session , hdf_data_dir , combo_query_iit , lbcc_image , use_indices , image_row = row , R0 = R0 ) if plot : Plotting . PlotCorrectedImage ( lbcc_data , los_image = original_los , nfig = 100 + inst_index * 10 + index , title = \"Corrected LBCC Image for \" + instrument ) Plotting . PlotCorrectedImage ( corrected_iit_data , los_image = original_los , nfig = 200 + inst_index * 10 + index , title = \"Corrected IIT Image for \" + instrument ) Plotting . PlotCorrectedImage ( lbcc_data - corrected_iit_data , los_image = original_los , nfig = 300 + inst_index * 10 + index , title = \"Difference Plot for \" + instrument ) 1.) db_funcs.query_euv_images queries database for images (from EUV_Images table) in specified date range queries for reference instrument to get minimum and maximum Carrington rotation 2.) db_funcs.query_euv_images_rot queries for instrument in question based on Carrington rotation range 3.) db_funcs.query_inst_combo queries database for closest image combinations to date observed does this for both the LBC and IIT methods 4.) lbcc_funcs.apply_lbc applies Limb-Brightening Correction to images and creates LBCCImage datatype 5.) apply_iit applies Inter-Instrument Transformation Correction to images and creates IITImage datatype 6.) Plotting.PlotCorrectedImage plots LBC images, IIT corrected images, and the difference between them Apply IIT This is a sub-step that applies the Inter-Instrument Transformation Correction to individual image and returns the correct IIT Image. It is called during the third step of Inter-Instrument Transformation. 1 2 3 4 5 6 7 8 9 10 11 12 13 def apply_iit ( db_session , hdf_data_dir , inst_combo_query , lbcc_image , image_row , R0 = 1.01 ): \"\"\" function to apply IIT to a specific image, returns corrected image \"\"\" method_id_info = db_funcs . get_method_id ( db_session , meth_name , meth_desc = None , var_names = None , var_descs = None , create = False ) alpha_x_parameters = db_funcs . query_var_val ( db_session , meth_name , date_obs = lbcc_image . date_obs , inst_combo_query = inst_combo_query ) corrected_iit_data [ use_indices ] = 10 ** ( alpha * np . log10 ( lbcc_data [ use_indices ]) + x ) iit_image = psi_d_types . create_iit_image ( lbcc_image , corrected_iit_data , method_id_info [ 1 ], hdf_path ) return lbcc_image , iit_image , use_indices , alpha , x 1.) db_funcs.get_method_id queries database for method id associated with method name 2.) db_funcs.query_var_val queries database for variable values associated with specific image (from Var_Vals table) 3.) corrected_iit_data[use_indices] = 10 ** (alpha * np.log10(lbcc_data[use_indices]) + x) applies correction to image based off alpha, x, and lbcc corrected data arrays 4.) psi_d_types.create_iit_image create IIT Image datatype from corrected IIT data Generate Histogram Plots This function generates histogram plots comparing data from before and after the IIT correction. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def plot_iit_histograms ( db_session , hdf_data_dir , hist_query_time_min , hist_query_time_max , inst_list , ref_inst , n_intensity_bins = 200 , lat_band = [ - np . pi / 2.4 , np . pi / 2.4 ], R0 = 1.01 , log10 = True ): \"\"\" function to create corrected IIT histograms then plot original, LBC Corrected, and IIT Corrected histograms for comparison \"\"\" pd_hist = db_funcs . query_hist ( db_session = db_session , meth_id = method_id [ 1 ], n_intensity_bins = n_intensity_bins , lat_band = np . array ( lat_band ) . tobytes (), time_min = hist_query_time_min , time_max = hist_query_time_max ) combo_query_lbc = db_funcs . query_inst_combo ( db_session , hist_query_time_min , hist_query_time_max , meth_name = \"LBCC Theoretic\" , instrument = instrument ) combo_query_iit = db_funcs . query_inst_combo ( db_session , hist_query_time_min , hist_query_time_max , meth_name = \"IIT\" , instrument = instrument ) image_pd = db_funcs . query_euv_images ( db_session = db_session , time_min = hist_query_time_min , time_max = hist_query_time_max , instrument = query_instrument ) original_los , lbcc_image , mu_indices , use_indices , theoretic_query = lbcc_funcs . apply_lbc ( db_session , hdf_data_dir , combo_query_lbc , image_row = row , n_intensity_bins = n_intensity_bins , R0 = R0 ) original_los_hist = psi_d_types . LosImage . iit_hist ( original_los , intensity_bin_edges , lat_band , log10 ) lbcc_image , iit_image , use_indices , alpha , x = apply_iit ( db_session , hdf_data_dir , combo_query_iit , lbcc_image , use_indices , image_row = row , R0 = R0 ) hist_iit = psi_d_types . IITImage . iit_hist ( iit_image , lat_band , log10 ) Plotting . Plot1d_Hist ( norm_original_hist , instrument , inst_index , intensity_bin_edges , color_list , linestyle_list , figure = 100 , xlabel = \"Intensity (log10)\" , ylabel = \"H(I)\" , title = \"Histogram: Original Image\" ) Plotting . Plot1d_Hist ( norm_lbc_hist , instrument , inst_index , intensity_bin_edges , color_list , linestyle_list , figure = 200 , xlabel = \"Intensity (log10)\" , ylabel = \"H(I)\" , title = \"Histogram: Post LBCC\" ) Plotting . Plot1d_Hist ( norm_corrected_hist , instrument , inst_index , intensity_bin_edges , color_list , linestyle_list , figure = 300 , xlabel = \"Intensity (log10)\" , ylabel = \"H(I)\" , title = \"Histogram: Post IIT\" ) 1.) db_funcs.query_hist queries database for histograms (from Histogram table) in specified date range 2.) db_funcs.query_euv_images queries database for images (from EUV_Images table) in specified date range 3.) db_funcs.query_inst_combo queries database for closest image combinations to date observed does this for both the LBC and IIT methods 3.) lbcc_funcs.apply_lbc applies Limb-Brightening Correction to images and creates LBCCImage datatype 4.) psi_d_types.LosImage.iit_hist create 1D IIT Histogram from original LOS image data 5.) apply_iit applies Inter-Instrument Transformation Correction to images and creates IITImage datatype 6.) psi_d_types.IITImage.iit_hist create 1D IIT Histogram from corrected IIT image data 7.) Plotting.Plot1d_Hist plot 1D Normalized IIT Histograms for original, LBC, and IIT data","title":"Inter-Instrument Transformation"},{"location":"ipp/iit/#inter-instrument-transformation","text":"The goal of the inter-instrument correction is to equate the intensities from one instrument to the intensities of another. The choice of which instrument to use as the \"reference instrument\" is an updatable parameter.","title":"Inter-Instrument Transformation"},{"location":"ipp/iit/#examples-of-corrected-images","text":"These images of before and after applying IIT are from the different instruments at Carrington Rotation 2108.59. These can be enlarged by clicking image titles.","title":"Examples of Corrected Images"},{"location":"ipp/iit/#aia-images","text":"Original AIA Image Corrected AIA Image Difference AIA Image","title":"AIA Images"},{"location":"ipp/iit/#euvi-a-images","text":"Original STA Image Corrected STA Image Difference STA Image","title":"EUVI-A Images"},{"location":"ipp/iit/#euvi-b-images","text":"Original STB Image Corrected STB Image Difference STB Image","title":"EUVI-B Images"},{"location":"ipp/iit/#examples-of-histograms","text":"200 Intensity Bin Histograms before and after IIT Correction. LBC Corrected Histogram IIT Corrected Histogram","title":"Examples of Histograms"},{"location":"ipp/iit/#analysis-pipeline","text":"","title":"Analysis Pipeline"},{"location":"ipp/iit/#compute-histograms-and-save-to-database","text":"This function applies the limb-brightening correction, calculates the associated IIT histogram, and saves these histograms to the database. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def create_histograms ( db_session , inst_list , lbc_query_time_min , lbc_query_time_max , hdf_data_dir , n_mu_bins = 18 , n_intensity_bins = 200 , lat_band = [ - np . pi / 64. , np . pi / 64. ], log10 = True , R0 = 1.01 ): \"\"\" function to apply LBC, create and save histograms to the database \"\"\" image_pd = db_funcs . query_euv_images ( db_session = db_session , time_min = lbc_query_time_min , time_max = lbc_query_time_max , instrument = query_instrument ) combo_query = db_funcs . query_inst_combo ( db_session , lbc_query_time_min , lbc_query_time_max , meth_name = \"LBCC Theoretic\" , instrument = instrument ) original_los , lbcc_image , mu_indices , use_indices , theoretic_query = lbcc_funcs . apply_lbc ( db_session , hdf_data_dir , combo_query , image_row = row , n_intensity_bins = n_intensity_bins , R0 = R0 ) hist = psi_d_types . LBCCImage . iit_hist ( lbcc_image , lat_band , log10 ) iit_hist = psi_d_types . create_iit_hist ( lbcc_image , method_id [ 1 ], lat_band , hist ) db_funcs . add_hist ( db_session , iit_hist ) 1.) db_funcs.query_euv_images queries database for images (from EUV_Images table) in specified date range 2.) db_funcs.query_inst_combo queries database for closest image combinations to date observed 3.) lbcc_funcs.apply_lbc applies Limb-Brightening Correction to images and creates LBCCImage datatype 4.) psi_d_types.LBCCImage.iit_hist calculates IIT histogram from LBC corrected data 5.) psi_d_types.create_iit_hist creates IIT histogram datatype 6.) db_funcs.add_hist saves histograms to database (table Histogram) associating an image_id, meth_id, and basic information with histogram","title":"Compute Histograms and Save to Database"},{"location":"ipp/iit/#calculate-and-save-correction-coefficients","text":"This function queries the database for IIT histograms, calculates correction coefficients, and saves them to the database. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def calc_iit_coefficients ( db_session , inst_list , ref_inst , calc_query_time_min , calc_query_time_max , weekday = 0 , number_of_days = 180 , n_intensity_bins = 200 , lat_band = [ - np . pi / 2.4 , np . pi / 2.4 ], create = False ): \"\"\" function to query IIT histograms, calculate IIT coefficients, and save to database \"\"\" euv_images = db_funcs . query_euv_images ( db_session , time_min = calc_query_time_min , time_max = calc_query_time_max , instrument = ref_instrument ) ref_hist_pd = db_funcs . query_hist ( db_session = db_session , meth_id = method_id [ 1 ], n_intensity_bins = n_intensity_bins , lat_band = np . array ( lat_band ) . tobytes (), time_min = calc_query_time_min - datetime . timedelta ( days = number_of_days ), time_max = calc_query_time_max + datetime . timedelta ( days = number_of_days ), instrument = ref_instrument ) rot_images = db_funcs . query_euv_images_rot ( db_session , rot_min = rot_min , rot_max = rot_max , instrument = query_instrument ) alpha_x_parameters = iit . optim_iit_linear ( norm_hist_ref , norm_hist_fit , intensity_bin_edges , init_pars = init_pars ) db_funcs . store_iit_values ( db_session , pd_hist , meth_name , meth_desc , alpha_x_parameters . x , create ) 1.) db_funcs.query_euv_images queries database for euv images for the reference instrument used to get a range of Carrington rotation for which to calculate fit coefficients 2.) db_funcs.query_hist queries database for histograms (from Histogram table) in specified date range 3.) db_funcs.query_euv_images_rot queries database for euv images by Carrington rotation range 4.) iit.optim_iit_linear use linear optimization method to calculate fit parameters hist_ref and hist_fit are the reference histogram and the instrument histogram for the fit these are determined using boolean indexing 5.) db_funcs.store_iit_values save the two fit coefficients to database using function store_iit_values creates image combination combo_id of image_ids and dates in Images_Combos table creates association between each image_id and combo_id in Image_Combo_Assoc table creates new method \u201cIIT\u201d with an associated meth_id in Meth_Defs table creates new variable definitions \"alpha and \"x\"\" with an associated var_id in Var_Defs table store variable value as float in Var_Vals table with associated combo_id, meth_id, and var_id","title":"Calculate and Save Correction Coefficients"},{"location":"ipp/iit/#apply-inter-instrument-transformation-and-plot-new-images","text":"This function queries the database for IIT coefficients, applies the correction, and plots resulting images. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def apply_iit_correction ( db_session , hdf_data_dir , iit_query_time_min , iit_query_time_max , inst_list , n_mu_bins , n_intensity_bins , n_images_plot = 1 , plot = False ): \"\"\" function to query IIT correction coefficients, apply correction, and plot resulting images \"\"\" euv_images = db_funcs . query_euv_images ( db_session , time_min = iit_query_time_min , time_max = iit_query_time_max , instrument = ref_instrument ) rot_images = db_funcs . query_euv_images_rot ( db_session , rot_min = rot_min , rot_max = rot_max , instrument = query_instrument ) combo_query_lbc = db_funcs . query_inst_combo ( db_session , iit_query_time_min , iit_query_time_max , lbc_meth_name , instrument ) combo_query_iit = db_funcs . query_inst_combo ( db_session , iit_query_time_min , iit_query_time_max , iit_meth_name , instrument ) original_los , lbcc_image , mu_indices , use_indices , theoretic_query = lbcc_funcs . apply_lbc ( db_session , hdf_data_dir , combo_query_lbc , image_row = row , n_intensity_bins = n_intensity_bins , R0 = R0 ) lbcc_image , iit_image , use_indices , alpha , x = apply_iit ( db_session , hdf_data_dir , combo_query_iit , lbcc_image , use_indices , image_row = row , R0 = R0 ) if plot : Plotting . PlotCorrectedImage ( lbcc_data , los_image = original_los , nfig = 100 + inst_index * 10 + index , title = \"Corrected LBCC Image for \" + instrument ) Plotting . PlotCorrectedImage ( corrected_iit_data , los_image = original_los , nfig = 200 + inst_index * 10 + index , title = \"Corrected IIT Image for \" + instrument ) Plotting . PlotCorrectedImage ( lbcc_data - corrected_iit_data , los_image = original_los , nfig = 300 + inst_index * 10 + index , title = \"Difference Plot for \" + instrument ) 1.) db_funcs.query_euv_images queries database for images (from EUV_Images table) in specified date range queries for reference instrument to get minimum and maximum Carrington rotation 2.) db_funcs.query_euv_images_rot queries for instrument in question based on Carrington rotation range 3.) db_funcs.query_inst_combo queries database for closest image combinations to date observed does this for both the LBC and IIT methods 4.) lbcc_funcs.apply_lbc applies Limb-Brightening Correction to images and creates LBCCImage datatype 5.) apply_iit applies Inter-Instrument Transformation Correction to images and creates IITImage datatype 6.) Plotting.PlotCorrectedImage plots LBC images, IIT corrected images, and the difference between them","title":"Apply Inter-Instrument Transformation and Plot New Images"},{"location":"ipp/iit/#apply-iit","text":"This is a sub-step that applies the Inter-Instrument Transformation Correction to individual image and returns the correct IIT Image. It is called during the third step of Inter-Instrument Transformation. 1 2 3 4 5 6 7 8 9 10 11 12 13 def apply_iit ( db_session , hdf_data_dir , inst_combo_query , lbcc_image , image_row , R0 = 1.01 ): \"\"\" function to apply IIT to a specific image, returns corrected image \"\"\" method_id_info = db_funcs . get_method_id ( db_session , meth_name , meth_desc = None , var_names = None , var_descs = None , create = False ) alpha_x_parameters = db_funcs . query_var_val ( db_session , meth_name , date_obs = lbcc_image . date_obs , inst_combo_query = inst_combo_query ) corrected_iit_data [ use_indices ] = 10 ** ( alpha * np . log10 ( lbcc_data [ use_indices ]) + x ) iit_image = psi_d_types . create_iit_image ( lbcc_image , corrected_iit_data , method_id_info [ 1 ], hdf_path ) return lbcc_image , iit_image , use_indices , alpha , x 1.) db_funcs.get_method_id queries database for method id associated with method name 2.) db_funcs.query_var_val queries database for variable values associated with specific image (from Var_Vals table) 3.) corrected_iit_data[use_indices] = 10 ** (alpha * np.log10(lbcc_data[use_indices]) + x) applies correction to image based off alpha, x, and lbcc corrected data arrays 4.) psi_d_types.create_iit_image create IIT Image datatype from corrected IIT data","title":"Apply IIT"},{"location":"ipp/iit/#generate-histogram-plots","text":"This function generates histogram plots comparing data from before and after the IIT correction. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def plot_iit_histograms ( db_session , hdf_data_dir , hist_query_time_min , hist_query_time_max , inst_list , ref_inst , n_intensity_bins = 200 , lat_band = [ - np . pi / 2.4 , np . pi / 2.4 ], R0 = 1.01 , log10 = True ): \"\"\" function to create corrected IIT histograms then plot original, LBC Corrected, and IIT Corrected histograms for comparison \"\"\" pd_hist = db_funcs . query_hist ( db_session = db_session , meth_id = method_id [ 1 ], n_intensity_bins = n_intensity_bins , lat_band = np . array ( lat_band ) . tobytes (), time_min = hist_query_time_min , time_max = hist_query_time_max ) combo_query_lbc = db_funcs . query_inst_combo ( db_session , hist_query_time_min , hist_query_time_max , meth_name = \"LBCC Theoretic\" , instrument = instrument ) combo_query_iit = db_funcs . query_inst_combo ( db_session , hist_query_time_min , hist_query_time_max , meth_name = \"IIT\" , instrument = instrument ) image_pd = db_funcs . query_euv_images ( db_session = db_session , time_min = hist_query_time_min , time_max = hist_query_time_max , instrument = query_instrument ) original_los , lbcc_image , mu_indices , use_indices , theoretic_query = lbcc_funcs . apply_lbc ( db_session , hdf_data_dir , combo_query_lbc , image_row = row , n_intensity_bins = n_intensity_bins , R0 = R0 ) original_los_hist = psi_d_types . LosImage . iit_hist ( original_los , intensity_bin_edges , lat_band , log10 ) lbcc_image , iit_image , use_indices , alpha , x = apply_iit ( db_session , hdf_data_dir , combo_query_iit , lbcc_image , use_indices , image_row = row , R0 = R0 ) hist_iit = psi_d_types . IITImage . iit_hist ( iit_image , lat_band , log10 ) Plotting . Plot1d_Hist ( norm_original_hist , instrument , inst_index , intensity_bin_edges , color_list , linestyle_list , figure = 100 , xlabel = \"Intensity (log10)\" , ylabel = \"H(I)\" , title = \"Histogram: Original Image\" ) Plotting . Plot1d_Hist ( norm_lbc_hist , instrument , inst_index , intensity_bin_edges , color_list , linestyle_list , figure = 200 , xlabel = \"Intensity (log10)\" , ylabel = \"H(I)\" , title = \"Histogram: Post LBCC\" ) Plotting . Plot1d_Hist ( norm_corrected_hist , instrument , inst_index , intensity_bin_edges , color_list , linestyle_list , figure = 300 , xlabel = \"Intensity (log10)\" , ylabel = \"H(I)\" , title = \"Histogram: Post IIT\" ) 1.) db_funcs.query_hist queries database for histograms (from Histogram table) in specified date range 2.) db_funcs.query_euv_images queries database for images (from EUV_Images table) in specified date range 3.) db_funcs.query_inst_combo queries database for closest image combinations to date observed does this for both the LBC and IIT methods 3.) lbcc_funcs.apply_lbc applies Limb-Brightening Correction to images and creates LBCCImage datatype 4.) psi_d_types.LosImage.iit_hist create 1D IIT Histogram from original LOS image data 5.) apply_iit applies Inter-Instrument Transformation Correction to images and creates IITImage datatype 6.) psi_d_types.IITImage.iit_hist create 1D IIT Histogram from corrected IIT image data 7.) Plotting.Plot1d_Hist plot 1D Normalized IIT Histograms for original, LBC, and IIT data","title":"Generate Histogram Plots"},{"location":"ipp/lbc/","text":"Limb-Brightening Correction Limb Brightening Correction (LBC) is the second step in the data pre-processing pipeline. The goal of LBC is to correct for brightening of structures that is dependent upon their distance from disk center. Examples of Corrected Images These images of before and after applying LBC are from the different instruments on April 1, 2011. These can be enlarged by clicking image titles. AIA Images Original AIA Image Corrected AIA Image Difference AIA Image EUVI-A Images Original STA Image Corrected STA Image Difference STA Image EUVI-B Images Original STB Image Corrected STB Image Difference STB Image Theoretical Analysis Pipeline Compute Histograms and Save to Database This function computes 2D Histograms from processed images for use in the LBC process. It then saves these computed histograms to the database. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 def save_histograms ( db_session , hdf_data_dir , inst_list , hist_query_time_min , hist_query_time_max , n_mu_bins = 18 , n_intensity_bins = 200 , lat_band = [ - np . pi / 64. , np . pi / 64. ], log10 = True , R0 = 1.01 ): \"\"\" function to create and save histograms to database \"\"\" query_pd = db_funcs . query_euv_images ( db_session = db_session , time_min = hist_query_time_min , time_max = hist_query_time_max , instrument = query_instrument ) temp_hist = los_temp . mu_hist ( image_intensity_bin_edges , mu_bin_edges , lat_band = lat_band , log10 = log10 ) hist_lbcc = psi_d_types . create_lbcc_hist ( hdf_path , row . image_id , method_id [ 1 ], mu_bin_edges , image_intensity_bin_edges , lat_band , temp_hist ) db_funcs . add_hist ( db_session , hist_lbcc ) 1.) db_funcs.query_euv_images queries database for images (from EUV_Images table) in specified date range 2.) los_temp.mu_hist creates histogram based on number of mu and intensity bins 3.) psi_d_types.create_lbcc_hist create histogram datatype from lbcc histogram 4.) db_funcs.add_hist saves histograms to database (table Histogram) associating an image_id, meth_id, and basic information with histogram Calculate and Save Theoretical Fit Parameters This function queries histograms from the database then calculates LBC fit parameters which are then saved in the database. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def calc_theoretic_fit ( db_session , inst_list , calc_query_time_min , calc_query_time_max , weekday = 0 , number_of_days = 180 , n_mu_bins = 18 , n_intensity_bins = 200 , lat_band = [ - np . pi / 64. , np . pi / 64. ], create = False ): \"\"\" function to calculate and save (to database) theoretic LBC fit parameters \"\"\" pd_hist = db_funcs . query_hist ( db_session = db_session , meth_id = method_id [ 1 ], n_mu_bins = n_mu_bins , n_intensity_bins = n_intensity_bins , lat_band = np . array ( lat_band ) . tobytes (), time_min = np . datetime64 ( min_date ) . astype ( datetime . datetime ), time_max = np . datetime64 ( max_date ) . astype ( datetime . datetime ), instrument = query_instrument ) optim_out_theo = optim . minimize ( lbcc . get_functional_sse , init_pars , args = ( hist_ref , hist_mat , mu_vec , intensity_bin_array , model ), method = \"BFGS\" ) db_funcs . store_lbcc_values ( db_session , pd_hist , meth_name , meth_desc , var_name , var_desc , date_index , inst_index , optim_vals = optim_vals_theo [ 0 : 6 ], results = results_theo , create = True ) 1.) db_funcs.query_hist queries database for histograms (from Histogram table) in specified date range 2.) optim.minimize use theoretical optimization method to calculate fit parameters 3.) db_funcs.store_lbcc_values save the six fit parameters to database using function store_lbcc_values creates image combination combo_id of image_ids and dates in Images_Combos table creates association between each image_id and combo_id in Image_Combo_Assoc table creates new method \u201cLBCC Theoretic\u201d with an associated meth_id in Meth_Defs table creates new variable definitions \u201cTheoVar\u201d + index with an associated var_id in Var_Defs table store variable value as float in Var_Vals table with associated combo_id, meth_id, and var_id Apply Limb-Brightening Correction and Plot Corrected Images This function queries the database for LBC fit parameters then applies them to specified images, plotting resulting images before and after the correction. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def apply_lbc_correction ( db_session , hdf_data_dir , inst_list , lbc_query_time_min , lbc_query_time_max , n_intensity_bins = 200 , R0 = 1.01 , n_images_plot = 1 , plot = False ): \"\"\" function to apply limb-brightening correction and plot images within a certain time frame \"\"\" image_pd = db_funcs . query_euv_images ( db_session = db_session , time_min = lbc_query_time_min , time_max = lbc_query_time_max , instrument = query_instrument ) combo_query = db_funcs . query_inst_combo ( db_session , lbc_query_time_min , lbc_query_time_max , meth_name , instrument ) original_los , lbcc_image , mu_indices , use_indices = apply_lbc ( db_session , hdf_data_dir , combo_query , image_row = row , n_intensity_bins = n_intensity_bins , R0 = R0 ) if plot : Plotting . PlotImage ( original_los , nfig = 100 + inst_index * 10 + index , title = \"Original LOS Image for \" + instrument ) Plotting . PlotCorrectedImage ( corrected_data = lbcc_image . lbcc_data , los_image = original_los , nfig = 200 + inst_index * 10 + index , title = \"Corrected LBCC Image for \" + instrument ) Plotting . PlotCorrectedImage ( corrected_data = original_los . data - lbcc_image . lbcc_data , los_image = original_los , nfig = 300 + inst_index * 10 + index , title = \"Difference Plot for \" + instrument ) 1.) db_funcs.query_euv_images queries database for images (from EUV_Images table) in specified date range 2.) db_funcs.query_inst_combo queries database for closest image combinations to date observed 3.) db_funcs.query_var_val queries database for variable values associated with specific image (from Var_Vals table) 4.) lbcc.get_beta_y_theoretic_continuous_1d_indices calculates 1d beta and y arrays for valid mu indices uses variable values from query in step two uses original los image to determine indices for correction 5.) corrected_lbc_data[use_indices] = 10 ** (beta1d * np.log10(original_los.data[use_indices]) + y1d) applies correction to image based off beta, y, and original data arrays 6.) Plotting.PlotImage and Plotting.PlotCorrectedImage plots original and corrected images, and difference between them Apply LBC This is a sub-step that applies the Limb-Brightening Corrrection to individual image and returns the correct LBCC Image. It is called during the third step of Limb-Brightening. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def apply_lbc ( db_session , hdf_data_dir , inst_combo_query , image_row , n_intensity_bins = 200 , R0 = 1.01 ): \"\"\" function to apply LBC to a specific image, returns corrected image \"\"\" db_sesh , meth_id , var_ids = db_funcs . get_method_id ( db_session , meth_name , meth_desc = None , var_names = None , var_descs = None , create = False ) original_los = psi_d_types . read_los_image ( hdf_path ) theoretic_query = db_funcs . query_var_val ( db_session , meth_name , date_obs = original_los . info [ 'date_string' ], inst_combo_query = inst_combo_query ) beta1d , y1d , mu_indices , use_indices = lbcc . get_beta_y_theoretic_continuous_1d_indices ( theoretic_query , los_image = original_los ) corrected_lbc_data [ use_indices ] = 10 ** ( beta1d * np . log10 ( original_los . data [ use_indices ]) + y1d ) lbcc_image = psi_d_types . create_lbcc_image ( hdf_path , corrected_lbc_data , image_id = image_row . image_id , meth_id = meth_id , intensity_bin_edges = intensity_bin_edges ) return original_los , lbcc_image , mu_indices , use_indices , theoretic_query 1.) db_funcs.get_method_id queries database for method id associated with method name 2.) psi_d_types.read_los_image reads in los image from database 3.) db_funcs.query_var_val queries database for variable values associated with specific image (from Var_Vals table) 4.) lbcc.get_beta_y_theoretic_continuous_1d_indices calculates 1d beta and y arrays for valid mu indices uses variable values from query in step two uses original los image to determine indices for correction 5.) corrected_lbc_data[use_indices] = 10 ** (beta1d * np.log10(original_los.data[use_indices]) + y1d) applies correction to image based off beta, y, and original data arrays 6.) psi_d_types.create_lbcc_image create LBCC Image datatype from corrected LBC data Generate Plots of Beta and y This function queries the database for LBC fit parameters then generates plots of Beta and y over time. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 def generate_theoretic_plots ( db_session , inst_list , plot_query_time_min , plot_query_time_max , weekday , image_out_path , year = '2011' , time_period = '6 Month' , plot_week = 0 , n_mu_bins = 18 ): \"\"\" function to generate plots of beta/y over time and beta/y v. mu \"\"\" combo_query = db_funcs . query_inst_combo ( db_session , plot_query_time_min , plot_query_time_max , meth_name , instrument ) theoretic_query [ date_index , :] = db_funcs . query_var_val ( db_session , meth_name , date_obs = np . datetime64 ( center_date ) . astype ( datetime . datetime ), inst_combo_query = inst_combo_query ) plot_beta [ mu_index , date_index ], plot_y [ mu_index , date_index ] = lbcc . get_beta_y_theoretic_based ( theoretic_query [ date_index , :], mu ) beta_y_v_mu [ index , :] = lbcc . get_beta_y_theoretic_based ( theoretic_query [ plot_week , :], mu ) 1.) db_funcs.query_inst_combo queries database for closest image combinations to date observed 2.) db_funcs.query_var_val query fit parameters from database 3.) lbcc.get_beta_y_theoretic_based(theoretic_query[date_index, :], mu) calculate beta and y correction coefficients over time using theoretic fit parameters and mu values used for plotting beta and y over time 4.) lbcc.get_beta_y_theoretic_based(theoretic_query[plot_week, :], mu) calculate beta and y correction coefficients for a specific week using theoretic fit parameters and mu values used for plotting beta and y v. mu for a specific week Generate Histogram Plots This function queries the database for histograms and LBC fit parameters then generates plots of histograms before and after the LBC correction. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def generate_histogram_plots ( db_session , hdf_data_dir , inst_list , hist_plot_query_time_min , hist_plot_query_time_max , n_hist_plots = 1 , n_mu_bins = 18 , n_intensity_bins = 200 , lat_band = [ - np . pi / 64. , np . pi / 64. ], log10 = True ): \"\"\" function to generate plots of histograms before and after limb-brightening \"\"\" pd_hist = db_funcs . query_hist ( db_session = db_session , meth_id = method_id [ 1 ], n_mu_bins = n_mu_bins , n_intensity_bins = n_intensity_bins , lat_band = np . array ( lat_band ) . tobytes (), time_min = hist_plot_query_time_min , time_max = hist_plot_query_time_max , instrument = query_instrument ) Plotting . Plot2d_Hist ( plot_hist , date_obs , instrument , intensity_bin_edges , mu_bin_edges , figure , plot_index ) original_los , lbcc_image , mu_indices , use_indices = iit_funcs . apply_lbc_correction ( db_session , hdf_data_dir , instrument , row , n_intensity_bins , R0 ) hist_lbcc = psi_d_types . create_lbcc_hist ( hdf_path , row . image_id , method_id [ 1 ], mu_bin_edges , intensity_bin_edges , lat_band , temp_hist ) Plotting . Plot_LBCC_Hists ( plot_hist , date_obs , instrument , intensity_bin_edges , mu_bin_edges , figure , plot_index ) 1.) db_funcs.query_hist queries database for histograms (from Histogram table) in specified date range 2.) Plotting.Plot2d_Hist plots 2D histogram with plot title and axes labels 3.) iit_funcs.apply_lbc_correction applies Limb-Brightening Correction to images and creates LBCCImage datatype 4.) psi_d_types.create_lbcc_hist create histogram datatype from lbcc histogram 5.) Plotting.Plot_LBCC_Hists plots original and LBC corrected 2D histograms","title":"Limb-Brightening Correction"},{"location":"ipp/lbc/#limb-brightening-correction","text":"Limb Brightening Correction (LBC) is the second step in the data pre-processing pipeline. The goal of LBC is to correct for brightening of structures that is dependent upon their distance from disk center.","title":"Limb-Brightening Correction"},{"location":"ipp/lbc/#examples-of-corrected-images","text":"These images of before and after applying LBC are from the different instruments on April 1, 2011. These can be enlarged by clicking image titles.","title":"Examples of Corrected Images"},{"location":"ipp/lbc/#aia-images","text":"Original AIA Image Corrected AIA Image Difference AIA Image","title":"AIA Images"},{"location":"ipp/lbc/#euvi-a-images","text":"Original STA Image Corrected STA Image Difference STA Image","title":"EUVI-A Images"},{"location":"ipp/lbc/#euvi-b-images","text":"Original STB Image Corrected STB Image Difference STB Image","title":"EUVI-B Images"},{"location":"ipp/lbc/#theoretical-analysis-pipeline","text":"","title":"Theoretical Analysis Pipeline"},{"location":"ipp/lbc/#compute-histograms-and-save-to-database","text":"This function computes 2D Histograms from processed images for use in the LBC process. It then saves these computed histograms to the database. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 def save_histograms ( db_session , hdf_data_dir , inst_list , hist_query_time_min , hist_query_time_max , n_mu_bins = 18 , n_intensity_bins = 200 , lat_band = [ - np . pi / 64. , np . pi / 64. ], log10 = True , R0 = 1.01 ): \"\"\" function to create and save histograms to database \"\"\" query_pd = db_funcs . query_euv_images ( db_session = db_session , time_min = hist_query_time_min , time_max = hist_query_time_max , instrument = query_instrument ) temp_hist = los_temp . mu_hist ( image_intensity_bin_edges , mu_bin_edges , lat_band = lat_band , log10 = log10 ) hist_lbcc = psi_d_types . create_lbcc_hist ( hdf_path , row . image_id , method_id [ 1 ], mu_bin_edges , image_intensity_bin_edges , lat_band , temp_hist ) db_funcs . add_hist ( db_session , hist_lbcc ) 1.) db_funcs.query_euv_images queries database for images (from EUV_Images table) in specified date range 2.) los_temp.mu_hist creates histogram based on number of mu and intensity bins 3.) psi_d_types.create_lbcc_hist create histogram datatype from lbcc histogram 4.) db_funcs.add_hist saves histograms to database (table Histogram) associating an image_id, meth_id, and basic information with histogram","title":"Compute Histograms and Save to Database"},{"location":"ipp/lbc/#calculate-and-save-theoretical-fit-parameters","text":"This function queries histograms from the database then calculates LBC fit parameters which are then saved in the database. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def calc_theoretic_fit ( db_session , inst_list , calc_query_time_min , calc_query_time_max , weekday = 0 , number_of_days = 180 , n_mu_bins = 18 , n_intensity_bins = 200 , lat_band = [ - np . pi / 64. , np . pi / 64. ], create = False ): \"\"\" function to calculate and save (to database) theoretic LBC fit parameters \"\"\" pd_hist = db_funcs . query_hist ( db_session = db_session , meth_id = method_id [ 1 ], n_mu_bins = n_mu_bins , n_intensity_bins = n_intensity_bins , lat_band = np . array ( lat_band ) . tobytes (), time_min = np . datetime64 ( min_date ) . astype ( datetime . datetime ), time_max = np . datetime64 ( max_date ) . astype ( datetime . datetime ), instrument = query_instrument ) optim_out_theo = optim . minimize ( lbcc . get_functional_sse , init_pars , args = ( hist_ref , hist_mat , mu_vec , intensity_bin_array , model ), method = \"BFGS\" ) db_funcs . store_lbcc_values ( db_session , pd_hist , meth_name , meth_desc , var_name , var_desc , date_index , inst_index , optim_vals = optim_vals_theo [ 0 : 6 ], results = results_theo , create = True ) 1.) db_funcs.query_hist queries database for histograms (from Histogram table) in specified date range 2.) optim.minimize use theoretical optimization method to calculate fit parameters 3.) db_funcs.store_lbcc_values save the six fit parameters to database using function store_lbcc_values creates image combination combo_id of image_ids and dates in Images_Combos table creates association between each image_id and combo_id in Image_Combo_Assoc table creates new method \u201cLBCC Theoretic\u201d with an associated meth_id in Meth_Defs table creates new variable definitions \u201cTheoVar\u201d + index with an associated var_id in Var_Defs table store variable value as float in Var_Vals table with associated combo_id, meth_id, and var_id","title":"Calculate and Save Theoretical Fit Parameters"},{"location":"ipp/lbc/#apply-limb-brightening-correction-and-plot-corrected-images","text":"This function queries the database for LBC fit parameters then applies them to specified images, plotting resulting images before and after the correction. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def apply_lbc_correction ( db_session , hdf_data_dir , inst_list , lbc_query_time_min , lbc_query_time_max , n_intensity_bins = 200 , R0 = 1.01 , n_images_plot = 1 , plot = False ): \"\"\" function to apply limb-brightening correction and plot images within a certain time frame \"\"\" image_pd = db_funcs . query_euv_images ( db_session = db_session , time_min = lbc_query_time_min , time_max = lbc_query_time_max , instrument = query_instrument ) combo_query = db_funcs . query_inst_combo ( db_session , lbc_query_time_min , lbc_query_time_max , meth_name , instrument ) original_los , lbcc_image , mu_indices , use_indices = apply_lbc ( db_session , hdf_data_dir , combo_query , image_row = row , n_intensity_bins = n_intensity_bins , R0 = R0 ) if plot : Plotting . PlotImage ( original_los , nfig = 100 + inst_index * 10 + index , title = \"Original LOS Image for \" + instrument ) Plotting . PlotCorrectedImage ( corrected_data = lbcc_image . lbcc_data , los_image = original_los , nfig = 200 + inst_index * 10 + index , title = \"Corrected LBCC Image for \" + instrument ) Plotting . PlotCorrectedImage ( corrected_data = original_los . data - lbcc_image . lbcc_data , los_image = original_los , nfig = 300 + inst_index * 10 + index , title = \"Difference Plot for \" + instrument ) 1.) db_funcs.query_euv_images queries database for images (from EUV_Images table) in specified date range 2.) db_funcs.query_inst_combo queries database for closest image combinations to date observed 3.) db_funcs.query_var_val queries database for variable values associated with specific image (from Var_Vals table) 4.) lbcc.get_beta_y_theoretic_continuous_1d_indices calculates 1d beta and y arrays for valid mu indices uses variable values from query in step two uses original los image to determine indices for correction 5.) corrected_lbc_data[use_indices] = 10 ** (beta1d * np.log10(original_los.data[use_indices]) + y1d) applies correction to image based off beta, y, and original data arrays 6.) Plotting.PlotImage and Plotting.PlotCorrectedImage plots original and corrected images, and difference between them","title":"Apply Limb-Brightening Correction and Plot Corrected Images"},{"location":"ipp/lbc/#apply-lbc","text":"This is a sub-step that applies the Limb-Brightening Corrrection to individual image and returns the correct LBCC Image. It is called during the third step of Limb-Brightening. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def apply_lbc ( db_session , hdf_data_dir , inst_combo_query , image_row , n_intensity_bins = 200 , R0 = 1.01 ): \"\"\" function to apply LBC to a specific image, returns corrected image \"\"\" db_sesh , meth_id , var_ids = db_funcs . get_method_id ( db_session , meth_name , meth_desc = None , var_names = None , var_descs = None , create = False ) original_los = psi_d_types . read_los_image ( hdf_path ) theoretic_query = db_funcs . query_var_val ( db_session , meth_name , date_obs = original_los . info [ 'date_string' ], inst_combo_query = inst_combo_query ) beta1d , y1d , mu_indices , use_indices = lbcc . get_beta_y_theoretic_continuous_1d_indices ( theoretic_query , los_image = original_los ) corrected_lbc_data [ use_indices ] = 10 ** ( beta1d * np . log10 ( original_los . data [ use_indices ]) + y1d ) lbcc_image = psi_d_types . create_lbcc_image ( hdf_path , corrected_lbc_data , image_id = image_row . image_id , meth_id = meth_id , intensity_bin_edges = intensity_bin_edges ) return original_los , lbcc_image , mu_indices , use_indices , theoretic_query 1.) db_funcs.get_method_id queries database for method id associated with method name 2.) psi_d_types.read_los_image reads in los image from database 3.) db_funcs.query_var_val queries database for variable values associated with specific image (from Var_Vals table) 4.) lbcc.get_beta_y_theoretic_continuous_1d_indices calculates 1d beta and y arrays for valid mu indices uses variable values from query in step two uses original los image to determine indices for correction 5.) corrected_lbc_data[use_indices] = 10 ** (beta1d * np.log10(original_los.data[use_indices]) + y1d) applies correction to image based off beta, y, and original data arrays 6.) psi_d_types.create_lbcc_image create LBCC Image datatype from corrected LBC data","title":"Apply LBC"},{"location":"ipp/lbc/#generate-plots-of-beta-and-y","text":"This function queries the database for LBC fit parameters then generates plots of Beta and y over time. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 def generate_theoretic_plots ( db_session , inst_list , plot_query_time_min , plot_query_time_max , weekday , image_out_path , year = '2011' , time_period = '6 Month' , plot_week = 0 , n_mu_bins = 18 ): \"\"\" function to generate plots of beta/y over time and beta/y v. mu \"\"\" combo_query = db_funcs . query_inst_combo ( db_session , plot_query_time_min , plot_query_time_max , meth_name , instrument ) theoretic_query [ date_index , :] = db_funcs . query_var_val ( db_session , meth_name , date_obs = np . datetime64 ( center_date ) . astype ( datetime . datetime ), inst_combo_query = inst_combo_query ) plot_beta [ mu_index , date_index ], plot_y [ mu_index , date_index ] = lbcc . get_beta_y_theoretic_based ( theoretic_query [ date_index , :], mu ) beta_y_v_mu [ index , :] = lbcc . get_beta_y_theoretic_based ( theoretic_query [ plot_week , :], mu ) 1.) db_funcs.query_inst_combo queries database for closest image combinations to date observed 2.) db_funcs.query_var_val query fit parameters from database 3.) lbcc.get_beta_y_theoretic_based(theoretic_query[date_index, :], mu) calculate beta and y correction coefficients over time using theoretic fit parameters and mu values used for plotting beta and y over time 4.) lbcc.get_beta_y_theoretic_based(theoretic_query[plot_week, :], mu) calculate beta and y correction coefficients for a specific week using theoretic fit parameters and mu values used for plotting beta and y v. mu for a specific week","title":"Generate Plots of Beta and y"},{"location":"ipp/lbc/#generate-histogram-plots","text":"This function queries the database for histograms and LBC fit parameters then generates plots of histograms before and after the LBC correction. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def generate_histogram_plots ( db_session , hdf_data_dir , inst_list , hist_plot_query_time_min , hist_plot_query_time_max , n_hist_plots = 1 , n_mu_bins = 18 , n_intensity_bins = 200 , lat_band = [ - np . pi / 64. , np . pi / 64. ], log10 = True ): \"\"\" function to generate plots of histograms before and after limb-brightening \"\"\" pd_hist = db_funcs . query_hist ( db_session = db_session , meth_id = method_id [ 1 ], n_mu_bins = n_mu_bins , n_intensity_bins = n_intensity_bins , lat_band = np . array ( lat_band ) . tobytes (), time_min = hist_plot_query_time_min , time_max = hist_plot_query_time_max , instrument = query_instrument ) Plotting . Plot2d_Hist ( plot_hist , date_obs , instrument , intensity_bin_edges , mu_bin_edges , figure , plot_index ) original_los , lbcc_image , mu_indices , use_indices = iit_funcs . apply_lbc_correction ( db_session , hdf_data_dir , instrument , row , n_intensity_bins , R0 ) hist_lbcc = psi_d_types . create_lbcc_hist ( hdf_path , row . image_id , method_id [ 1 ], mu_bin_edges , intensity_bin_edges , lat_band , temp_hist ) Plotting . Plot_LBCC_Hists ( plot_hist , date_obs , instrument , intensity_bin_edges , mu_bin_edges , figure , plot_index ) 1.) db_funcs.query_hist queries database for histograms (from Histogram table) in specified date range 2.) Plotting.Plot2d_Hist plots 2D histogram with plot title and axes labels 3.) iit_funcs.apply_lbc_correction applies Limb-Brightening Correction to images and creates LBCCImage datatype 4.) psi_d_types.create_lbcc_hist create histogram datatype from lbcc histogram 5.) Plotting.Plot_LBCC_Hists plots original and LBC corrected 2D histograms","title":"Generate Histogram Plots"},{"location":"ipp/psf/","text":"PSF Deconvolution Page regarding PSF deconvolution.","title":"PSF Deconvolution"},{"location":"ipp/psf/#psf-deconvolution","text":"Page regarding PSF deconvolution.","title":"PSF Deconvolution"},{"location":"map/cmb/","text":"Combining Maps Maps are combined using a minimum intensity merge method. Maps are originally created for each instrument image individually then merged. There can be some points of overlap between different instrument maps and this is resolved by taking the data with the minimum intensity from each point of overlap. Coronal Hole data is then chosen based off which data points are used to create the original EUV maps. This method ensures that resulting maps are more continuous at seams. We also use a cutoff mu value to limit limb data distortion. In merging regions of overlap, we use data with a mu value greater than the cutoff value. In areas without overlap, any data available is used (mu cutoff of 0). Combine Maps Function The combine maps function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def combine_maps ( map_list , chd_map_list = None , mu_cutoff = 0.0 , mu_cut_over = None , del_mu = None ): \"\"\" function to combine maps from a list of PsiMap objects based on a mu_cutoff and minimum intensity merge Two methods to determine best minimum intensity merge - using mu cutoff values, or using del mu value return: combined EUV map, combined CHD map \"\"\" map_list [ ii ] . data [ map_list [ ii ] . mu < mu_cutoff ] = map_list [ ii ] . no_data_val if mu_cut_over is not None : overlap [:, :, ii ] = np . logical_and ( data_array [:, :, ii ] != map_list [ 0 ] . no_data_val , data_array [:, :, jj ] != map_list [ 0 ] . no_data_val ) good_index [:, :, ii ] = np . logical_or ( np . logical_and ( overlap [:, :, ii ], mu_array [:, :, ii ] >= mu_cut_over ), np . logical_and ( data_array [:, :, ii ] != map_list [ 0 ] . no_data_val , mu_array [:, :, ii ] >= mu_cutoff )) data_array [ np . logical_not ( good_index )] = float_info . max elif del_mu is not None : good_index [:, :, ii ] = mu_array [:, :, ii ] > ( max_mu - del_mu ) data_array [ np . logical_not ( good_index )] = float_info . max data_array [ data_array == map_list [ 0 ] . no_data_val ] = float_info . max map_index = np . argmin ( data_array , axis = 2 ) keep_data = data_array [ row_index , col_index , map_index ] keep_chd = chd_array [ row_index , col_index , map_index ] chd_map = psi_d_types . PsiMap ( keep_chd , map_list [ 0 ] . x , map_list [ 0 ] . y , mu = keep_mu , origin_image = keep_image , no_data_val = map_list [ 0 ] . no_data_val ) euv_map = psi_d_types . PsiMap ( keep_data , map_list [ 0 ] . x , map_list [ 0 ] . y , mu = keep_mu , origin_image = keep_image , no_data_val = map_list [ 0 ] . no_data_val ) return euv_map , chd_map 1.) map_list[ii].data[map_list[ii].mu < mu_cutoff] = map_list[ii].no_data_val for all pixels with mu < mu_cutoff, set intensity to no_data_val 2.1) np.logical_or(np.logical_and(overlap[:, :, ii], mu_array[:, :, ii] >= mu_cut_over), np.logical_and( data_array[:, :, ii] != map_list[0].no_data_val, mu_array[:, :, ii] >= mu_cutoff)) to determine \"good indices\" based of Caplan et. al. 2016: check for overlap, in areas of overlap choose data where mu > mu_cut_over in areas of no overlap, choose data where mu > mu_cutoff 2.2) mu_array[:, :, ii] > (max_mu - del_mu) determines \"good indices\" based off the value of del_mu 3.) data_array[np.logical_not(good_index)] = float_info.max, data_array[data_array == map_list[0].no_data_val] = float_info.max make poor mu pixels unusable to merge, make no_data_vals unusable to merge 4.) map_index = np.argmin(data_array, axis=2) find minimum intensity of remaining pixels 5.) keep_data = data_array[row_index, col_index, map_index], keep_data = data_array[row_index, col_index, map_index] choose data to use for the EUV and CHD map 6.) psi_d_types.PsiMap create new PsiMap object for both EUV and CHD combined maps","title":"Combining Maps"},{"location":"map/cmb/#combining-maps","text":"Maps are combined using a minimum intensity merge method. Maps are originally created for each instrument image individually then merged. There can be some points of overlap between different instrument maps and this is resolved by taking the data with the minimum intensity from each point of overlap. Coronal Hole data is then chosen based off which data points are used to create the original EUV maps. This method ensures that resulting maps are more continuous at seams. We also use a cutoff mu value to limit limb data distortion. In merging regions of overlap, we use data with a mu value greater than the cutoff value. In areas without overlap, any data available is used (mu cutoff of 0).","title":"Combining Maps"},{"location":"map/cmb/#combine-maps-function","text":"The combine maps function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def combine_maps ( map_list , chd_map_list = None , mu_cutoff = 0.0 , mu_cut_over = None , del_mu = None ): \"\"\" function to combine maps from a list of PsiMap objects based on a mu_cutoff and minimum intensity merge Two methods to determine best minimum intensity merge - using mu cutoff values, or using del mu value return: combined EUV map, combined CHD map \"\"\" map_list [ ii ] . data [ map_list [ ii ] . mu < mu_cutoff ] = map_list [ ii ] . no_data_val if mu_cut_over is not None : overlap [:, :, ii ] = np . logical_and ( data_array [:, :, ii ] != map_list [ 0 ] . no_data_val , data_array [:, :, jj ] != map_list [ 0 ] . no_data_val ) good_index [:, :, ii ] = np . logical_or ( np . logical_and ( overlap [:, :, ii ], mu_array [:, :, ii ] >= mu_cut_over ), np . logical_and ( data_array [:, :, ii ] != map_list [ 0 ] . no_data_val , mu_array [:, :, ii ] >= mu_cutoff )) data_array [ np . logical_not ( good_index )] = float_info . max elif del_mu is not None : good_index [:, :, ii ] = mu_array [:, :, ii ] > ( max_mu - del_mu ) data_array [ np . logical_not ( good_index )] = float_info . max data_array [ data_array == map_list [ 0 ] . no_data_val ] = float_info . max map_index = np . argmin ( data_array , axis = 2 ) keep_data = data_array [ row_index , col_index , map_index ] keep_chd = chd_array [ row_index , col_index , map_index ] chd_map = psi_d_types . PsiMap ( keep_chd , map_list [ 0 ] . x , map_list [ 0 ] . y , mu = keep_mu , origin_image = keep_image , no_data_val = map_list [ 0 ] . no_data_val ) euv_map = psi_d_types . PsiMap ( keep_data , map_list [ 0 ] . x , map_list [ 0 ] . y , mu = keep_mu , origin_image = keep_image , no_data_val = map_list [ 0 ] . no_data_val ) return euv_map , chd_map 1.) map_list[ii].data[map_list[ii].mu < mu_cutoff] = map_list[ii].no_data_val for all pixels with mu < mu_cutoff, set intensity to no_data_val 2.1) np.logical_or(np.logical_and(overlap[:, :, ii], mu_array[:, :, ii] >= mu_cut_over), np.logical_and( data_array[:, :, ii] != map_list[0].no_data_val, mu_array[:, :, ii] >= mu_cutoff)) to determine \"good indices\" based of Caplan et. al. 2016: check for overlap, in areas of overlap choose data where mu > mu_cut_over in areas of no overlap, choose data where mu > mu_cutoff 2.2) mu_array[:, :, ii] > (max_mu - del_mu) determines \"good indices\" based off the value of del_mu 3.) data_array[np.logical_not(good_index)] = float_info.max, data_array[data_array == map_list[0].no_data_val] = float_info.max make poor mu pixels unusable to merge, make no_data_vals unusable to merge 4.) map_index = np.argmin(data_array, axis=2) find minimum intensity of remaining pixels 5.) keep_data = data_array[row_index, col_index, map_index], keep_data = data_array[row_index, col_index, map_index] choose data to use for the EUV and CHD map 6.) psi_d_types.PsiMap create new PsiMap object for both EUV and CHD combined maps","title":"Combine Maps Function"},{"location":"map/int/","text":"Interpolation Currently use linear interpolation, working on other methods utilizing astropy package functionality.","title":"Interpolation"},{"location":"map/int/#interpolation","text":"Currently use linear interpolation, working on other methods utilizing astropy package functionality.","title":"Interpolation"},{"location":"map/map/","text":"Mapping Pipeline After the calculation of the image pre-processing parameters (LBC and IIT), the mapping process undergoes five main steps through which EUV Images are converted to EUV and CHD Maps. 1.) Selecting Images 2.) Apply Pre-Processing Corrections a.) generate moving average dates b.) query for image combos associated with dates c.) apply LBC d.) apply IIT 3.) Coronal Hole Detection 4.) Create Single Instrument Maps 5.) Combine Maps and Save to the Database Mapping Pipeline Functions Select Images The first step in map creation is querying the database for all EUV Images in the relevant time frame and creating a methods dataframe. These functions are database functions and the full code can be found here . 1 2 query_pd = db_funcs . query_euv_images ( db_session = db_session , time_min = query_time_min , time_max = query_time_max ) methods_list = db_funcs . generate_methdf ( query_pd ) 1.) db_funcs.query_euv_images queries the database for EUV Images between time_min and time_max 2.) db_funcs.generate_methdf generates an empty pandas dataframe to later store method information columns hold associated method and variable information Apply Pre-Processing Corrections Limb-Brightening and Inter-Instrument Transformation Corrections are applied to images. Due to memory and storage issues, the rest of the mapping pipeline is applied to images based off date to limit the amount of data stored in memory. Dates for Processing This function creates an array of moving average dates which are looped through to apply corrections. 1 2 3 4 5 6 7 def get_dates ( query_time_min , query_time_max , map_freq ): \"\"\" function to create moving average dates based on hourly frequency of map creation \"\"\" map_frequency = int (( query_time_max - query_time_min ) . seconds / 3600 / map_freq ) moving_avg_centers = np . array ([ np . datetime64 ( str ( query_time_min )) + ii * np . timedelta64 ( map_freq , 'h' ) for ii in range ( map_frequency + 1 )]) return moving_avg_centers 1.) int((query_time_max - query_time_min).seconds / 3600 / map_freq) convert the map_freq integer to hours 2.) np.array(...) create moving average centers array based upon map frequency Query for Image Combos This function creates lists of combo queries for each instrument. It returns lists for LBC and IIT combo queries. 1 2 3 4 5 6 7 8 9 10 def get_inst_combos ( db_session , inst_list , time_min , time_max ): \"\"\" function to create instrument based lists of combo queries for image pre-processing \"\"\" for inst_index , instrument in enumerate ( inst_list ): lbc_combo = db_funcs . query_inst_combo ( db_session , time_min - datetime . timedelta ( days = 180 ), time_max + datetime . timedelta ( days = 180 ), meth_name = 'LBCC' , instrument = instrument ) iit_combo = db_funcs . query_inst_combo ( db_session , time_min - datetime . timedelta ( days = 180 ), time_max + datetime . timedelta ( days = 180 ), meth_name = 'IIT' , instrument = instrument ) lbc_combo_query [ inst_index ] = lbc_combo iit_combo_query [ inst_index ] = iit_combo return lbc_combo_query , iit_combo_query 1.) db_funcs.query_inst_combo queries database for image combinations for specific instrument within the 180 day range does this for both the LBC and IIT methods 2.) lbc_combo_query[inst_index] = lbc_combo add the combo query to the combo query list at the inst_index does this for both the LBC and IIT methods Apply Image Corrections This function applies the image pre-processing corrections to images of the center date in question. It returns a list of processed IIT Images and reference values for Coronal Hole Detection. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def apply_ipp ( db_session , center_date , query_pd , inst_list , hdf_data_dir , lbc_combo_query , iit_combo_query , methods_list , n_intensity_bins = 200 , R0 = 1.01 ): \"\"\" function to apply image pre-processing (limb-brightening, inter-instrument transformation) corrections to EUV images for creation of maps \"\"\" ref_alpha , ref_x = db_funcs . query_var_val ( db_session , meth_name = 'IIT' , date_obs = date_time , inst_combo_query = iit_combo_query [ sta_ind ]) for inst_ind , instrument in enumerate ( inst_list ): los_list [ inst_ind ], lbcc_image , mu_indices , use_ind , theoretic_query = lbcc_funcs . apply_lbc ( db_session , hdf_data_dir , lbc_combo_query [ inst_ind ], image_row = image_row , n_intensity_bins = n_intensity_bins , R0 = R0 ) lbcc_image , iit_list [ inst_ind ], use_indices [ inst_ind ], alpha , x = iit_funcs . apply_iit ( db_session , iit_combo_query [ inst_ind ], lbcc_image , use_ind , los_list [ inst_ind ], R0 = R0 ) ipp_method = { 'meth_name' : ( \"LBCC\" , \"IIT\" ), 'meth_description' :[ \"LBCC Theoretic Fit Method\" , \"IIT Fit Method\" ] , 'var_name' : ( \"LBCC\" , \"IIT\" ), 'var_description' : ( \" \" , \" \" )} methods_list [ inst_ind ] = methods_list [ inst_ind ] . append ( pd . DataFrame ( data = ipp_method ), sort = False ) return date_pd , los_list , iit_list , use_indices , methods_list , ref_alpha , ref_x 1.) db_funcs.query_var_val this is a database function to query variable values ref_alpha and ref_x are the IIT values for the STA Image at this date; these values are used to calculate threshold values for CH Detection 2.) lbcc_funcs.apply_lbc applies Limb-Brightening Correction to images and creates LBCCImage datatype 3.) iit_funcs.apply_iit applies Inter-Instrument Transformation Correction to images and creates IITImage datatype which is added to the iit_list 4.) methods_list[inst_ind].append add the LBC and IIT Correction methods to the methods dataframe Coronal Hole Detection This function applies the Fortran Coronal Hole Detection algorithm and returns a list of CHD Images for mapping. 1 2 3 4 5 6 7 8 9 10 11 def chd ( iit_list , los_list , use_indices , inst_list , thresh1 , thresh2 , ref_alpha , ref_x , nc , iters ): \"\"\" function to apply CHD algorithm and create list of CHD Images from a list of IIT Images \"\"\" for inst_ind , instrument in enumerate ( inst_list ): t1 = thresh1 * ref_alpha + ref_x t2 = thresh2 * ref_alpha + ref_x ezseg_output , iters_used = ezsegwrapper . ezseg ( np . log10 ( image_data ), use_chd , nx , ny , t1 , t2 , nc , iters ) chd_image_list [ inst_ind ] = datatypes . create_chd_image ( los_list [ inst_ind ], chd_result ) return chd_image_list 1.) t1 = thresh1 * ref_alpha + ref_x re-calculate threshold 1 and 2 values based off the EUVI-A IIT values 2.) ezsegwrapper.ezseg call the python wrapper function for the CH Detection algorithm 3.) datatypes.create_chd_image create CHD Image datatype and add to the CHD Image list for mapping Single Maps This function creates single instrument maps from both IIT Images and CHD Images. This mapping is done through linear interpolation onto a Carrington map. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def create_singles_maps ( inst_list , date_pd , iit_list , chd_image_list , methods_list , map_x = None , map_y = None , R0 = 1.01 ): \"\"\" function to map single instrument images to a Carrington map \"\"\" for inst_ind , instrument in enumerate ( inst_list ): map_list [ inst_ind ] = iit_list [ inst_ind ] . interp_to_map ( R0 = R0 , map_x = map_x , map_y = map_y , image_num = image_row . data_id ) chd_map_list [ inst_ind ] = chd_image_list [ inst_ind ] . interp_to_map ( R0 = R0 , map_x = map_x , map_y = map_y , image_num = image_row . data_id ) interp_method = { 'meth_name' : ( \"Im2Map_Lin_Interp_1\" ,), 'meth_description' :[ \"Use SciPy.RegularGridInterpolator() to linearly interpolate from an Image to a Map\" ] * 1 , 'var_name' : ( \"R0\" ,), 'var_description' : ( \"Solar radii\" ,), 'var_val' : ( R0 ,)} methods_list [ inst_ind ] = methods_list [ inst_ind ] . append ( pd . DataFrame ( data = interp_method ), sort = False ) map_list [ inst_ind ] . append_method_info ( methods_list [ inst_ind ]) chd_map_list [ inst_ind ] . append_method_info ( methods_list [ inst_ind ]) return map_list , chd_map_list , methods_list , image_info , map_info 1.) iit_list[inst_ind].interp_to_map, chd_image_list[inst_ind].interp_to_map interpolate IIT corrected, CHD image to Carrington map using linear interpolation 2.) methods_list[inst_ind].append append linear interpolation mapping method to the methods list 3.) map_list[inst_ind].append_method_info, chd_map_list[inst_ind].append_method_info append method information to the both the EUV and CHD map lists Combine Maps This function creates combined EUV and CHD maps from individual instruments maps. Then saves method, map parameter values, and maps to the database. Maps are combined using a Minimum Intensity Merge. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def create_combined_maps ( db_session , map_data_dir , map_list , chd_map_list , methods_list , image_info , map_info , mu_cut_over = None , del_mu = None , mu_cutoff = 0.0 ): \"\"\" function to create combined EUV and CHD maps and save to database with associated method information \"\"\" if del_mu is not None : euv_combined , chd_combined = combine_maps ( euv_maps , chd_maps , del_mu = del_mu , mu_cutoff = mu_cutoff ) combined_method = { 'meth_name' : ( \"Min-Int-Merge_1\" , \"Min-Int-Merge_1\" ), 'meth_description' :[ \"Minimum intensity merge: using del mu\" ] * 2 , 'var_name' : ( \"mu_cutoff\" , \"del_mu\" ), 'var_description' : ( \"lower mu cutoff value\" , \"max acceptable mu range\" ), 'var_val' : ( mu_cutoff , del_mu )} else : euv_combined , chd_combined = combine_maps ( euv_maps , chd_maps , mu_cut_over = mu_cut_over , mu_cutoff = mu_cutoff ) combined_method = { 'meth_name' : ( \"Min-Int-Merge_2\" , \"Min-Int-Merge_2\" ), 'meth_description' :[ \"Minimum intensity merge: based on Caplan et. al.\" ] * 2 , 'var_name' : ( \"mu_cutoff\" , \"mu_cut_over\" ), 'var_description' : ( \"lower mu cutoff value\" , \"mu cutoff value in areas of overlap\" ), 'var_val' : ( mu_cutoff , mu_cut_over )} euv_combined . append_method_info ( methods_list ) euv_combined . append_method_info ( pd . DataFrame ( data = combined_method )) euv_combined . append_image_info ( image_info ) euv_combined . append_map_info ( map_info ) chd_combined . append_method_info ( methods_list ) chd_combined . append_method_info ( pd . DataFrame ( data = combined_method )) chd_combined . append_image_info ( image_info ) chd_combined . append_map_info ( map_info ) Plotting . PlotMap ( euv_combined , nfig = \"EUV Combined map for: \" + str ( euv_combined . image_info . date_obs [ 0 ]), title = \"Minimum Intensity Merge Map \\n Date: \" + str ( euv_combined . image_info . date_obs [ 0 ])) Plotting . PlotMap ( euv_combined , nfig = \"EUV/CHD Combined map for: \" + str ( euv_combined . image_info . date_obs [ 0 ]), title = \"Minimum Intensity EUV/CHD Merge Map \\n Date: \" + str ( euv_combined . image_info . date_obs [ 0 ])) Plotting . PlotMap ( chd_combined , nfig = \"EUV/CHD Combined map for: \" + str ( chd_combined . image_info . date_obs [ 0 ]), title = \"Minimum Intensity EUV/CHD Merge Map \\n Date: \" + str ( chd_combined . image_info . date_obs [ 0 ]), map_type = 'CHD' ) euv_combined . write_to_file ( map_data_dir , map_type = 'synoptic_euv' , filename = None , db_session = db_session ) chd_combined . write_to_file ( map_data_dir , map_type = 'synoptic_chd' , filename = None , db_session = db_session ) return euv_combined , chd_combined 1.) combine_maps function that combines EUV and CHD maps using a minimum intensity merge there are currently two implemented methods for the minimum intensity merge depending on initial input parameters 2.) euv_combined.append_method_info, euv_combined.append_image_info, euv_combined.append_map_info append methods list and combination method information to the both the EUV and CHD combined maps appends image and map info to combined maps, used for database storage 3.) Plotting.PlotMap plot the combined EUV and CHD maps 4.) euv_combined.write_to_file, chd_combined.write_to_file PSI Map function that writes the map to file and saves to the database using function add_map_dbase_record generates filename for map based off base path and map type creates method combination of LBC, IIT, Interpolation, and Minimum Intensity Merge creates Image Combination associated with each method stores map variable values (R0, mu_cutoff, del_mu/mu_cut_over) in database Var Vals Map table stores map information and filename in EUV Maps table","title":"Mapping Pipeline"},{"location":"map/map/#mapping-pipeline","text":"After the calculation of the image pre-processing parameters (LBC and IIT), the mapping process undergoes five main steps through which EUV Images are converted to EUV and CHD Maps. 1.) Selecting Images 2.) Apply Pre-Processing Corrections a.) generate moving average dates b.) query for image combos associated with dates c.) apply LBC d.) apply IIT 3.) Coronal Hole Detection 4.) Create Single Instrument Maps 5.) Combine Maps and Save to the Database","title":"Mapping Pipeline"},{"location":"map/map/#mapping-pipeline-functions","text":"","title":"Mapping Pipeline Functions"},{"location":"map/map/#select-images","text":"The first step in map creation is querying the database for all EUV Images in the relevant time frame and creating a methods dataframe. These functions are database functions and the full code can be found here . 1 2 query_pd = db_funcs . query_euv_images ( db_session = db_session , time_min = query_time_min , time_max = query_time_max ) methods_list = db_funcs . generate_methdf ( query_pd ) 1.) db_funcs.query_euv_images queries the database for EUV Images between time_min and time_max 2.) db_funcs.generate_methdf generates an empty pandas dataframe to later store method information columns hold associated method and variable information","title":"Select Images"},{"location":"map/map/#apply-pre-processing-corrections","text":"Limb-Brightening and Inter-Instrument Transformation Corrections are applied to images. Due to memory and storage issues, the rest of the mapping pipeline is applied to images based off date to limit the amount of data stored in memory.","title":"Apply Pre-Processing Corrections"},{"location":"map/map/#dates-for-processing","text":"This function creates an array of moving average dates which are looped through to apply corrections. 1 2 3 4 5 6 7 def get_dates ( query_time_min , query_time_max , map_freq ): \"\"\" function to create moving average dates based on hourly frequency of map creation \"\"\" map_frequency = int (( query_time_max - query_time_min ) . seconds / 3600 / map_freq ) moving_avg_centers = np . array ([ np . datetime64 ( str ( query_time_min )) + ii * np . timedelta64 ( map_freq , 'h' ) for ii in range ( map_frequency + 1 )]) return moving_avg_centers 1.) int((query_time_max - query_time_min).seconds / 3600 / map_freq) convert the map_freq integer to hours 2.) np.array(...) create moving average centers array based upon map frequency","title":"Dates for Processing"},{"location":"map/map/#query-for-image-combos","text":"This function creates lists of combo queries for each instrument. It returns lists for LBC and IIT combo queries. 1 2 3 4 5 6 7 8 9 10 def get_inst_combos ( db_session , inst_list , time_min , time_max ): \"\"\" function to create instrument based lists of combo queries for image pre-processing \"\"\" for inst_index , instrument in enumerate ( inst_list ): lbc_combo = db_funcs . query_inst_combo ( db_session , time_min - datetime . timedelta ( days = 180 ), time_max + datetime . timedelta ( days = 180 ), meth_name = 'LBCC' , instrument = instrument ) iit_combo = db_funcs . query_inst_combo ( db_session , time_min - datetime . timedelta ( days = 180 ), time_max + datetime . timedelta ( days = 180 ), meth_name = 'IIT' , instrument = instrument ) lbc_combo_query [ inst_index ] = lbc_combo iit_combo_query [ inst_index ] = iit_combo return lbc_combo_query , iit_combo_query 1.) db_funcs.query_inst_combo queries database for image combinations for specific instrument within the 180 day range does this for both the LBC and IIT methods 2.) lbc_combo_query[inst_index] = lbc_combo add the combo query to the combo query list at the inst_index does this for both the LBC and IIT methods","title":"Query for Image Combos"},{"location":"map/map/#apply-image-corrections","text":"This function applies the image pre-processing corrections to images of the center date in question. It returns a list of processed IIT Images and reference values for Coronal Hole Detection. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def apply_ipp ( db_session , center_date , query_pd , inst_list , hdf_data_dir , lbc_combo_query , iit_combo_query , methods_list , n_intensity_bins = 200 , R0 = 1.01 ): \"\"\" function to apply image pre-processing (limb-brightening, inter-instrument transformation) corrections to EUV images for creation of maps \"\"\" ref_alpha , ref_x = db_funcs . query_var_val ( db_session , meth_name = 'IIT' , date_obs = date_time , inst_combo_query = iit_combo_query [ sta_ind ]) for inst_ind , instrument in enumerate ( inst_list ): los_list [ inst_ind ], lbcc_image , mu_indices , use_ind , theoretic_query = lbcc_funcs . apply_lbc ( db_session , hdf_data_dir , lbc_combo_query [ inst_ind ], image_row = image_row , n_intensity_bins = n_intensity_bins , R0 = R0 ) lbcc_image , iit_list [ inst_ind ], use_indices [ inst_ind ], alpha , x = iit_funcs . apply_iit ( db_session , iit_combo_query [ inst_ind ], lbcc_image , use_ind , los_list [ inst_ind ], R0 = R0 ) ipp_method = { 'meth_name' : ( \"LBCC\" , \"IIT\" ), 'meth_description' :[ \"LBCC Theoretic Fit Method\" , \"IIT Fit Method\" ] , 'var_name' : ( \"LBCC\" , \"IIT\" ), 'var_description' : ( \" \" , \" \" )} methods_list [ inst_ind ] = methods_list [ inst_ind ] . append ( pd . DataFrame ( data = ipp_method ), sort = False ) return date_pd , los_list , iit_list , use_indices , methods_list , ref_alpha , ref_x 1.) db_funcs.query_var_val this is a database function to query variable values ref_alpha and ref_x are the IIT values for the STA Image at this date; these values are used to calculate threshold values for CH Detection 2.) lbcc_funcs.apply_lbc applies Limb-Brightening Correction to images and creates LBCCImage datatype 3.) iit_funcs.apply_iit applies Inter-Instrument Transformation Correction to images and creates IITImage datatype which is added to the iit_list 4.) methods_list[inst_ind].append add the LBC and IIT Correction methods to the methods dataframe","title":"Apply Image Corrections"},{"location":"map/map/#coronal-hole-detection","text":"This function applies the Fortran Coronal Hole Detection algorithm and returns a list of CHD Images for mapping. 1 2 3 4 5 6 7 8 9 10 11 def chd ( iit_list , los_list , use_indices , inst_list , thresh1 , thresh2 , ref_alpha , ref_x , nc , iters ): \"\"\" function to apply CHD algorithm and create list of CHD Images from a list of IIT Images \"\"\" for inst_ind , instrument in enumerate ( inst_list ): t1 = thresh1 * ref_alpha + ref_x t2 = thresh2 * ref_alpha + ref_x ezseg_output , iters_used = ezsegwrapper . ezseg ( np . log10 ( image_data ), use_chd , nx , ny , t1 , t2 , nc , iters ) chd_image_list [ inst_ind ] = datatypes . create_chd_image ( los_list [ inst_ind ], chd_result ) return chd_image_list 1.) t1 = thresh1 * ref_alpha + ref_x re-calculate threshold 1 and 2 values based off the EUVI-A IIT values 2.) ezsegwrapper.ezseg call the python wrapper function for the CH Detection algorithm 3.) datatypes.create_chd_image create CHD Image datatype and add to the CHD Image list for mapping","title":"Coronal Hole Detection"},{"location":"map/map/#single-maps","text":"This function creates single instrument maps from both IIT Images and CHD Images. This mapping is done through linear interpolation onto a Carrington map. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def create_singles_maps ( inst_list , date_pd , iit_list , chd_image_list , methods_list , map_x = None , map_y = None , R0 = 1.01 ): \"\"\" function to map single instrument images to a Carrington map \"\"\" for inst_ind , instrument in enumerate ( inst_list ): map_list [ inst_ind ] = iit_list [ inst_ind ] . interp_to_map ( R0 = R0 , map_x = map_x , map_y = map_y , image_num = image_row . data_id ) chd_map_list [ inst_ind ] = chd_image_list [ inst_ind ] . interp_to_map ( R0 = R0 , map_x = map_x , map_y = map_y , image_num = image_row . data_id ) interp_method = { 'meth_name' : ( \"Im2Map_Lin_Interp_1\" ,), 'meth_description' :[ \"Use SciPy.RegularGridInterpolator() to linearly interpolate from an Image to a Map\" ] * 1 , 'var_name' : ( \"R0\" ,), 'var_description' : ( \"Solar radii\" ,), 'var_val' : ( R0 ,)} methods_list [ inst_ind ] = methods_list [ inst_ind ] . append ( pd . DataFrame ( data = interp_method ), sort = False ) map_list [ inst_ind ] . append_method_info ( methods_list [ inst_ind ]) chd_map_list [ inst_ind ] . append_method_info ( methods_list [ inst_ind ]) return map_list , chd_map_list , methods_list , image_info , map_info 1.) iit_list[inst_ind].interp_to_map, chd_image_list[inst_ind].interp_to_map interpolate IIT corrected, CHD image to Carrington map using linear interpolation 2.) methods_list[inst_ind].append append linear interpolation mapping method to the methods list 3.) map_list[inst_ind].append_method_info, chd_map_list[inst_ind].append_method_info append method information to the both the EUV and CHD map lists","title":"Single Maps"},{"location":"map/map/#combine-maps","text":"This function creates combined EUV and CHD maps from individual instruments maps. Then saves method, map parameter values, and maps to the database. Maps are combined using a Minimum Intensity Merge. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def create_combined_maps ( db_session , map_data_dir , map_list , chd_map_list , methods_list , image_info , map_info , mu_cut_over = None , del_mu = None , mu_cutoff = 0.0 ): \"\"\" function to create combined EUV and CHD maps and save to database with associated method information \"\"\" if del_mu is not None : euv_combined , chd_combined = combine_maps ( euv_maps , chd_maps , del_mu = del_mu , mu_cutoff = mu_cutoff ) combined_method = { 'meth_name' : ( \"Min-Int-Merge_1\" , \"Min-Int-Merge_1\" ), 'meth_description' :[ \"Minimum intensity merge: using del mu\" ] * 2 , 'var_name' : ( \"mu_cutoff\" , \"del_mu\" ), 'var_description' : ( \"lower mu cutoff value\" , \"max acceptable mu range\" ), 'var_val' : ( mu_cutoff , del_mu )} else : euv_combined , chd_combined = combine_maps ( euv_maps , chd_maps , mu_cut_over = mu_cut_over , mu_cutoff = mu_cutoff ) combined_method = { 'meth_name' : ( \"Min-Int-Merge_2\" , \"Min-Int-Merge_2\" ), 'meth_description' :[ \"Minimum intensity merge: based on Caplan et. al.\" ] * 2 , 'var_name' : ( \"mu_cutoff\" , \"mu_cut_over\" ), 'var_description' : ( \"lower mu cutoff value\" , \"mu cutoff value in areas of overlap\" ), 'var_val' : ( mu_cutoff , mu_cut_over )} euv_combined . append_method_info ( methods_list ) euv_combined . append_method_info ( pd . DataFrame ( data = combined_method )) euv_combined . append_image_info ( image_info ) euv_combined . append_map_info ( map_info ) chd_combined . append_method_info ( methods_list ) chd_combined . append_method_info ( pd . DataFrame ( data = combined_method )) chd_combined . append_image_info ( image_info ) chd_combined . append_map_info ( map_info ) Plotting . PlotMap ( euv_combined , nfig = \"EUV Combined map for: \" + str ( euv_combined . image_info . date_obs [ 0 ]), title = \"Minimum Intensity Merge Map \\n Date: \" + str ( euv_combined . image_info . date_obs [ 0 ])) Plotting . PlotMap ( euv_combined , nfig = \"EUV/CHD Combined map for: \" + str ( euv_combined . image_info . date_obs [ 0 ]), title = \"Minimum Intensity EUV/CHD Merge Map \\n Date: \" + str ( euv_combined . image_info . date_obs [ 0 ])) Plotting . PlotMap ( chd_combined , nfig = \"EUV/CHD Combined map for: \" + str ( chd_combined . image_info . date_obs [ 0 ]), title = \"Minimum Intensity EUV/CHD Merge Map \\n Date: \" + str ( chd_combined . image_info . date_obs [ 0 ]), map_type = 'CHD' ) euv_combined . write_to_file ( map_data_dir , map_type = 'synoptic_euv' , filename = None , db_session = db_session ) chd_combined . write_to_file ( map_data_dir , map_type = 'synoptic_chd' , filename = None , db_session = db_session ) return euv_combined , chd_combined 1.) combine_maps function that combines EUV and CHD maps using a minimum intensity merge there are currently two implemented methods for the minimum intensity merge depending on initial input parameters 2.) euv_combined.append_method_info, euv_combined.append_image_info, euv_combined.append_map_info append methods list and combination method information to the both the EUV and CHD combined maps appends image and map info to combined maps, used for database storage 3.) Plotting.PlotMap plot the combined EUV and CHD maps 4.) euv_combined.write_to_file, chd_combined.write_to_file PSI Map function that writes the map to file and saves to the database using function add_map_dbase_record generates filename for map based off base path and map type creates method combination of LBC, IIT, Interpolation, and Minimum Intensity Merge creates Image Combination associated with each method stores map variable values (R0, mu_cutoff, del_mu/mu_cut_over) in database Var Vals Map table stores map information and filename in EUV Maps table","title":"Combine Maps"},{"location":"ml/areaoverlap/","text":"Coronal Hole Area Overlap The final step in accurately matching CHs between frames is to evaluate the area overlapping the two regions. Say we have a new coronal hole that was identified in the latest frame and yet to be classified, denoted by S_{0} S_{0} . Let C_{1} C_{1} denote a class of coronal holes where C_{1} = \\{S_{1}, S_{2}, ..., S_{m} \\} C_{1} = \\{S_{1}, S_{2}, ..., S_{m} \\} . Assuming kNN algorithm resulted in a high probability of the new identified coronal hole to be associated with C_{1} C_{1} , we will now measure their area overlap. The area overlap will be measured as a weighted average, so P = \\frac{1}{\\sum_{j=1}^{m} w_{j}} \\sum_{j=1}^{m} \\frac{w_{j}}{2} [\\frac{A(S_{j} \\cap S_{0})}{A(S_{0})} + \\frac{A(S_{j} \\cap S_{0})}{A(S_{j})}] P = \\frac{1}{\\sum_{j=1}^{m} w_{j}} \\sum_{j=1}^{m} \\frac{w_{j}}{2} [\\frac{A(S_{j} \\cap S_{0})}{A(S_{0})} + \\frac{A(S_{j} \\cap S_{0})}{A(S_{j})}] where the weights are denoted by w_{j} = \\frac{1}{f_{j} - f_{0}} w_{j} = \\frac{1}{f_{j} - f_{0}} ; f_{j} f_{j} denotes the frame number of S_{j} S_{j} , and f_{0} f_{0} is the latest frame number. Therefore, the associated weight is related to the coronal hole frame proximity to the new identified S_{0} S_{0} . Then, if the average area overlap is higher than some threshold (\\texttt{AreaMatchThresh}- hyper parameter, Table 1), we classify the new CH as a previously detected CH, otherwise, it will get a new unique ID. In the case where there is high overlap with more than one class, we have the case of CHs merging. The merged CH will get the parent ID with the highest area overlap. However, in the connectivity graph (Sec. Connectivity ) the merged CH will be linked to all parent CH nodes.","title":"Area Overlap"},{"location":"ml/areaoverlap/#coronal-hole-area-overlap","text":"The final step in accurately matching CHs between frames is to evaluate the area overlapping the two regions. Say we have a new coronal hole that was identified in the latest frame and yet to be classified, denoted by S_{0} S_{0} . Let C_{1} C_{1} denote a class of coronal holes where C_{1} = \\{S_{1}, S_{2}, ..., S_{m} \\} C_{1} = \\{S_{1}, S_{2}, ..., S_{m} \\} . Assuming kNN algorithm resulted in a high probability of the new identified coronal hole to be associated with C_{1} C_{1} , we will now measure their area overlap. The area overlap will be measured as a weighted average, so P = \\frac{1}{\\sum_{j=1}^{m} w_{j}} \\sum_{j=1}^{m} \\frac{w_{j}}{2} [\\frac{A(S_{j} \\cap S_{0})}{A(S_{0})} + \\frac{A(S_{j} \\cap S_{0})}{A(S_{j})}] P = \\frac{1}{\\sum_{j=1}^{m} w_{j}} \\sum_{j=1}^{m} \\frac{w_{j}}{2} [\\frac{A(S_{j} \\cap S_{0})}{A(S_{0})} + \\frac{A(S_{j} \\cap S_{0})}{A(S_{j})}] where the weights are denoted by w_{j} = \\frac{1}{f_{j} - f_{0}} w_{j} = \\frac{1}{f_{j} - f_{0}} ; f_{j} f_{j} denotes the frame number of S_{j} S_{j} , and f_{0} f_{0} is the latest frame number. Therefore, the associated weight is related to the coronal hole frame proximity to the new identified S_{0} S_{0} . Then, if the average area overlap is higher than some threshold (\\texttt{AreaMatchThresh}- hyper parameter, Table 1), we classify the new CH as a previously detected CH, otherwise, it will get a new unique ID. In the case where there is high overlap with more than one class, we have the case of CHs merging. The merged CH will get the parent ID with the highest area overlap. However, in the connectivity graph (Sec. Connectivity ) the merged CH will be linked to all parent CH nodes.","title":"Coronal Hole Area Overlap"},{"location":"ml/chd/","text":"Machine Learning Detection of Coronal Holes (and Active Regions) Brute Force Detection Fortran region growing algorithm very dependent on users CH definition subjective to the individual CNN - Supervised Learning to Detect CH labeled image data based on Fortran two-threshold detection algorithm U-Net Convolutional Neural Network built using tensorflow Example Images This is data that has never been seen before by the network. The right most column is the predicted CH detection map using the neural network. Test Data Training Loss Synchronic CH Supervised Detection Map K-Means Clustering unsupervised learning method centroid based clustering algorithm to generate CH/non-CH groups clusters both by intensity and spatial data to ensure connectivity amongst clusters Advantages: unsupervised * no requirement of segmentation masks or labeling easy to create multiple cluster types (find AR as well) * can basically regenerate EUV map \u2014 use for prediction Synchronic CH Unsupervised Detection Map","title":"Machine Learning Detection"},{"location":"ml/chd/#machine-learning-detection-of-coronal-holes-and-active-regions","text":"","title":"Machine Learning Detection of Coronal Holes (and Active Regions)"},{"location":"ml/chd/#brute-force-detection","text":"Fortran region growing algorithm very dependent on users CH definition subjective to the individual","title":"Brute Force Detection"},{"location":"ml/chd/#cnn-supervised-learning-to-detect-ch","text":"labeled image data based on Fortran two-threshold detection algorithm U-Net Convolutional Neural Network built using tensorflow","title":"CNN - Supervised Learning to Detect CH"},{"location":"ml/chd/#example-images","text":"This is data that has never been seen before by the network. The right most column is the predicted CH detection map using the neural network.","title":"Example Images"},{"location":"ml/chd/#test-data","text":"","title":"Test Data"},{"location":"ml/chd/#training-loss","text":"","title":"Training Loss"},{"location":"ml/chd/#synchronic-ch-supervised-detection-map","text":"","title":"Synchronic CH Supervised Detection Map"},{"location":"ml/chd/#k-means-clustering","text":"unsupervised learning method centroid based clustering algorithm to generate CH/non-CH groups clusters both by intensity and spatial data to ensure connectivity amongst clusters Advantages: unsupervised * no requirement of segmentation masks or labeling easy to create multiple cluster types (find AR as well) * can basically regenerate EUV map \u2014 use for prediction","title":"K-Means Clustering"},{"location":"ml/chd/#synchronic-ch-unsupervised-detection-map","text":"","title":"Synchronic CH Unsupervised Detection Map"},{"location":"ml/connectivity/","text":"Connectivity Graph With the purpose of identifying coronal holes that split and merge, we implemented a connectivity graph, where each node is a coronal hole. The edges between nodes are established for two cases: (1) If there is an area overlap with the previous frame set of identified CHs that is greater than \\texttt{ConnectivityThresh} \\texttt{ConnectivityThresh} . (2) If the CH ID has appeared previously in the database, then we draw an edge with its most recent appearance. Therefore, the connectivity graph is a set of connected directed weighted subgraphs, where edge weights corresponds to average ratio area overlap between two nodes. Implementation A Graph G(V, E) is a data structure that is defined by a set of Vertices (or Nodes) (V) and and a set of Edges (E). The coronal hole connectivity graph will be a set of connected weighted subgraphs , where nodes are CH contour objects and edges correspond the area overlap ratio (see areaoverlap ). Plotting Subgraphs In the plot below, the connected subgraphs are ordered hierarchically based on the subgraph average node area. The edge color is based on the edge weight or area ratio, hence, dark edges correspond with strong overlap whereas light edges correspond to a weak overlap.","title":"CH Connectivity"},{"location":"ml/connectivity/#connectivity-graph","text":"With the purpose of identifying coronal holes that split and merge, we implemented a connectivity graph, where each node is a coronal hole. The edges between nodes are established for two cases: (1) If there is an area overlap with the previous frame set of identified CHs that is greater than \\texttt{ConnectivityThresh} \\texttt{ConnectivityThresh} . (2) If the CH ID has appeared previously in the database, then we draw an edge with its most recent appearance. Therefore, the connectivity graph is a set of connected directed weighted subgraphs, where edge weights corresponds to average ratio area overlap between two nodes.","title":"Connectivity Graph"},{"location":"ml/connectivity/#implementation","text":"A Graph G(V, E) is a data structure that is defined by a set of Vertices (or Nodes) (V) and and a set of Edges (E). The coronal hole connectivity graph will be a set of connected weighted subgraphs , where nodes are CH contour objects and edges correspond the area overlap ratio (see areaoverlap ).","title":"Implementation"},{"location":"ml/connectivity/#plotting-subgraphs","text":"In the plot below, the connected subgraphs are ordered hierarchically based on the subgraph average node area. The edge color is based on the edge weight or area ratio, hence, dark edges correspond with strong overlap whereas light edges correspond to a weak overlap.","title":"Plotting Subgraphs"},{"location":"ml/knn/","text":"How to match coronal holes between sequential frames? K - Nearest Neighbor Algorithm KNN algorithm is a simple supervised machine learning algorithm that is used to solve classification problems. KNN is easy to implement and understand. Classification is based on proximity. Here, we will use the coronal hole centroid location to classify its ID number based on previously identified coronal holes. Example of KNN classification The test sample (green circle) should be classified either to red triangle or blue square. Case 1 : If K = 3 (solid line circle) then it is 75% red and 25% being blue. Case 2 : If K = 5 (dashed circle) then the new coronal hole is 60% blue and 40% red. Therefore, in this case when k k =5, the test sample will be classified as blue. However, when k k =3, the test sample is classified as red. Notice that KNN algorithm is sensitive to the hyperparamter k k . Implementation To prune the list of possible CH class associations and reduce the complexity of the proposed tracking algorithm, we utilize k- Nearest Neighbors (kNN) algorithm. kNN algorithm is a simple supervised machine learning algorithm that is used to solve classification problems. Since kNN is supervised all training measurements are labeled. Consequently, in the context of our problem, the CH centroid location are treated as the training dataset along with the corresponding CH labels. The labels indicate the CH class. Let the training dataset be denoted as \\begin{equation} T = \\{(\\bar C_{i}, L_{i}) \\text{ | } i \\in \\mathbb{N} \\text{ and } i \\in [1, n]\\} \\end{equation} \\begin{equation} T = \\{(\\bar C_{i}, L_{i}) \\text{ | } i \\in \\mathbb{N} \\text{ and } i \\in [1, n]\\} \\end{equation} where n n is the number of previously identified CHs in a given window of frames, \\bar C_{i} = (\\bar x_{i}, \\bar y_{i}, \\bar z_{i}) \\bar C_{i} = (\\bar x_{i}, \\bar y_{i}, \\bar z_{i}) is the centroid location in Cartesian coordinates, and L L is the corresponding label. Note that the number of unique labels is less than or equal to the number of centroids in T T . Our goal is to prune the list of possible labels for a newly identified CH, denoted by X X , based on its distance to all other centroids in T T . The distance is measured in Cartesian coordinates \\mathbb{R}^3 \\mathbb{R}^3 and is equipped with the Euclidean norm (||\\cdot ||_{2}) (||\\cdot ||_{2}) . Given the training dataset, let the centroids be reordered based on their distance to X X , such that {\\|C_{1}-X\\|_{2}\\leq \\dots \\leq \\|C_{n}-X\\|_{2}} {\\|C_{1}-X\\|_{2}\\leq \\dots \\leq \\|C_{n}-X\\|_{2}} . Let \\tilde T \\subseteq T \\tilde T \\subseteq T contain the first k k centroids that are of closest proximity to the newly identified X X . The baseline kNN metric is defined as \\begin{equation} P_{knn}(X=L_{i}|\\tilde T) = \\frac{1}{k} \\sum_{j=1}^{k} I(\\tilde T_{j}=L_{i}) \\end{equation} \\begin{equation} P_{knn}(X=L_{i}|\\tilde T) = \\frac{1}{k} \\sum_{j=1}^{k} I(\\tilde T_{j}=L_{i}) \\end{equation} where I I is the indicator function of the subset \\tilde T \\tilde T , which evaluates to 1 when the argument is true and 0 otherwise. The baseline kNN results described above can be sensitive to the choice of the hyper-parameter k k . If k k is too small it can discard important associations, and otherwise, when k k is too large, it will include points that are far from the query centroid ( X X ). To overcome this challenge, we implement a modification of the baseline kNN, namely, the weighted kNN algorithm. The assigned weight is \\frac{1}{d} \\frac{1}{d} where d d is the euclidean distance. \\begin{equation} \\begin{array}{cc} P_{wknn}(X=L_{i}|{\\tilde T}) = \\frac{1}{\\sum_{j=1}^{k} {w_{j}}} \\sum_{j=1}^{k} w_{j} I(\\tilde T_{j}=L_{i}) \\\\ w_{j} = \\frac{1}{\\|X - C_{j}\\|_{2}} \\end{array} \\end{equation} \\begin{equation} \\begin{array}{cc} P_{wknn}(X=L_{i}|{\\tilde T}) = \\frac{1}{\\sum_{j=1}^{k} {w_{j}}} \\sum_{j=1}^{k} w_{j} I(\\tilde T_{j}=L_{i}) \\\\ w_{j} = \\frac{1}{\\|X - C_{j}\\|_{2}} \\end{array} \\end{equation} The list of associated classes is then pruned by a certain threshold \\texttt{kNNThresh} \\approx 0.1 \\texttt{kNNThresh} \\approx 0.1 . Meaning, all class associations where P_{wknn} < 0.1 P_{wknn} < 0.1 are removed from potential class list. KNN Centroid Example The training dataset: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 c # ( lon , lat ) class ================================ 0 [ 3 . 0 . 1 ] red 1 [ 3 . 2 .] red 2 [ 3 . 2 1 . ] red 3 [ 6 . 2 . 64 ] blue 4 [ 5 . 28 2 . 5 ] blue 5 [ 6 . 18 3 .] blue ? [ 4 , 1 ] TO BE COMPUTED Lets compute the distance between [4, 1] and all the 6 centroids in the library (K=6). 1 2 3 4 5 6 7 8 9 10 11 12 13 d c # class 2 0 . 423 red 1 0 . 871 red 3 0 . 926 blue 4 0 . 988 blue 5 1 . 131 blue 0 1 . 18 red Compute the weighted sum probability based on distance. W(X) = [1/1.18, 1/0.871, 1/0.423, 1/0.926, 1/0.988, 1/1.131] W(X) = [1/1.18, 1/0.871, 1/0.423, 1/0.926, 1/0.988, 1/1.131] T_{red} = 1/1.18 + 1/0.871 + 1/0.423 = 4.359 T_{red} = 1/1.18 + 1/0.871 + 1/0.423 = 4.359 T_{blue} = 1/0.926 + 1/0.988 + 1/1.131 = 2.976 T_{blue} = 1/0.926 + 1/0.988 + 1/1.131 = 2.976 P_{red} = \\frac{T_{red}}{T_{red} + T_{blue}} = 0.594 P_{red} = \\frac{T_{red}}{T_{red} + T_{blue}} = 0.594 P_{blue} = \\frac{T_{blue}}{T_{red} + T_{blue}} = 0.405 P_{blue} = \\frac{T_{blue}}{T_{red} + T_{blue}} = 0.405 Predicted probability of [4, 1] is 0.594 and 0.405 for class red and blue, respectively. Since both P_{red} P_{red} and P_{blue} P_{blue} are above \\texttt{kNNThresh} \\texttt{kNNThresh} then the tracking algorithm will proceed to compute the area overlap between both classes to evaluate the class of the test sample.","title":"KNN"},{"location":"ml/knn/#how-to-match-coronal-holes-between-sequential-frames","text":"","title":"How to match coronal holes between sequential frames?"},{"location":"ml/knn/#k-nearest-neighbor-algorithm","text":"KNN algorithm is a simple supervised machine learning algorithm that is used to solve classification problems. KNN is easy to implement and understand. Classification is based on proximity. Here, we will use the coronal hole centroid location to classify its ID number based on previously identified coronal holes.","title":"K - Nearest Neighbor Algorithm"},{"location":"ml/knn/#example-of-knn-classification","text":"The test sample (green circle) should be classified either to red triangle or blue square. Case 1 : If K = 3 (solid line circle) then it is 75% red and 25% being blue. Case 2 : If K = 5 (dashed circle) then the new coronal hole is 60% blue and 40% red. Therefore, in this case when k k =5, the test sample will be classified as blue. However, when k k =3, the test sample is classified as red. Notice that KNN algorithm is sensitive to the hyperparamter k k .","title":"Example of KNN classification"},{"location":"ml/knn/#implementation","text":"To prune the list of possible CH class associations and reduce the complexity of the proposed tracking algorithm, we utilize k- Nearest Neighbors (kNN) algorithm. kNN algorithm is a simple supervised machine learning algorithm that is used to solve classification problems. Since kNN is supervised all training measurements are labeled. Consequently, in the context of our problem, the CH centroid location are treated as the training dataset along with the corresponding CH labels. The labels indicate the CH class. Let the training dataset be denoted as \\begin{equation} T = \\{(\\bar C_{i}, L_{i}) \\text{ | } i \\in \\mathbb{N} \\text{ and } i \\in [1, n]\\} \\end{equation} \\begin{equation} T = \\{(\\bar C_{i}, L_{i}) \\text{ | } i \\in \\mathbb{N} \\text{ and } i \\in [1, n]\\} \\end{equation} where n n is the number of previously identified CHs in a given window of frames, \\bar C_{i} = (\\bar x_{i}, \\bar y_{i}, \\bar z_{i}) \\bar C_{i} = (\\bar x_{i}, \\bar y_{i}, \\bar z_{i}) is the centroid location in Cartesian coordinates, and L L is the corresponding label. Note that the number of unique labels is less than or equal to the number of centroids in T T . Our goal is to prune the list of possible labels for a newly identified CH, denoted by X X , based on its distance to all other centroids in T T . The distance is measured in Cartesian coordinates \\mathbb{R}^3 \\mathbb{R}^3 and is equipped with the Euclidean norm (||\\cdot ||_{2}) (||\\cdot ||_{2}) . Given the training dataset, let the centroids be reordered based on their distance to X X , such that {\\|C_{1}-X\\|_{2}\\leq \\dots \\leq \\|C_{n}-X\\|_{2}} {\\|C_{1}-X\\|_{2}\\leq \\dots \\leq \\|C_{n}-X\\|_{2}} . Let \\tilde T \\subseteq T \\tilde T \\subseteq T contain the first k k centroids that are of closest proximity to the newly identified X X . The baseline kNN metric is defined as \\begin{equation} P_{knn}(X=L_{i}|\\tilde T) = \\frac{1}{k} \\sum_{j=1}^{k} I(\\tilde T_{j}=L_{i}) \\end{equation} \\begin{equation} P_{knn}(X=L_{i}|\\tilde T) = \\frac{1}{k} \\sum_{j=1}^{k} I(\\tilde T_{j}=L_{i}) \\end{equation} where I I is the indicator function of the subset \\tilde T \\tilde T , which evaluates to 1 when the argument is true and 0 otherwise. The baseline kNN results described above can be sensitive to the choice of the hyper-parameter k k . If k k is too small it can discard important associations, and otherwise, when k k is too large, it will include points that are far from the query centroid ( X X ). To overcome this challenge, we implement a modification of the baseline kNN, namely, the weighted kNN algorithm. The assigned weight is \\frac{1}{d} \\frac{1}{d} where d d is the euclidean distance. \\begin{equation} \\begin{array}{cc} P_{wknn}(X=L_{i}|{\\tilde T}) = \\frac{1}{\\sum_{j=1}^{k} {w_{j}}} \\sum_{j=1}^{k} w_{j} I(\\tilde T_{j}=L_{i}) \\\\ w_{j} = \\frac{1}{\\|X - C_{j}\\|_{2}} \\end{array} \\end{equation} \\begin{equation} \\begin{array}{cc} P_{wknn}(X=L_{i}|{\\tilde T}) = \\frac{1}{\\sum_{j=1}^{k} {w_{j}}} \\sum_{j=1}^{k} w_{j} I(\\tilde T_{j}=L_{i}) \\\\ w_{j} = \\frac{1}{\\|X - C_{j}\\|_{2}} \\end{array} \\end{equation} The list of associated classes is then pruned by a certain threshold \\texttt{kNNThresh} \\approx 0.1 \\texttt{kNNThresh} \\approx 0.1 . Meaning, all class associations where P_{wknn} < 0.1 P_{wknn} < 0.1 are removed from potential class list.","title":"Implementation"},{"location":"ml/knn/#knn-centroid-example","text":"The training dataset: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 c # ( lon , lat ) class ================================ 0 [ 3 . 0 . 1 ] red 1 [ 3 . 2 .] red 2 [ 3 . 2 1 . ] red 3 [ 6 . 2 . 64 ] blue 4 [ 5 . 28 2 . 5 ] blue 5 [ 6 . 18 3 .] blue ? [ 4 , 1 ] TO BE COMPUTED Lets compute the distance between [4, 1] and all the 6 centroids in the library (K=6). 1 2 3 4 5 6 7 8 9 10 11 12 13 d c # class 2 0 . 423 red 1 0 . 871 red 3 0 . 926 blue 4 0 . 988 blue 5 1 . 131 blue 0 1 . 18 red Compute the weighted sum probability based on distance. W(X) = [1/1.18, 1/0.871, 1/0.423, 1/0.926, 1/0.988, 1/1.131] W(X) = [1/1.18, 1/0.871, 1/0.423, 1/0.926, 1/0.988, 1/1.131] T_{red} = 1/1.18 + 1/0.871 + 1/0.423 = 4.359 T_{red} = 1/1.18 + 1/0.871 + 1/0.423 = 4.359 T_{blue} = 1/0.926 + 1/0.988 + 1/1.131 = 2.976 T_{blue} = 1/0.926 + 1/0.988 + 1/1.131 = 2.976 P_{red} = \\frac{T_{red}}{T_{red} + T_{blue}} = 0.594 P_{red} = \\frac{T_{red}}{T_{red} + T_{blue}} = 0.594 P_{blue} = \\frac{T_{blue}}{T_{red} + T_{blue}} = 0.405 P_{blue} = \\frac{T_{blue}}{T_{red} + T_{blue}} = 0.405 Predicted probability of [4, 1] is 0.594 and 0.405 for class red and blue, respectively. Since both P_{red} P_{red} and P_{blue} P_{blue} are above \\texttt{kNNThresh} \\texttt{kNNThresh} then the tracking algorithm will proceed to compute the area overlap between both classes to evaluate the class of the test sample.","title":"KNN Centroid Example"},{"location":"ml/latitude_weighted_dilation/","text":"Latitude Weighted Dilation - Overcome Spherical Coordinates Projection Distortion Introduction Mathematical Morphology Definitions Let A A be the original set and B B be the structuring element (also referred to as the kernel). Binary Dilation and Erosion Erosion : A \\ominus B = \\{z| B_{z} \\subseteq A\\} A \\ominus B = \\{z| B_{z} \\subseteq A\\} , s.t. A \\ominus B \\subseteq A A \\ominus B \\subseteq A . Dilation : A \\oplus B = \\{z| B_{z} \\cap A \\subseteq A\\} A \\oplus B = \\{z| B_{z} \\cap A \\subseteq A\\} , s.t. A \\oplus B \\supseteq A A \\oplus B \\supseteq A . For erosion, the structuring element center is marked True if there is a full overlap with structuring element, whereas with dilation, shifted elements have any overlap with original set A A . Greyscale Dilation and Erosion Denoting an image by f(x) f(x) and the structuring function by B B , the grayscale dilation of f f by B B is given by Erosion : f \\ominus B = \\inf_{(k, l) \\in B}{f_{i+k, j+l}} f \\ominus B = \\inf_{(k, l) \\in B}{f_{i+k, j+l}} Dilation : f \\oplus B = \\sup_{(k, l) \\in B}{f_{i+k, j+l}} f \\oplus B = \\sup_{(k, l) \\in B}{f_{i+k, j+l}} Implementation Input Input image is in lat-lon projection and can be greyscaled or a binary image. Latitude Weighted Dilation Apply a latitude weighted dilation to overcome spherical coordinate projection distortion near the poles. By dilating the original image based on latitude, the coronal holes at the poles will be clustered based on distance. The structuring element is a one dimensional kernel whose width depends on latitude, more specifically w(\\theta) = \\begin{array}{cc} \\{ & \\begin{array}{cc} n_{p} & 0 \\leq \\theta \\leq \\alpha \\\\ \\frac{\\gamma}{\\sin(\\theta)} & \\alpha < \\theta < \\beta \\\\ n_{p} & \\beta \\leq \\theta \\leq \\pi \\end{array} \\} \\end{array} w(\\theta) = \\begin{array}{cc} \\{ & \\begin{array}{cc} n_{p} & 0 \\leq \\theta \\leq \\alpha \\\\ \\frac{\\gamma}{\\sin(\\theta)} & \\alpha < \\theta < \\beta \\\\ n_{p} & \\beta \\leq \\theta \\leq \\pi \\end{array} \\} \\end{array} where \\alpha = \\arcsin(\\frac{\\gamma}{n_{p}}) \\alpha = \\arcsin(\\frac{\\gamma}{n_{p}}) , \\beta = \\pi - \\alpha \\beta = \\pi - \\alpha (from symmetry), and \\gamma \\gamma is a hyper parameter. In addition, the dilation in latitude direction is uniform since spacial distortion is only in longitude direction. Figure- Classifying coronal holes using a latitude weighted dilation. (a) The input greyscaled synchronic CH Map. (b) The input image after applying a latitude weighted dilation described by equation (1) and (2). (c) Dilated filled contour image where each coronal hole is associated with a unique color. (d) Lastly, the dilated filled contour image multiplied by the binary input image.","title":"Latitude Weighted Dilation"},{"location":"ml/latitude_weighted_dilation/#latitude-weighted-dilation-overcome-spherical-coordinates-projection-distortion","text":"","title":"Latitude Weighted Dilation - Overcome Spherical Coordinates Projection Distortion"},{"location":"ml/latitude_weighted_dilation/#introduction","text":"","title":"Introduction"},{"location":"ml/latitude_weighted_dilation/#mathematical-morphology-definitions","text":"Let A A be the original set and B B be the structuring element (also referred to as the kernel).","title":"Mathematical Morphology Definitions"},{"location":"ml/latitude_weighted_dilation/#binary-dilation-and-erosion","text":"Erosion : A \\ominus B = \\{z| B_{z} \\subseteq A\\} A \\ominus B = \\{z| B_{z} \\subseteq A\\} , s.t. A \\ominus B \\subseteq A A \\ominus B \\subseteq A . Dilation : A \\oplus B = \\{z| B_{z} \\cap A \\subseteq A\\} A \\oplus B = \\{z| B_{z} \\cap A \\subseteq A\\} , s.t. A \\oplus B \\supseteq A A \\oplus B \\supseteq A . For erosion, the structuring element center is marked True if there is a full overlap with structuring element, whereas with dilation, shifted elements have any overlap with original set A A .","title":"Binary Dilation and Erosion"},{"location":"ml/latitude_weighted_dilation/#greyscale-dilation-and-erosion","text":"Denoting an image by f(x) f(x) and the structuring function by B B , the grayscale dilation of f f by B B is given by Erosion : f \\ominus B = \\inf_{(k, l) \\in B}{f_{i+k, j+l}} f \\ominus B = \\inf_{(k, l) \\in B}{f_{i+k, j+l}} Dilation : f \\oplus B = \\sup_{(k, l) \\in B}{f_{i+k, j+l}} f \\oplus B = \\sup_{(k, l) \\in B}{f_{i+k, j+l}}","title":"Greyscale Dilation and Erosion"},{"location":"ml/latitude_weighted_dilation/#implementation","text":"","title":"Implementation"},{"location":"ml/latitude_weighted_dilation/#input","text":"Input image is in lat-lon projection and can be greyscaled or a binary image.","title":"Input"},{"location":"ml/latitude_weighted_dilation/#latitude-weighted-dilation","text":"Apply a latitude weighted dilation to overcome spherical coordinate projection distortion near the poles. By dilating the original image based on latitude, the coronal holes at the poles will be clustered based on distance. The structuring element is a one dimensional kernel whose width depends on latitude, more specifically w(\\theta) = \\begin{array}{cc} \\{ & \\begin{array}{cc} n_{p} & 0 \\leq \\theta \\leq \\alpha \\\\ \\frac{\\gamma}{\\sin(\\theta)} & \\alpha < \\theta < \\beta \\\\ n_{p} & \\beta \\leq \\theta \\leq \\pi \\end{array} \\} \\end{array} w(\\theta) = \\begin{array}{cc} \\{ & \\begin{array}{cc} n_{p} & 0 \\leq \\theta \\leq \\alpha \\\\ \\frac{\\gamma}{\\sin(\\theta)} & \\alpha < \\theta < \\beta \\\\ n_{p} & \\beta \\leq \\theta \\leq \\pi \\end{array} \\} \\end{array} where \\alpha = \\arcsin(\\frac{\\gamma}{n_{p}}) \\alpha = \\arcsin(\\frac{\\gamma}{n_{p}}) , \\beta = \\pi - \\alpha \\beta = \\pi - \\alpha (from symmetry), and \\gamma \\gamma is a hyper parameter. In addition, the dilation in latitude direction is uniform since spacial distortion is only in longitude direction. Figure- Classifying coronal holes using a latitude weighted dilation. (a) The input greyscaled synchronic CH Map. (b) The input image after applying a latitude weighted dilation described by equation (1) and (2). (c) Dilated filled contour image where each coronal hole is associated with a unique color. (d) Lastly, the dilated filled contour image multiplied by the binary input image.","title":"Latitude Weighted Dilation"},{"location":"ml/match_overview/","text":"Matching Coronal Holes between Frames Algorithm Overview In this project, we leverage machine learning techniques to match coronal holes between sequential frames. As time evolves, the coronal hole remains in proximity to its previous frame location. Therefore, in order to match CHs between sequential frames, we evaluate the CHs centroid (center of mass) and set of pixel location. Since computing the area overlap between two CHs can require intensive computational work, we first prune the list of possible classes by computing the centroid distance between the new CH and the ones identified in the previous frames. The coronal hole classification algorithm is a two step process, where in step 1 , we match CHs based on their centroid location using K-Nearest Neighbors (KNN) , and in the step 2 , we measure the area overlap between the coronal holes of which KNN resulted in a high probability of being associated.","title":"Overview"},{"location":"ml/match_overview/#matching-coronal-holes-between-frames-algorithm-overview","text":"In this project, we leverage machine learning techniques to match coronal holes between sequential frames. As time evolves, the coronal hole remains in proximity to its previous frame location. Therefore, in order to match CHs between sequential frames, we evaluate the CHs centroid (center of mass) and set of pixel location. Since computing the area overlap between two CHs can require intensive computational work, we first prune the list of possible classes by computing the centroid distance between the new CH and the ones identified in the previous frames. The coronal hole classification algorithm is a two step process, where in step 1 , we match CHs based on their centroid location using K-Nearest Neighbors (KNN) , and in the step 2 , we measure the area overlap between the coronal holes of which KNN resulted in a high probability of being associated.","title":"Matching Coronal Holes between Frames Algorithm Overview"},{"location":"ml/polarprojection/","text":"Polar Projection The current images are in latitude - longitude coordinates with the poles placed at the top and bottom of the image. In this current projection, clustering coronal holes at the poles is challenging due to the distortion near the poles. In order to overcome this issue, we suggest a coordinate mapping placing the poles at the equator. This transformation can be done in projection.py module. Mapping Transformation Input: image dimensions (n_{\\theta}, n_{\\phi}) (n_{\\theta}, n_{\\phi}) , where \\theta \\in [0, \\pi] \\theta \\in [0, \\pi] and \\phi \\in [0, 2\\pi] \\phi \\in [0, 2\\pi] . Step 1 : Convert to Cartesian coordinates ( x,y,z x,y,z ). x = \\rho \\sin(\\theta)\\cos(\\phi) x = \\rho \\sin(\\theta)\\cos(\\phi) , y = \\rho \\sin(\\theta)\\sin(\\phi) y = \\rho \\sin(\\theta)\\sin(\\phi) , z = \\rho \\cos(\\theta) z = \\rho \\cos(\\theta) , where \\rho = 1 \\rho = 1 . Step 2 : Rotate about the x axis with \\alpha = \\pi/2 \\alpha = \\pi/2 . Step 3 : Map back to spherical coordinates ( \\theta \\theta , \\phi \\phi ). \\theta = \\arccos(\\frac{z}{\\rho}) = \\arccos(\\sin(\\theta)\\sin(\\phi)) \\theta = \\arccos(\\frac{z}{\\rho}) = \\arccos(\\sin(\\theta)\\sin(\\phi)) , \\phi = \\arctan(\\frac{y}{x}) = \\arctan(\\frac{-\\cos(\\theta)}{\\sin(\\theta)\\cos(\\phi)}) \\phi = \\arctan(\\frac{y}{x}) = \\arctan(\\frac{-\\cos(\\theta)}{\\sin(\\theta)\\cos(\\phi)}) .","title":"Polar Projection"},{"location":"ml/polarprojection/#polar-projection","text":"The current images are in latitude - longitude coordinates with the poles placed at the top and bottom of the image. In this current projection, clustering coronal holes at the poles is challenging due to the distortion near the poles. In order to overcome this issue, we suggest a coordinate mapping placing the poles at the equator. This transformation can be done in projection.py module.","title":"Polar Projection"},{"location":"ml/polarprojection/#mapping-transformation","text":"Input: image dimensions (n_{\\theta}, n_{\\phi}) (n_{\\theta}, n_{\\phi}) , where \\theta \\in [0, \\pi] \\theta \\in [0, \\pi] and \\phi \\in [0, 2\\pi] \\phi \\in [0, 2\\pi] . Step 1 : Convert to Cartesian coordinates ( x,y,z x,y,z ). x = \\rho \\sin(\\theta)\\cos(\\phi) x = \\rho \\sin(\\theta)\\cos(\\phi) , y = \\rho \\sin(\\theta)\\sin(\\phi) y = \\rho \\sin(\\theta)\\sin(\\phi) , z = \\rho \\cos(\\theta) z = \\rho \\cos(\\theta) , where \\rho = 1 \\rho = 1 . Step 2 : Rotate about the x axis with \\alpha = \\pi/2 \\alpha = \\pi/2 . Step 3 : Map back to spherical coordinates ( \\theta \\theta , \\phi \\phi ). \\theta = \\arccos(\\frac{z}{\\rho}) = \\arccos(\\sin(\\theta)\\sin(\\phi)) \\theta = \\arccos(\\frac{z}{\\rho}) = \\arccos(\\sin(\\theta)\\sin(\\phi)) , \\phi = \\arctan(\\frac{y}{x}) = \\arctan(\\frac{-\\cos(\\theta)}{\\sin(\\theta)\\cos(\\phi)}) \\phi = \\arctan(\\frac{y}{x}) = \\arctan(\\frac{-\\cos(\\theta)}{\\sin(\\theta)\\cos(\\phi)}) .","title":"Mapping Transformation"},{"location":"ml/pred/","text":"CH-Net Future Frame Prediction Yet to be discovered/implemented ...","title":"CH-Net Future Frame Predictions"},{"location":"ml/pred/#ch-net-future-frame-prediction","text":"Yet to be discovered/implemented ...","title":"CH-Net Future Frame Prediction"},{"location":"ml/tracking_features/","text":"Centroid Computed in cartesian coordinates and then map back to spherical (weighted average with respect to mesh grid). Area The sum of all pixels area contained in coronal hole. Straight bounding box Bounding rectangle perpendicular to image axis. This feature can measure how the object spreads over time. straight bounding box corners straight bounding box area Rotated bounding box Rectangle with minimum pixel area that contains the coronal hole. This can tell us the approximate tilt of the coronal hole and its convexity. rotated bounding box corners rotated bounding box area rotated bounding box angle - angle with respect to north and largest side. Convex Hull Measure the convexity of the coronal hole by comparing of the convex hull vs the coronal hole area. convex hull set of pixel points. convex hull arclength using the haversine metric. Tilt with respect to north in spherical coordinates. PCA approach - returns angle, tilt significance (eigenvalue ratio). If the eigenvalue ratio is approximately 1 then the tilt is insignificant (circle-like), whereas, when the eigenvalue ratio >>1 then the coronal hole tilt is apparent.","title":"CH Features"},{"location":"ml/tracking_features/#centroid","text":"Computed in cartesian coordinates and then map back to spherical (weighted average with respect to mesh grid).","title":"Centroid"},{"location":"ml/tracking_features/#area","text":"The sum of all pixels area contained in coronal hole.","title":"Area"},{"location":"ml/tracking_features/#straight-bounding-box","text":"Bounding rectangle perpendicular to image axis. This feature can measure how the object spreads over time. straight bounding box corners straight bounding box area","title":"Straight bounding box"},{"location":"ml/tracking_features/#rotated-bounding-box","text":"Rectangle with minimum pixel area that contains the coronal hole. This can tell us the approximate tilt of the coronal hole and its convexity. rotated bounding box corners rotated bounding box area rotated bounding box angle - angle with respect to north and largest side.","title":"Rotated bounding box"},{"location":"ml/tracking_features/#convex-hull","text":"Measure the convexity of the coronal hole by comparing of the convex hull vs the coronal hole area. convex hull set of pixel points. convex hull arclength using the haversine metric.","title":"Convex Hull"},{"location":"ml/tracking_features/#tilt-with-respect-to-north-in-spherical-coordinates","text":"PCA approach - returns angle, tilt significance (eigenvalue ratio). If the eigenvalue ratio is approximately 1 then the tilt is insignificant (circle-like), whereas, when the eigenvalue ratio >>1 then the coronal hole tilt is apparent.","title":"Tilt with respect to north in spherical coordinates."},{"location":"ml/tracking_overview/","text":"Coronal Hole Tracking (CHT) Algorithm Synopsis Solar coronal holes (CHs) are regions of open magnetic fields resulting in streams of relatively fast solar wind. Such coronal holes can impact Earth's magnetosphere and can cause geomagnetic storming. Because of their potential impact, detecting and tracking coronal holes is of great interest. In order to investigate the evolution of well-observed and long-lived CH, we developed a robust method for automatic tracking of CHs between sequential multi-instrument EUV/CH synchronic maps. The tracking algorithm identifies connectivity, open magnetic flux regions, and computes CH features such as area, center of mass, tilt, etc... as a function of time. This tracking algorithm leverages morphological image processing and machine learning techniques. We tested and verified this method on the several time periods and obtained state-of-the-art results. Keywords - Solar Physics, Coronal Holes, Image Processing, Object Tracking, Machine Learning, Computer Vision. Overview Our goal is to automatically track and classify CHs that appear in a video sequence. In addition, we aim to identify both CHs merging and splitting by a connectivity graph. The tracking algorithm consists of four main phases: (1) classifying coronal holes using a latitude weighted dilation, (2) computing coronal hole spatio-temporal features such as area, center of mass, rotated and straight bounding box, convex hull, tilt, etc... (3) matching coronal holes between sequential frames, and (4) coronal hole connectivity graph. Implementation Data structures used in the tracking algorithm.","title":"Overview"},{"location":"ml/tracking_overview/#coronal-hole-tracking-cht-algorithm","text":"","title":"Coronal Hole Tracking (CHT) Algorithm"},{"location":"ml/tracking_overview/#synopsis","text":"Solar coronal holes (CHs) are regions of open magnetic fields resulting in streams of relatively fast solar wind. Such coronal holes can impact Earth's magnetosphere and can cause geomagnetic storming. Because of their potential impact, detecting and tracking coronal holes is of great interest. In order to investigate the evolution of well-observed and long-lived CH, we developed a robust method for automatic tracking of CHs between sequential multi-instrument EUV/CH synchronic maps. The tracking algorithm identifies connectivity, open magnetic flux regions, and computes CH features such as area, center of mass, tilt, etc... as a function of time. This tracking algorithm leverages morphological image processing and machine learning techniques. We tested and verified this method on the several time periods and obtained state-of-the-art results. Keywords - Solar Physics, Coronal Holes, Image Processing, Object Tracking, Machine Learning, Computer Vision.","title":"Synopsis"},{"location":"ml/tracking_overview/#overview","text":"Our goal is to automatically track and classify CHs that appear in a video sequence. In addition, we aim to identify both CHs merging and splitting by a connectivity graph. The tracking algorithm consists of four main phases: (1) classifying coronal holes using a latitude weighted dilation, (2) computing coronal hole spatio-temporal features such as area, center of mass, rotated and straight bounding box, convex hull, tilt, etc... (3) matching coronal holes between sequential frames, and (4) coronal hole connectivity graph.","title":"Overview"},{"location":"ml/tracking_overview/#implementation","text":"Data structures used in the tracking algorithm.","title":"Implementation"}]}