{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Coronal Hole Detection Project For Python files visit the CHD Git repository . Project Outline Data Collection Image Pre-Processing: PSF Deconvolution Limb-Brightening Correction Inter-Instrument Transformation Coronal Hole Detection Mapping Project Pipeline Some sort of graphic here...","title":"Home"},{"location":"#welcome-to-the-coronal-hole-detection-project","text":"For Python files visit the CHD Git repository .","title":"Welcome to the Coronal Hole Detection Project"},{"location":"#project-outline","text":"Data Collection Image Pre-Processing: PSF Deconvolution Limb-Brightening Correction Inter-Instrument Transformation Coronal Hole Detection Mapping","title":"Project Outline"},{"location":"#project-pipeline","text":"Some sort of graphic here...","title":"Project Pipeline"},{"location":"about/","text":"About the Project Contact Cooper Downs: cdowns@predsci.com James Turtle: jturtle@predsci.com Jon Linker: linkerj@predsci.com Tamar Ervin (Intern): tamar@predsci.com","title":"About the Project"},{"location":"about/#about-the-project","text":"","title":"About the Project"},{"location":"about/#contact","text":"Cooper Downs: cdowns@predsci.com James Turtle: jturtle@predsci.com Jon Linker: linkerj@predsci.com Tamar Ervin (Intern): tamar@predsci.com","title":"Contact"},{"location":"chd/chd/","text":"Coronal Hole Detection Coronal Hole Detection is carried out using a two-threshold region growing algorithm. Algorithm 1 2 3 4 5 6 7 8 9 10 11 12 13 \"\"\" python wrapper function for fortran algorithm @param image_data: EUV Image data for detection @param use_chd: matrix of size (nx,ny) which contains 1's where there is valid image data, and non-zero values for areas with invalid/no IMG data. @param nx, ny: image dimensions @param t1, t2: threshold values @param nc: pixel connectivity parameter - number of consecutive pixels needed for connectivity @param iters: maximum number of iterations allowed @return ezseg_output: segmentation map where 0 marks a detection @return iters_used: number of iterations preformed \"\"\" ezseg_output , iters_used = ezsegwrapper . ezseg ( np . log10 ( image_data ), use_chd , nx , ny , t1 , t2 , nc , iters ) 1.) viable pixels are checked to see if the intensity level is below threshold 1 if so, pixel is marked 2.) in each iteration, pixels are checked if their intensity is between threshold 1 and 2, and if they have the required number of connected pixels if so, pixel is marked 3.) continues until no more pixels are marked Example Maps Example minimum intensity merge maps with and without Coronal Hole Detection overlaid. You can click on image titles to enlarge images. Maps with both methods of minimum intensity merge are shown. Minimum Intensity Merge with Two Threshold Mu Cutoff Values April 11, 2011 EUV Map Combined EUV/CHD Map May 15, 2011 EUV Map Combined EUV/CHD Map July 21, 2011 EUV Map Combined EUV/CHD Map August 18, 2011 EUV Map Combined EUV/CHD Map October 1, 2011 EUV Map Combined EUV/CHD Map Minimum Intensity Merge based on Maximum Mu Value April 11, 2011 EUV Map Combined EUV/CHD Map May 15, 2011 EUV Map Combined EUV/CHD Map July 21, 2011 EUV Map Combined EUV/CHD Map August 18, 2011 EUV Map Combined EUV/CHD Map October 1, 2011 EUV Map Combined EUV/CHD Map","title":"CH Detection"},{"location":"chd/chd/#coronal-hole-detection","text":"Coronal Hole Detection is carried out using a two-threshold region growing algorithm.","title":"Coronal Hole Detection"},{"location":"chd/chd/#algorithm","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 \"\"\" python wrapper function for fortran algorithm @param image_data: EUV Image data for detection @param use_chd: matrix of size (nx,ny) which contains 1's where there is valid image data, and non-zero values for areas with invalid/no IMG data. @param nx, ny: image dimensions @param t1, t2: threshold values @param nc: pixel connectivity parameter - number of consecutive pixels needed for connectivity @param iters: maximum number of iterations allowed @return ezseg_output: segmentation map where 0 marks a detection @return iters_used: number of iterations preformed \"\"\" ezseg_output , iters_used = ezsegwrapper . ezseg ( np . log10 ( image_data ), use_chd , nx , ny , t1 , t2 , nc , iters ) 1.) viable pixels are checked to see if the intensity level is below threshold 1 if so, pixel is marked 2.) in each iteration, pixels are checked if their intensity is between threshold 1 and 2, and if they have the required number of connected pixels if so, pixel is marked 3.) continues until no more pixels are marked","title":"Algorithm"},{"location":"chd/chd/#example-maps","text":"Example minimum intensity merge maps with and without Coronal Hole Detection overlaid. You can click on image titles to enlarge images. Maps with both methods of minimum intensity merge are shown.","title":"Example Maps"},{"location":"chd/chd/#minimum-intensity-merge-with-two-threshold-mu-cutoff-values","text":"April 11, 2011 EUV Map Combined EUV/CHD Map May 15, 2011 EUV Map Combined EUV/CHD Map July 21, 2011 EUV Map Combined EUV/CHD Map August 18, 2011 EUV Map Combined EUV/CHD Map October 1, 2011 EUV Map Combined EUV/CHD Map","title":"Minimum Intensity Merge with Two Threshold Mu Cutoff Values"},{"location":"chd/chd/#minimum-intensity-merge-based-on-maximum-mu-value","text":"April 11, 2011 EUV Map Combined EUV/CHD Map May 15, 2011 EUV Map Combined EUV/CHD Map July 21, 2011 EUV Map Combined EUV/CHD Map August 18, 2011 EUV Map Combined EUV/CHD Map October 1, 2011 EUV Map Combined EUV/CHD Map","title":"Minimum Intensity Merge based on Maximum Mu Value"},{"location":"chd/f2py/","text":"Creating Wrapper Function for Ezseg Algorithm How to create a wrapper function for Fortran code: more information can be found here 1.) python -m numpy.f2py ezseg.f -m ezsegwrapper -h ezseg.pyf creates ezseg.pyf file with ezsegwrapper function 2.) cp ezseg.pyf ezsegwrapper.pyf copy file and update to be python compatible 3.) python -m numpy.f2py -c ezsegwrapper.pyf ezseg.f creates shared module ezsegwrapper.so","title":"Fortran to Python"},{"location":"chd/f2py/#creating-wrapper-function-for-ezseg-algorithm","text":"How to create a wrapper function for Fortran code: more information can be found here 1.) python -m numpy.f2py ezseg.f -m ezsegwrapper -h ezseg.pyf creates ezseg.pyf file with ezsegwrapper function 2.) cp ezseg.pyf ezsegwrapper.pyf copy file and update to be python compatible 3.) python -m numpy.f2py -c ezsegwrapper.pyf ezseg.f creates shared module ezsegwrapper.so","title":"Creating Wrapper Function for Ezseg Algorithm"},{"location":"db/db/","text":"Database Information The Image Pre-Processing Pipeline is built upon a database to store images, histograms, and fit parameter values. Updating the Database The original database is now quite different than the database needed to query and save calculated parameters. In order to generate the necessary updates to run the code, do the following: 1.) install the python package Alembic in your python environment conda install -c conda-forge alembic additional installation information can be found here 2.) in the (CHD) project folder, run the script to update the database alembic upgrade head this will run the latest updates to the database 3.) to return to the original database, run the downgrade script alembic downgrade base this will return the database to it's original form 4.) to only run certain upgrades/downgrades run a specific number of revisions (n is the number of revisions to either upgrade/downgrade) alembic upgrade +n alembic downgrade -n run a specific upgrade/downgrade: scripts are found here alembic upgrade \"revision\" alembic downgrade \"revision\" \"revision\" refers to the first 3+ identifying characters from the specific script 5.) to create new update scripts alembic revision -m \"revision name\" edit this script with the upgrade/downgrade necessary Database Schema","title":"Database Basics"},{"location":"db/db/#database-information","text":"The Image Pre-Processing Pipeline is built upon a database to store images, histograms, and fit parameter values.","title":"Database Information"},{"location":"db/db/#updating-the-database","text":"The original database is now quite different than the database needed to query and save calculated parameters. In order to generate the necessary updates to run the code, do the following: 1.) install the python package Alembic in your python environment conda install -c conda-forge alembic additional installation information can be found here 2.) in the (CHD) project folder, run the script to update the database alembic upgrade head this will run the latest updates to the database 3.) to return to the original database, run the downgrade script alembic downgrade base this will return the database to it's original form 4.) to only run certain upgrades/downgrades run a specific number of revisions (n is the number of revisions to either upgrade/downgrade) alembic upgrade +n alembic downgrade -n run a specific upgrade/downgrade: scripts are found here alembic upgrade \"revision\" alembic downgrade \"revision\" \"revision\" refers to the first 3+ identifying characters from the specific script 5.) to create new update scripts alembic revision -m \"revision name\" edit this script with the upgrade/downgrade necessary","title":"Updating the Database"},{"location":"db/db/#database-schema","text":"","title":"Database Schema"},{"location":"db/iit/","text":"Database for Inter-Instrument Transformation For the Inter-Instrument Transformation, the database is used to query EUV Images and LBC Fit Parameters to apply the correction. 1D Intensity Histograms are created and stored in the database. After calculation, fit parameters are stored in the database then queried to apply the IIT Correction. Tables Histogram This table stores histogram and information associated with IIT Histograms. The histograms are created in Step One of Inter-Instrument Transformation Correction. Columns: hist_id: auto-incremented integer id associated with the histogram (Primary Key, Integer) image_id: integer id associated with image (Foreign Key: EUV Images, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) date_obs: time of image observation (DateTime) wavelength: observation wavelength (Integer) n_mu_bins: number of mu bins (Integer) n_intensity_bins: number of intensity bins (Integer) lat_band: latitude band (Blob) mu_bin_edges: array of mu bin edges from number of mu bins (Blob) intensity_bin_edges: array of intensity bin edges from number of intensity bins (Blob) hist: histogram associated with image (Blob) Image Combos This table stores information regarding the combination of images used to calculate the fit parameter. Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) n_images: number of images in combination (Integer) date_mean: mean date of images in image combination (DateTime) date_max: maximum date of images in image combination (DateTime) date_min: minimum date of images in image combination (DateTime) Image Combo Assoc This table stores specific image ids with the associated combo id. Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Foreign Key: Image Combos, Integer) image_id: integer id associated with image (Primary Key, Foreign Key: EUV Images, Integer) Method Defs This table stores information about a correction method and an associated integer method id. Columns: meth_id: auto-incremented integer id associated with the specific method (Primary Key, Integer) meth_name: method name (String) meth_description: description of method (String) Var Defs This table stores information about a variable and an associated integer variable id. Columns: var_id: auto-incremented integer id associated with the specific variable (Primary Key, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) var_name: variable name (String) var_description: description of variable (String) Var Vals This table stores variable values with the associated variable, method, and image combination. These values are calculated from the IIT fit analysis ( IIT Step Two ). These values are queried during the application of the correction ( IIT Step Three ) and during the creation of histogram plots ( IIT Step Four ). Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Foreign Key: Image Combos, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) var_id: auto-incremented integer id associated with the specific variable (Primary Key, Foreign Key: Var Defs, Integer) var_val: variable value (Float)","title":"Database for IIT"},{"location":"db/iit/#database-for-inter-instrument-transformation","text":"For the Inter-Instrument Transformation, the database is used to query EUV Images and LBC Fit Parameters to apply the correction. 1D Intensity Histograms are created and stored in the database. After calculation, fit parameters are stored in the database then queried to apply the IIT Correction.","title":"Database for Inter-Instrument Transformation"},{"location":"db/iit/#tables","text":"","title":"Tables"},{"location":"db/iit/#histogram","text":"This table stores histogram and information associated with IIT Histograms. The histograms are created in Step One of Inter-Instrument Transformation Correction. Columns: hist_id: auto-incremented integer id associated with the histogram (Primary Key, Integer) image_id: integer id associated with image (Foreign Key: EUV Images, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) date_obs: time of image observation (DateTime) wavelength: observation wavelength (Integer) n_mu_bins: number of mu bins (Integer) n_intensity_bins: number of intensity bins (Integer) lat_band: latitude band (Blob) mu_bin_edges: array of mu bin edges from number of mu bins (Blob) intensity_bin_edges: array of intensity bin edges from number of intensity bins (Blob) hist: histogram associated with image (Blob)","title":"Histogram"},{"location":"db/iit/#image-combos","text":"This table stores information regarding the combination of images used to calculate the fit parameter. Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) n_images: number of images in combination (Integer) date_mean: mean date of images in image combination (DateTime) date_max: maximum date of images in image combination (DateTime) date_min: minimum date of images in image combination (DateTime)","title":"Image Combos"},{"location":"db/iit/#image-combo-assoc","text":"This table stores specific image ids with the associated combo id. Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Foreign Key: Image Combos, Integer) image_id: integer id associated with image (Primary Key, Foreign Key: EUV Images, Integer)","title":"Image Combo Assoc"},{"location":"db/iit/#method-defs","text":"This table stores information about a correction method and an associated integer method id. Columns: meth_id: auto-incremented integer id associated with the specific method (Primary Key, Integer) meth_name: method name (String) meth_description: description of method (String)","title":"Method Defs"},{"location":"db/iit/#var-defs","text":"This table stores information about a variable and an associated integer variable id. Columns: var_id: auto-incremented integer id associated with the specific variable (Primary Key, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) var_name: variable name (String) var_description: description of variable (String)","title":"Var Defs"},{"location":"db/iit/#var-vals","text":"This table stores variable values with the associated variable, method, and image combination. These values are calculated from the IIT fit analysis ( IIT Step Two ). These values are queried during the application of the correction ( IIT Step Three ) and during the creation of histogram plots ( IIT Step Four ). Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Foreign Key: Image Combos, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) var_id: auto-incremented integer id associated with the specific variable (Primary Key, Foreign Key: Var Defs, Integer) var_val: variable value (Float)","title":"Var Vals"},{"location":"db/lbc/","text":"Database for Limb-Brightening Correction For the Limb-Brightening Correction, the database is used to query for images, store histograms, and store fit parameter values. These fit parameter values can then be queried in order to apply the Limb-Brightening correction. Tables EUV Images This table stores files and information associated with EUV Images. Columns: image_id: auto-incremented integer id associated with the image (Primary Key, Integer) date_obs: time of image observation (DateTime) instrument: observation instrument (String) wavelength: observation wavelength (Integer) fname_raw: associated fits file (String) fname_hdf: associated hdf5 file (String) distance: associated distance (Float) cr_lon: Carrington Longitude (Float) cr_lat: Carrington Latitude (Float) cr_rot: Carrington Rotation (Float) flag: default 0 (Integer) time_of_download: time of image download to database (DateTime) Histogram This table stores histogram and information associated with LBC Histograms. The histograms are created in Step One of Limb Brightening. Columns: hist_id: auto-incremented integer id associated with the histogram (Primary Key, Integer) image_id: integer id associated with image (Foreign Key: EUV Images, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) date_obs: time of image observation (DateTime) wavelength: observation wavelength (Integer) n_mu_bins: number of mu bins (Integer) n_intensity_bins: number of intensity bins (Integer) lat_band: latitude band (Blob) mu_bin_edges: array of mu bin edges from number of mu bins (Blob) intensity_bin_edges: array of intensity bin edges from number of intensity bins (Blob) hist: histogram associated with image (Blob) Image Combos This table stores information regarding the combination of images used to calculate the fit parameter. Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) n_images: number of images in combination (Integer) date_mean: mean date of images in image combination (DateTime) date_max: maximum date of images in image combination (DateTime) date_min: minimum date of images in image combination (DateTime) Image Combo Assoc This table stores specific image ids with the associated combo id. Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Foreign Key: Image Combos, Integer) image_id: integer id associated with image (Primary Key, Foreign Key: EUV Images, Integer) Method Defs This table stores information about a correction method and an associated integer method id. Columns: meth_id: auto-incremented integer id associated with the specific method (Primary Key, Integer) meth_name: method name (String) meth_description: description of method (String) Var Defs This table stores information about a variable and an associated integer variable id. Columns: var_id: auto-incremented integer id associated with the specific variable (Primary Key, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) var_name: variable name (String) var_description: description of variable (String) Var Vals This table stores variable values with the associated variable, method, and image combination. These values are calculated from the theoretical fit analysis ( LBC Step Two ). These values are queried during the application of the correction ( LBC Step Three ) and during the creation of beta and y plots ( LBC Step Four ). Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Foreign Key: Image Combos, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) var_id: auto-incremented integer id associated with the specific variable (Primary Key, Foreign Key: Var Defs, Integer) var_val: variable value (Float)","title":"Database for LBC"},{"location":"db/lbc/#database-for-limb-brightening-correction","text":"For the Limb-Brightening Correction, the database is used to query for images, store histograms, and store fit parameter values. These fit parameter values can then be queried in order to apply the Limb-Brightening correction.","title":"Database for Limb-Brightening Correction"},{"location":"db/lbc/#tables","text":"","title":"Tables"},{"location":"db/lbc/#euv-images","text":"This table stores files and information associated with EUV Images. Columns: image_id: auto-incremented integer id associated with the image (Primary Key, Integer) date_obs: time of image observation (DateTime) instrument: observation instrument (String) wavelength: observation wavelength (Integer) fname_raw: associated fits file (String) fname_hdf: associated hdf5 file (String) distance: associated distance (Float) cr_lon: Carrington Longitude (Float) cr_lat: Carrington Latitude (Float) cr_rot: Carrington Rotation (Float) flag: default 0 (Integer) time_of_download: time of image download to database (DateTime)","title":"EUV Images"},{"location":"db/lbc/#histogram","text":"This table stores histogram and information associated with LBC Histograms. The histograms are created in Step One of Limb Brightening. Columns: hist_id: auto-incremented integer id associated with the histogram (Primary Key, Integer) image_id: integer id associated with image (Foreign Key: EUV Images, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) date_obs: time of image observation (DateTime) wavelength: observation wavelength (Integer) n_mu_bins: number of mu bins (Integer) n_intensity_bins: number of intensity bins (Integer) lat_band: latitude band (Blob) mu_bin_edges: array of mu bin edges from number of mu bins (Blob) intensity_bin_edges: array of intensity bin edges from number of intensity bins (Blob) hist: histogram associated with image (Blob)","title":"Histogram"},{"location":"db/lbc/#image-combos","text":"This table stores information regarding the combination of images used to calculate the fit parameter. Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) n_images: number of images in combination (Integer) date_mean: mean date of images in image combination (DateTime) date_max: maximum date of images in image combination (DateTime) date_min: minimum date of images in image combination (DateTime)","title":"Image Combos"},{"location":"db/lbc/#image-combo-assoc","text":"This table stores specific image ids with the associated combo id. Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Foreign Key: Image Combos, Integer) image_id: integer id associated with image (Primary Key, Foreign Key: EUV Images, Integer)","title":"Image Combo Assoc"},{"location":"db/lbc/#method-defs","text":"This table stores information about a correction method and an associated integer method id. Columns: meth_id: auto-incremented integer id associated with the specific method (Primary Key, Integer) meth_name: method name (String) meth_description: description of method (String)","title":"Method Defs"},{"location":"db/lbc/#var-defs","text":"This table stores information about a variable and an associated integer variable id. Columns: var_id: auto-incremented integer id associated with the specific variable (Primary Key, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) var_name: variable name (String) var_description: description of variable (String)","title":"Var Defs"},{"location":"db/lbc/#var-vals","text":"This table stores variable values with the associated variable, method, and image combination. These values are calculated from the theoretical fit analysis ( LBC Step Two ). These values are queried during the application of the correction ( LBC Step Three ) and during the creation of beta and y plots ( LBC Step Four ). Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Foreign Key: Image Combos, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) var_id: auto-incremented integer id associated with the specific variable (Primary Key, Foreign Key: Var Defs, Integer) var_val: variable value (Float)","title":"Var Vals"},{"location":"db/map/","text":"Database for Coronal Hole Detection and Mapping For creation of EUV and CHD maps, the database is used to query EUV Images, LBC/IIT Correction Coefficients, and save mapping methods and resulting maps to the database. Tables EUV Images This table stores files and information associated with EUV Images. It is queried to get the original EUV Images before applying Image Pre-Processing (LBC and IIT) Corrections. Columns: image_id: auto-incremented integer id associated with the image (Primary Key, Integer) date_obs: time of image observation (DateTime) instrument: observation instrument (String) wavelength: observation wavelength (Integer) fname_raw: associated fits file (String) fname_hdf: associated hdf5 file (String) distance: associated distance (Float) cr_lon: Carrington Longitude (Float) cr_lat: Carrington Latitude (Float) cr_rot: Carrington Rotation (Float) flag: default 0 (Integer) time_of_download: time of image download to database (DateTime) Image Combos This table stores information regarding the combination of images used to calculate the fit parameter. It is used to determine what combo id corresponds to the date in question. Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) n_images: number of images in combination (Integer) date_mean: mean date of images in image combination (DateTime) date_max: maximum date of images in image combination (DateTime) date_min: minimum date of images in image combination (DateTime) Image Combo Assoc This table stores specific image ids with the associated combo id. It is used when querying for the correct combo id. Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Foreign Key: Image Combos, Integer) image_id: integer id associated with image (Primary Key, Foreign Key: EUV Images, Integer) Var Vals This table stores variable values with the associated variable, method, and image combination. It is queried for the correction parameters used for Limb-Brightening and Inter-Instrument Transformation corrections. Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Foreign Key: Image Combos, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) var_id: auto-incremented integer id associated with the specific variable (Primary Key, Foreign Key: Var Defs, Integer) var_val: variable value (Float) Method Defs This table stores information about a correction method and an associated integer method id, used when querying for correction parameters. Columns: meth_id: auto-incremented integer id associated with the specific method (Primary Key, Integer) meth_name: method name (String) meth_description: description of method (String) Var Defs This table stores information about a variable and an associated integer variable id. It is used when querying for correction parameters to apply LBC/IIT. Columns: var_id: auto-incremented integer id associated with the specific variable (Primary Key, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) var_name: variable name (String) var_description: description of variable (String) Var Vals Map This table stores variable values with the associated map, variable, method, and image combination. Values are saved here when the map is saved to the database. Columns: map_id: auto-incremented interger id associated with specific map (Primary Key, Integer) combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Foreign Key: Image Combos, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) var_id: auto-incremented integer id associated with the specific variable (Primary Key, Foreign Key: Var Defs, Integer) var_val: variable value (Float) Method Combos This table stores information about associated correction methods used in the creation of a map. A new method combination is created when a map is saved to the database. Columns: meth_combo_id: auto-incremented integer id associated with the specific method combination (Primary Key, Integer) n_methods: number of associated methods (Integer) Method Combo Assoc This table associates method combo ids with the method id. Columns: meth_combo_id: auto-incremented integer id associated with the specific method combination (Primary Key, Foreign Key: Method Combos, Integer) meth_id: auto-incremented integer id associated with the specific method (Primary Key, Foreign Key: Method Defs, Integer) EUV Maps This table stores files and information associated with EUV Maps. Columns: map_id: auto-incremented integer id associated with the map (Primary Key, Integer) combo_id: auto-incremented integer id associated with that specific combination of images (Foreign Key: Image Combos, Integer) meth_combo_id: auto-incremented integer id associated with the specific method combination (Primary Key, Foreign Key: Method Combos, Integer) fname: associated hdf5 file, saved either as a 'single' or 'synoptic' map (String) time_of_compute: time of map computation (DateTime)","title":"Database for CHD Mapping"},{"location":"db/map/#database-for-coronal-hole-detection-and-mapping","text":"For creation of EUV and CHD maps, the database is used to query EUV Images, LBC/IIT Correction Coefficients, and save mapping methods and resulting maps to the database.","title":"Database for Coronal Hole Detection and Mapping"},{"location":"db/map/#tables","text":"","title":"Tables"},{"location":"db/map/#euv-images","text":"This table stores files and information associated with EUV Images. It is queried to get the original EUV Images before applying Image Pre-Processing (LBC and IIT) Corrections. Columns: image_id: auto-incremented integer id associated with the image (Primary Key, Integer) date_obs: time of image observation (DateTime) instrument: observation instrument (String) wavelength: observation wavelength (Integer) fname_raw: associated fits file (String) fname_hdf: associated hdf5 file (String) distance: associated distance (Float) cr_lon: Carrington Longitude (Float) cr_lat: Carrington Latitude (Float) cr_rot: Carrington Rotation (Float) flag: default 0 (Integer) time_of_download: time of image download to database (DateTime)","title":"EUV Images"},{"location":"db/map/#image-combos","text":"This table stores information regarding the combination of images used to calculate the fit parameter. It is used to determine what combo id corresponds to the date in question. Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) n_images: number of images in combination (Integer) date_mean: mean date of images in image combination (DateTime) date_max: maximum date of images in image combination (DateTime) date_min: minimum date of images in image combination (DateTime)","title":"Image Combos"},{"location":"db/map/#image-combo-assoc","text":"This table stores specific image ids with the associated combo id. It is used when querying for the correct combo id. Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Foreign Key: Image Combos, Integer) image_id: integer id associated with image (Primary Key, Foreign Key: EUV Images, Integer)","title":"Image Combo Assoc"},{"location":"db/map/#var-vals","text":"This table stores variable values with the associated variable, method, and image combination. It is queried for the correction parameters used for Limb-Brightening and Inter-Instrument Transformation corrections. Columns: combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Foreign Key: Image Combos, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) var_id: auto-incremented integer id associated with the specific variable (Primary Key, Foreign Key: Var Defs, Integer) var_val: variable value (Float)","title":"Var Vals"},{"location":"db/map/#method-defs","text":"This table stores information about a correction method and an associated integer method id, used when querying for correction parameters. Columns: meth_id: auto-incremented integer id associated with the specific method (Primary Key, Integer) meth_name: method name (String) meth_description: description of method (String)","title":"Method Defs"},{"location":"db/map/#var-defs","text":"This table stores information about a variable and an associated integer variable id. It is used when querying for correction parameters to apply LBC/IIT. Columns: var_id: auto-incremented integer id associated with the specific variable (Primary Key, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) var_name: variable name (String) var_description: description of variable (String)","title":"Var Defs"},{"location":"db/map/#var-vals-map","text":"This table stores variable values with the associated map, variable, method, and image combination. Values are saved here when the map is saved to the database. Columns: map_id: auto-incremented interger id associated with specific map (Primary Key, Integer) combo_id: auto-incremented integer id associated with that specific combination of images (Primary Key, Foreign Key: Image Combos, Integer) meth_id: auto-incremented integer id associated with the specific method (Foreign Key: Method Defs, Integer) var_id: auto-incremented integer id associated with the specific variable (Primary Key, Foreign Key: Var Defs, Integer) var_val: variable value (Float)","title":"Var Vals Map"},{"location":"db/map/#method-combos","text":"This table stores information about associated correction methods used in the creation of a map. A new method combination is created when a map is saved to the database. Columns: meth_combo_id: auto-incremented integer id associated with the specific method combination (Primary Key, Integer) n_methods: number of associated methods (Integer)","title":"Method Combos"},{"location":"db/map/#method-combo-assoc","text":"This table associates method combo ids with the method id. Columns: meth_combo_id: auto-incremented integer id associated with the specific method combination (Primary Key, Foreign Key: Method Combos, Integer) meth_id: auto-incremented integer id associated with the specific method (Primary Key, Foreign Key: Method Defs, Integer)","title":"Method Combo Assoc"},{"location":"db/map/#euv-maps","text":"This table stores files and information associated with EUV Maps. Columns: map_id: auto-incremented integer id associated with the map (Primary Key, Integer) combo_id: auto-incremented integer id associated with that specific combination of images (Foreign Key: Image Combos, Integer) meth_combo_id: auto-incremented integer id associated with the specific method combination (Primary Key, Foreign Key: Method Combos, Integer) fname: associated hdf5 file, saved either as a 'single' or 'synoptic' map (String) time_of_compute: time of map computation (DateTime)","title":"EUV Maps"},{"location":"dp/cr/","text":"Synoptic Maps Example Maps for the Month of June 2011 These example maps used data from June 1 to July 1 2011. The functions to create basic full CR EUV/CHD Maps can be found here while the example is here . The example to create mu-dependent CHD maps is found here . CR EUV Map CR CHD Map Mu-Dependent CR CHD Map","title":"Synoptic Maps"},{"location":"dp/cr/#synoptic-maps","text":"","title":"Synoptic Maps"},{"location":"dp/cr/#example-maps-for-the-month-of-june-2011","text":"These example maps used data from June 1 to July 1 2011. The functions to create basic full CR EUV/CHD Maps can be found here while the example is here . The example to create mu-dependent CHD maps is found here .","title":"Example Maps for the Month of June 2011"},{"location":"dp/cr/#cr-euv-map","text":"","title":"CR EUV Map"},{"location":"dp/cr/#cr-chd-map","text":"","title":"CR CHD Map"},{"location":"dp/cr/#mu-dependent-cr-chd-map","text":"","title":"Mu-Dependent CR CHD Map"},{"location":"dp/ed/","text":"Time-Varying Ensemble Detection There are currently two methods of creating time-varying maps. The first creates maps of varying timescale based on user entered intervals around a specific user inputted center date. These maps are then combined with a weighted average - either user inputted, or evenly weighted. The second method uses a Gaussian distribution and a user inputted center date. The closer an image is to the center date, the more weight it has. Images are added one at a time to create the resultant maps . Running Average Maps EUV Map CHD Map Gaussian Time Varying Maps EUV Map CHD Map Code Outline Running Average Maps This outline creates individual combined maps based on user defined timescales (1 day, 1 week, 2 weeks, etc.), then combines the maps based on user defined weighting. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 if timescale_weight is None : timescale_weight = [ 1.0 / len ( timescales )] * len ( timescales ) weight_sum = sum ( timescale_weight ) if weight_sum != 1 : raise ValueError ( \"The timescale weights do not sum to 1. Please reenter weights or change to None for even weighting of maps.\" ) lbc_combo_query , iit_combo_query = chd_funcs . get_inst_combos ( db_session , inst_list , time_min = max_time_min , time_max = max_time_max ) query_pd = db_funcs . query_euv_images ( db_session = db_session , time_min = query_time_min , time_max = query_time_max ) los_image , iit_image , methods_list , use_indices = cr_funcs . apply_ipp ( db_session , hdf_data_dir , inst_list , row , methods_list , lbc_combo_query , iit_combo_query , n_intensity_bins = n_intensity_bins , R0 = R0 ) chd_image = cr_funcs . chd ( db_session , inst_list , los_image , iit_image , use_indices , iit_combo_query , thresh1 = thresh1 , thresh2 = thresh2 , nc = nc , iters = iters ) euv_map , chd_map = cr_funcs . create_map ( iit_image , chd_image , methods_list , row , map_x = map_x , map_y = map_y , R0 = R0 ) euv_timescale [ time_ind ], chd_timescale [ time_ind ], combined_method , chd_combined_method = cr_funcs . cr_map ( euv_map , chd_map , euv_timescale [ time_ind ], chd_timescale [ time_ind ], image_info , map_info , mu_cutoff = mu_cutoff , mu_merge_cutoff = mu_merge_cutoff ) euv_combined , chd_combined , timescale_method = dp_funcs . create_timescale_maps ( euv_timescale , chd_timescale , timescale_weight , image_info_timescale , map_info_timescale ) dp_funcs . save_timescale_maps ( db_session , map_data_dir , euv_combined , chd_combined , image_info_timescale , map_info_timescale , methods_list , combined_method , chd_combined_method , timescale_method ) 1.) timescale_weight = [1.0 / len(timescales)] * len(timescales) if the timescale weights are undefined, create an array of even weights 2.) raise ValueError check that the weights in the array add up to 1 3.) chd_funcs.get_inst_combos query the appropriate combo ids for each instrument based off the maximum time range the user inputted 4.) db_funcs.query_euv_images query the database for images based off the time range and center date 5.) cr_funcs.apply_ipp apply image pre-processing corrections 6.) cr_funcs.chd apply the Coronal Hole Detection algorithm to the image 7.) cr_funcs.create_map convert the image and detection to a map 8.) cr_funcs.cr_map create combined maps using the method for synoptic mapping 9.) dp_funcs.create_timescale_maps combine the timescale maps to create a running average map, based off the timescale combination function 10.) dp_funcs.save_timescale_maps plot and save timescale maps to the database, including the methods combination Gaussian Time Varying Maps This method first creates a gaussian distribution and then weights images based off their closeness to a center date. 1 2 3 4 5 6 7 8 lbc_combo_query , iit_combo_query = chd_funcs . get_inst_combos ( db_session , inst_list , time_min = query_time_min , time_max = query_time_max ) query_pd = db_funcs . query_euv_images ( db_session = db_session , time_min = query_time_min , time_max = query_time_max ) norm_dist = dp_funcs . gauss_time ( query_pd , sigma ) los_image , iit_image , methods_list , use_indices = cr_funcs . apply_ipp ( db_session , hdf_data_dir , inst_list , row , methods_list , lbc_combo_query , iit_combo_query , n_intensity_bins = n_intensity_bins , R0 = R0 ) chd_image = cr_funcs . chd ( db_session , inst_list , los_image , iit_image , use_indices , iit_combo_query , thresh1 = thresh1 , thresh2 = thresh2 , nc = nc , iters = iters ) euv_map , chd_map = cr_funcs . create_map ( iit_image , chd_image , methods_list , row , map_x = map_x , map_y = map_y , R0 = R0 ) euv_combined , chd_combined , sum_wgt , combined_method = dp_funcs . time_wgt_map ( euv_map , chd_map , euv_combined , chd_combined , image_info , weight , sum_wgt , sigma , mu_cutoff ) dp_funcs . save_gauss_time_maps ( db_session , map_data_dir , euv_combined , chd_combined , image_info , map_info , methods_list , combined_method ) 1.) chd_funcs.get_inst_combos query the appropriate combo ids for each instrument based off the maximum time range the user inputted 2.) db_funcs.query_euv_images query the database for images based off the time range and center date 3.) dp_funcs.gauss_time generate a gaussian distribution based off the number of images queried and user defined sigma balue 4.) cr_funcs.apply_ipp apply image pre-processing corrections 5.) cr_funcs.chd apply the Coronal Hole Detection algorithm to the image 6.) cr_funcs.create_map convert the image and detection to a map 6.) dp_funcs.time_wgt_map create combined maps based off the weighting of the Gaussian distribution 7.) dp_funcs.save_gauss_time_maps plot and save time weighted maps to the database, including the methods combination","title":"Ensemble Detection"},{"location":"dp/ed/#time-varying-ensemble-detection","text":"There are currently two methods of creating time-varying maps. The first creates maps of varying timescale based on user entered intervals around a specific user inputted center date. These maps are then combined with a weighted average - either user inputted, or evenly weighted. The second method uses a Gaussian distribution and a user inputted center date. The closer an image is to the center date, the more weight it has. Images are added one at a time to create the resultant maps .","title":"Time-Varying Ensemble Detection"},{"location":"dp/ed/#running-average-maps","text":"EUV Map CHD Map","title":"Running Average Maps"},{"location":"dp/ed/#gaussian-time-varying-maps","text":"EUV Map CHD Map","title":"Gaussian Time Varying Maps"},{"location":"dp/ed/#code-outline","text":"","title":"Code Outline"},{"location":"dp/ed/#running-average-maps_1","text":"This outline creates individual combined maps based on user defined timescales (1 day, 1 week, 2 weeks, etc.), then combines the maps based on user defined weighting. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 if timescale_weight is None : timescale_weight = [ 1.0 / len ( timescales )] * len ( timescales ) weight_sum = sum ( timescale_weight ) if weight_sum != 1 : raise ValueError ( \"The timescale weights do not sum to 1. Please reenter weights or change to None for even weighting of maps.\" ) lbc_combo_query , iit_combo_query = chd_funcs . get_inst_combos ( db_session , inst_list , time_min = max_time_min , time_max = max_time_max ) query_pd = db_funcs . query_euv_images ( db_session = db_session , time_min = query_time_min , time_max = query_time_max ) los_image , iit_image , methods_list , use_indices = cr_funcs . apply_ipp ( db_session , hdf_data_dir , inst_list , row , methods_list , lbc_combo_query , iit_combo_query , n_intensity_bins = n_intensity_bins , R0 = R0 ) chd_image = cr_funcs . chd ( db_session , inst_list , los_image , iit_image , use_indices , iit_combo_query , thresh1 = thresh1 , thresh2 = thresh2 , nc = nc , iters = iters ) euv_map , chd_map = cr_funcs . create_map ( iit_image , chd_image , methods_list , row , map_x = map_x , map_y = map_y , R0 = R0 ) euv_timescale [ time_ind ], chd_timescale [ time_ind ], combined_method , chd_combined_method = cr_funcs . cr_map ( euv_map , chd_map , euv_timescale [ time_ind ], chd_timescale [ time_ind ], image_info , map_info , mu_cutoff = mu_cutoff , mu_merge_cutoff = mu_merge_cutoff ) euv_combined , chd_combined , timescale_method = dp_funcs . create_timescale_maps ( euv_timescale , chd_timescale , timescale_weight , image_info_timescale , map_info_timescale ) dp_funcs . save_timescale_maps ( db_session , map_data_dir , euv_combined , chd_combined , image_info_timescale , map_info_timescale , methods_list , combined_method , chd_combined_method , timescale_method ) 1.) timescale_weight = [1.0 / len(timescales)] * len(timescales) if the timescale weights are undefined, create an array of even weights 2.) raise ValueError check that the weights in the array add up to 1 3.) chd_funcs.get_inst_combos query the appropriate combo ids for each instrument based off the maximum time range the user inputted 4.) db_funcs.query_euv_images query the database for images based off the time range and center date 5.) cr_funcs.apply_ipp apply image pre-processing corrections 6.) cr_funcs.chd apply the Coronal Hole Detection algorithm to the image 7.) cr_funcs.create_map convert the image and detection to a map 8.) cr_funcs.cr_map create combined maps using the method for synoptic mapping 9.) dp_funcs.create_timescale_maps combine the timescale maps to create a running average map, based off the timescale combination function 10.) dp_funcs.save_timescale_maps plot and save timescale maps to the database, including the methods combination","title":"Running Average Maps"},{"location":"dp/ed/#gaussian-time-varying-maps_1","text":"This method first creates a gaussian distribution and then weights images based off their closeness to a center date. 1 2 3 4 5 6 7 8 lbc_combo_query , iit_combo_query = chd_funcs . get_inst_combos ( db_session , inst_list , time_min = query_time_min , time_max = query_time_max ) query_pd = db_funcs . query_euv_images ( db_session = db_session , time_min = query_time_min , time_max = query_time_max ) norm_dist = dp_funcs . gauss_time ( query_pd , sigma ) los_image , iit_image , methods_list , use_indices = cr_funcs . apply_ipp ( db_session , hdf_data_dir , inst_list , row , methods_list , lbc_combo_query , iit_combo_query , n_intensity_bins = n_intensity_bins , R0 = R0 ) chd_image = cr_funcs . chd ( db_session , inst_list , los_image , iit_image , use_indices , iit_combo_query , thresh1 = thresh1 , thresh2 = thresh2 , nc = nc , iters = iters ) euv_map , chd_map = cr_funcs . create_map ( iit_image , chd_image , methods_list , row , map_x = map_x , map_y = map_y , R0 = R0 ) euv_combined , chd_combined , sum_wgt , combined_method = dp_funcs . time_wgt_map ( euv_map , chd_map , euv_combined , chd_combined , image_info , weight , sum_wgt , sigma , mu_cutoff ) dp_funcs . save_gauss_time_maps ( db_session , map_data_dir , euv_combined , chd_combined , image_info , map_info , methods_list , combined_method ) 1.) chd_funcs.get_inst_combos query the appropriate combo ids for each instrument based off the maximum time range the user inputted 2.) db_funcs.query_euv_images query the database for images based off the time range and center date 3.) dp_funcs.gauss_time generate a gaussian distribution based off the number of images queried and user defined sigma balue 4.) cr_funcs.apply_ipp apply image pre-processing corrections 5.) cr_funcs.chd apply the Coronal Hole Detection algorithm to the image 6.) cr_funcs.create_map convert the image and detection to a map 6.) dp_funcs.time_wgt_map create combined maps based off the weighting of the Gaussian distribution 7.) dp_funcs.save_gauss_time_maps plot and save time weighted maps to the database, including the methods combination","title":"Gaussian Time Varying Maps"},{"location":"dp/qm/","text":"Quality Assurance Maps The goal of these maps is to determine where data at each pixel came from, and the mu value of the origin image at that point. Example Maps Synchronic Maps on June 18, 2011 EUV Map Quality EUV Map CHD Map Quality CHD Map Full CR EUV Map Quality CR EUV Map Code Outline 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def quality_map ( db_session , map_data_dir , inst_list , query_pd , euv_combined , chd_combined = None , color_list = None ): euv_origin_image = euv_combined . origin_image euv_origins = np . unique ( euv_origin_image ) euv_image = np . empty ( euv_origin_image . shape , dtype = object ) for euv_id in euv_origins : query_ind = np . where ( query_pd [ 'image_id' ] == euv_id ) instrument = query_pd [ 'instrument' ] . iloc [ query_ind [ 0 ]] if len ( instrument ) != 0 : euv_image = np . where ( euv_origin_image != euv_id , euv_image , instrument . iloc [ 0 ]) Plotting . PlotQualityMap ( euv_combined , euv_image , inst_list , color_list , nfig = 'EUV Quality Map ' + str ( euv_combined . image_info . date_obs [ 0 ]), title = 'EUV Quality Map: Mu Dependent \\n ' + str ( euv_combined . image_info . date_obs [ 0 ])) if chd_combined is not None : chd_origin_image = chd_combined . origin_image chd_origins = np . unique ( chd_origin_image ) chd_image = np . empty ( chd_origin_image . shape , dtype = object ) for chd_id in chd_origins : query_ind = np . where ( query_pd [ 'image_id' ] == chd_id ) instrument = query_pd [ 'instrument' ] . iloc [ query_ind [ 0 ]] if len ( instrument ) != 0 : chd_image = np . where ( euv_origin_image != chd_id , chd_image , instrument . iloc [ 0 ]) Plotting . PlotQualityMap ( chd_combined , chd_image , inst_list , color_list , nfig = 'CHD Quality Map ' + str ( chd_combined . image_info . date_obs [ 0 ]), title = 'CHD Quality Map: Mu Dependent \\n ' + str ( chd_combined . image_info . date_obs [ 0 ]), map_type = 'CHD' ) # save these maps to database return None 1.) query_ind = np.where(query_pd['image_id'] == euv_id) loop through the unique list of image ids and determine at what indices they are present 2.) euv_image = np.where(euv_origin_image != euv_id, euv_image, instrument.iloc[0]) add instrument name to array in correct pixel position 3.) Plotting.PlotQualityMap plot a quality map based off instrument and mu value of the final map","title":"Quality Maps"},{"location":"dp/qm/#quality-assurance-maps","text":"The goal of these maps is to determine where data at each pixel came from, and the mu value of the origin image at that point.","title":"Quality Assurance Maps"},{"location":"dp/qm/#example-maps","text":"","title":"Example Maps"},{"location":"dp/qm/#synchronic-maps-on-june-18-2011","text":"EUV Map Quality EUV Map CHD Map Quality CHD Map Full CR EUV Map Quality CR EUV Map","title":"Synchronic Maps on June 18, 2011"},{"location":"dp/qm/#code-outline","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def quality_map ( db_session , map_data_dir , inst_list , query_pd , euv_combined , chd_combined = None , color_list = None ): euv_origin_image = euv_combined . origin_image euv_origins = np . unique ( euv_origin_image ) euv_image = np . empty ( euv_origin_image . shape , dtype = object ) for euv_id in euv_origins : query_ind = np . where ( query_pd [ 'image_id' ] == euv_id ) instrument = query_pd [ 'instrument' ] . iloc [ query_ind [ 0 ]] if len ( instrument ) != 0 : euv_image = np . where ( euv_origin_image != euv_id , euv_image , instrument . iloc [ 0 ]) Plotting . PlotQualityMap ( euv_combined , euv_image , inst_list , color_list , nfig = 'EUV Quality Map ' + str ( euv_combined . image_info . date_obs [ 0 ]), title = 'EUV Quality Map: Mu Dependent \\n ' + str ( euv_combined . image_info . date_obs [ 0 ])) if chd_combined is not None : chd_origin_image = chd_combined . origin_image chd_origins = np . unique ( chd_origin_image ) chd_image = np . empty ( chd_origin_image . shape , dtype = object ) for chd_id in chd_origins : query_ind = np . where ( query_pd [ 'image_id' ] == chd_id ) instrument = query_pd [ 'instrument' ] . iloc [ query_ind [ 0 ]] if len ( instrument ) != 0 : chd_image = np . where ( euv_origin_image != chd_id , chd_image , instrument . iloc [ 0 ]) Plotting . PlotQualityMap ( chd_combined , chd_image , inst_list , color_list , nfig = 'CHD Quality Map ' + str ( chd_combined . image_info . date_obs [ 0 ]), title = 'CHD Quality Map: Mu Dependent \\n ' + str ( chd_combined . image_info . date_obs [ 0 ]), map_type = 'CHD' ) # save these maps to database return None 1.) query_ind = np.where(query_pd['image_id'] == euv_id) loop through the unique list of image ids and determine at what indices they are present 2.) euv_image = np.where(euv_origin_image != euv_id, euv_image, instrument.iloc[0]) add instrument name to array in correct pixel position 3.) Plotting.PlotQualityMap plot a quality map based off instrument and mu value of the final map","title":"Code Outline"},{"location":"dp/vt/","text":"Gaussian Varying Threshold Parameters These maps are created using varying CHD Threshold parameters. The threshold is varied based off a Gaussian distribution with a user inputted value of sigma (sigma=0.15) and thresholds for each image are randomly selected. The only different data product produced here is the Coronal Hole Detection map, the EUV map is the same as the synoptic EUV maps created. In these Gaussian weighted CHD Maps, coronal hole detections are weighted to produce a probability type map just as in the synoptic CHD Maps . Example Maps EUV Map CHD Map Code Outline 1 2 3 4 5 6 7 query_pd = db_funcs . query_euv_images ( db_session = db_session , time_min = query_time_min , time_max = query_time_max ) lbc_combo_query , iit_combo_query = chd_funcs . get_inst_combos ( db_session , inst_list , time_min = query_time_min , time_max = query_time_max ) los_image , iit_image , methods_list , use_indices = cr_funcs . apply_ipp ( db_session , hdf_data_dir , inst_list , row , methods_list , lbc_combo_query , iit_combo_query , n_intensity_bins = n_intensity_bins , R0 = R0 ) chd_image = dp_funcs . gauss_chd ( db_session , inst_list , los_image , iit_image , use_indices , iit_combo_query , thresh1 = thresh1 , thresh2 = thresh2 , nc = nc , iters = iters , sigma = sigma ) euv_map , chd_map = cr_funcs . create_map ( iit_image , chd_image , methods_list , row , map_x = map_x , map_y = map_y , R0 = R0 ) euv_combined , chd_combined , euv_combined_method , chd_combined_method = cr_funcs . cr_map ( euv_map , chd_map , euv_combined , chd_combined , image_info , map_info , mu_cutoff = mu_cutoff , mu_merge_cutoff = mu_merge_cutoff ) dp_funcs . save_threshold_maps ( db_session , map_data_dir , euv_combined , chd_combined , image_info , map_info , methods_list , euv_combined_method , chd_combined_method , sigma ) 1.) db_funcs.query_euv_images query the database for images based off the time range and center date 2.) chd_funcs.get_inst_combos query the appropriate combo ids for each instrument based off the maximum time range the user inputted 3.) cr_funcs.apply_ipp apply image pre-processing corrections 4.) dp_funcs.gauss_chd apply Coronal Hole Detection algorithm based on gaussian weighted threshold parameters 5.) cr_funcs.create_map convert the image and detection to a map 6.) cr_funcs.cr_map add image to synoptic map 7.) dp_funcs.save_threshold_maps plot and save varying threshold map to database with included methods","title":"Varying Threshold Maps"},{"location":"dp/vt/#gaussian-varying-threshold-parameters","text":"These maps are created using varying CHD Threshold parameters. The threshold is varied based off a Gaussian distribution with a user inputted value of sigma (sigma=0.15) and thresholds for each image are randomly selected. The only different data product produced here is the Coronal Hole Detection map, the EUV map is the same as the synoptic EUV maps created. In these Gaussian weighted CHD Maps, coronal hole detections are weighted to produce a probability type map just as in the synoptic CHD Maps .","title":"Gaussian Varying Threshold Parameters"},{"location":"dp/vt/#example-maps","text":"","title":"Example Maps"},{"location":"dp/vt/#euv-map","text":"","title":"EUV Map"},{"location":"dp/vt/#chd-map","text":"","title":"CHD Map"},{"location":"dp/vt/#code-outline","text":"1 2 3 4 5 6 7 query_pd = db_funcs . query_euv_images ( db_session = db_session , time_min = query_time_min , time_max = query_time_max ) lbc_combo_query , iit_combo_query = chd_funcs . get_inst_combos ( db_session , inst_list , time_min = query_time_min , time_max = query_time_max ) los_image , iit_image , methods_list , use_indices = cr_funcs . apply_ipp ( db_session , hdf_data_dir , inst_list , row , methods_list , lbc_combo_query , iit_combo_query , n_intensity_bins = n_intensity_bins , R0 = R0 ) chd_image = dp_funcs . gauss_chd ( db_session , inst_list , los_image , iit_image , use_indices , iit_combo_query , thresh1 = thresh1 , thresh2 = thresh2 , nc = nc , iters = iters , sigma = sigma ) euv_map , chd_map = cr_funcs . create_map ( iit_image , chd_image , methods_list , row , map_x = map_x , map_y = map_y , R0 = R0 ) euv_combined , chd_combined , euv_combined_method , chd_combined_method = cr_funcs . cr_map ( euv_map , chd_map , euv_combined , chd_combined , image_info , map_info , mu_cutoff = mu_cutoff , mu_merge_cutoff = mu_merge_cutoff ) dp_funcs . save_threshold_maps ( db_session , map_data_dir , euv_combined , chd_combined , image_info , map_info , methods_list , euv_combined_method , chd_combined_method , sigma ) 1.) db_funcs.query_euv_images query the database for images based off the time range and center date 2.) chd_funcs.get_inst_combos query the appropriate combo ids for each instrument based off the maximum time range the user inputted 3.) cr_funcs.apply_ipp apply image pre-processing corrections 4.) dp_funcs.gauss_chd apply Coronal Hole Detection algorithm based on gaussian weighted threshold parameters 5.) cr_funcs.create_map convert the image and detection to a map 6.) cr_funcs.cr_map add image to synoptic map 7.) dp_funcs.save_threshold_maps plot and save varying threshold map to database with included methods","title":"Code Outline"},{"location":"ipp/iit/","text":"Inter-Instrument Transformation The goal of the inter-instrument correction is to equate the intensities from one instrument to the intensities of another. The choice of which instrument to use as the \"reference instrument\" is an updatable parameter. Examples of Corrected Images These images of before and after applying IIT are from the different instruments at Carrington Rotation 2108.59. These can be enlarged by clicking image titles. AIA Images Original AIA Image Corrected AIA Image Difference AIA Image EUVI-A Images Original STA Image Corrected STA Image Difference STA Image EUVI-B Images Original STB Image Corrected STB Image Difference STB Image Examples of Histograms 200 Intensity Bin Histograms before and after IIT Correction. LBC Corrected Histogram IIT Corrected Histogram Analysis Pipeline Compute Histograms and Save to Database This function applies the limb-brightening correction, calculates the associated IIT histogram, and saves these histograms to the database. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def create_histograms ( db_session , inst_list , lbc_query_time_min , lbc_query_time_max , hdf_data_dir , n_mu_bins = 18 , n_intensity_bins = 200 , lat_band = [ - np . pi / 64. , np . pi / 64. ], log10 = True , R0 = 1.01 ): \"\"\" function to apply LBC, create and save histograms to the database \"\"\" image_pd = db_funcs . query_euv_images ( db_session = db_session , time_min = lbc_query_time_min , time_max = lbc_query_time_max , instrument = query_instrument ) combo_query = db_funcs . query_inst_combo ( db_session , lbc_query_time_min , lbc_query_time_max , meth_name = \"LBCC Theoretic\" , instrument = instrument ) original_los , lbcc_image , mu_indices , use_indices , theoretic_query = lbcc_funcs . apply_lbc ( db_session , hdf_data_dir , combo_query , image_row = row , n_intensity_bins = n_intensity_bins , R0 = R0 ) hist = psi_d_types . LBCCImage . iit_hist ( lbcc_image , lat_band , log10 ) iit_hist = psi_d_types . create_iit_hist ( lbcc_image , method_id [ 1 ], lat_band , hist ) db_funcs . add_hist ( db_session , iit_hist ) 1.) db_funcs.query_euv_images queries database for images (from EUV_Images table) in specified date range 2.) db_funcs.query_inst_combo queries database for closest image combinations to date observed 3.) lbcc_funcs.apply_lbc applies Limb-Brightening Correction to images and creates LBCCImage datatype 4.) psi_d_types.LBCCImage.iit_hist calculates IIT histogram from LBC corrected data 5.) psi_d_types.create_iit_hist creates IIT histogram datatype 6.) db_funcs.add_hist saves histograms to database (table Histogram) associating an image_id, meth_id, and basic information with histogram Calculate and Save Correction Coefficients This function queries the database for IIT histograms, calculates correction coefficients, and saves them to the database. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def calc_iit_coefficients ( db_session , inst_list , ref_inst , calc_query_time_min , calc_query_time_max , weekday = 0 , number_of_days = 180 , n_intensity_bins = 200 , lat_band = [ - np . pi / 2.4 , np . pi / 2.4 ], create = False ): \"\"\" function to query IIT histograms, calculate IIT coefficients, and save to database \"\"\" euv_images = db_funcs . query_euv_images ( db_session , time_min = calc_query_time_min , time_max = calc_query_time_max , instrument = ref_instrument ) ref_hist_pd = db_funcs . query_hist ( db_session = db_session , meth_id = method_id [ 1 ], n_intensity_bins = n_intensity_bins , lat_band = np . array ( lat_band ) . tobytes (), time_min = calc_query_time_min - datetime . timedelta ( days = number_of_days ), time_max = calc_query_time_max + datetime . timedelta ( days = number_of_days ), instrument = ref_instrument ) rot_images = db_funcs . query_euv_images_rot ( db_session , rot_min = rot_min , rot_max = rot_max , instrument = query_instrument ) alpha_x_parameters = iit . optim_iit_linear ( norm_hist_ref , norm_hist_fit , intensity_bin_edges , init_pars = init_pars ) db_funcs . store_iit_values ( db_session , pd_hist , meth_name , meth_desc , alpha_x_parameters . x , create ) 1.) db_funcs.query_euv_images queries database for euv images for the reference instrument used to get a range of Carrington rotation for which to calculate fit coefficients 2.) db_funcs.query_hist queries database for histograms (from Histogram table) in specified date range 3.) db_funcs.query_euv_images_rot queries database for euv images by Carrington rotation range 4.) iit.optim_iit_linear use linear optimization method to calculate fit parameters hist_ref and hist_fit are the reference histogram and the instrument histogram for the fit these are determined using boolean indexing 5.) db_funcs.store_iit_values save the two fit coefficients to database using function store_iit_values creates image combination combo_id of image_ids and dates in Images_Combos table creates association between each image_id and combo_id in Image_Combo_Assoc table creates new method \u201cIIT\u201d with an associated meth_id in Meth_Defs table creates new variable definitions \"alpha and \"x\"\" with an associated var_id in Var_Defs table store variable value as float in Var_Vals table with associated combo_id, meth_id, and var_id Apply Inter-Instrument Transformation and Plot New Images This function queries the database for IIT coefficients, applies the correction, and plots resulting images. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def apply_iit_correction ( db_session , hdf_data_dir , iit_query_time_min , iit_query_time_max , inst_list , n_mu_bins , n_intensity_bins , n_images_plot = 1 , plot = False ): \"\"\" function to query IIT correction coefficients, apply correction, and plot resulting images \"\"\" euv_images = db_funcs . query_euv_images ( db_session , time_min = iit_query_time_min , time_max = iit_query_time_max , instrument = ref_instrument ) rot_images = db_funcs . query_euv_images_rot ( db_session , rot_min = rot_min , rot_max = rot_max , instrument = query_instrument ) combo_query_lbc = db_funcs . query_inst_combo ( db_session , iit_query_time_min , iit_query_time_max , lbc_meth_name , instrument ) combo_query_iit = db_funcs . query_inst_combo ( db_session , iit_query_time_min , iit_query_time_max , iit_meth_name , instrument ) original_los , lbcc_image , mu_indices , use_indices , theoretic_query = lbcc_funcs . apply_lbc ( db_session , hdf_data_dir , combo_query_lbc , image_row = row , n_intensity_bins = n_intensity_bins , R0 = R0 ) lbcc_image , iit_image , use_indices , alpha , x = apply_iit ( db_session , hdf_data_dir , combo_query_iit , lbcc_image , use_indices , image_row = row , R0 = R0 ) if plot : Plotting . PlotCorrectedImage ( lbcc_data , los_image = original_los , nfig = 100 + inst_index * 10 + index , title = \"Corrected LBCC Image for \" + instrument ) Plotting . PlotCorrectedImage ( corrected_iit_data , los_image = original_los , nfig = 200 + inst_index * 10 + index , title = \"Corrected IIT Image for \" + instrument ) Plotting . PlotCorrectedImage ( lbcc_data - corrected_iit_data , los_image = original_los , nfig = 300 + inst_index * 10 + index , title = \"Difference Plot for \" + instrument ) 1.) db_funcs.query_euv_images queries database for images (from EUV_Images table) in specified date range queries for reference instrument to get minimum and maximum Carrington rotation 2.) db_funcs.query_euv_images_rot queries for instrument in question based on Carrington rotation range 3.) db_funcs.query_inst_combo queries database for closest image combinations to date observed does this for both the LBC and IIT methods 4.) lbcc_funcs.apply_lbc applies Limb-Brightening Correction to images and creates LBCCImage datatype 5.) apply_iit applies Inter-Instrument Transformation Correction to images and creates IITImage datatype 6.) Plotting.PlotCorrectedImage plots LBC images, IIT corrected images, and the difference between them Apply IIT This is a sub-step that applies the Inter-Instrument Transformation Correction to individual image and returns the correct IIT Image. It is called during the third step of Inter-Instrument Transformation. 1 2 3 4 5 6 7 8 9 10 11 12 13 def apply_iit ( db_session , hdf_data_dir , inst_combo_query , lbcc_image , image_row , R0 = 1.01 ): \"\"\" function to apply IIT to a specific image, returns corrected image \"\"\" method_id_info = db_funcs . get_method_id ( db_session , meth_name , meth_desc = None , var_names = None , var_descs = None , create = False ) alpha_x_parameters = db_funcs . query_var_val ( db_session , meth_name , date_obs = lbcc_image . date_obs , inst_combo_query = inst_combo_query ) corrected_iit_data [ use_indices ] = 10 ** ( alpha * np . log10 ( lbcc_data [ use_indices ]) + x ) iit_image = psi_d_types . create_iit_image ( lbcc_image , corrected_iit_data , method_id_info [ 1 ], hdf_path ) return lbcc_image , iit_image , use_indices , alpha , x 1.) db_funcs.get_method_id queries database for method id associated with method name 2.) db_funcs.query_var_val queries database for variable values associated with specific image (from Var_Vals table) 3.) corrected_iit_data[use_indices] = 10 ** (alpha * np.log10(lbcc_data[use_indices]) + x) applies correction to image based off alpha, x, and lbcc corrected data arrays 4.) psi_d_types.create_iit_image create IIT Image datatype from corrected IIT data Generate Histogram Plots This function generates histogram plots comparing data from before and after the IIT correction. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def plot_iit_histograms ( db_session , hdf_data_dir , hist_query_time_min , hist_query_time_max , inst_list , ref_inst , n_intensity_bins = 200 , lat_band = [ - np . pi / 2.4 , np . pi / 2.4 ], R0 = 1.01 , log10 = True ): \"\"\" function to create corrected IIT histograms then plot original, LBC Corrected, and IIT Corrected histograms for comparison \"\"\" pd_hist = db_funcs . query_hist ( db_session = db_session , meth_id = method_id [ 1 ], n_intensity_bins = n_intensity_bins , lat_band = np . array ( lat_band ) . tobytes (), time_min = hist_query_time_min , time_max = hist_query_time_max ) combo_query_lbc = db_funcs . query_inst_combo ( db_session , hist_query_time_min , hist_query_time_max , meth_name = \"LBCC Theoretic\" , instrument = instrument ) combo_query_iit = db_funcs . query_inst_combo ( db_session , hist_query_time_min , hist_query_time_max , meth_name = \"IIT\" , instrument = instrument ) image_pd = db_funcs . query_euv_images ( db_session = db_session , time_min = hist_query_time_min , time_max = hist_query_time_max , instrument = query_instrument ) original_los , lbcc_image , mu_indices , use_indices , theoretic_query = lbcc_funcs . apply_lbc ( db_session , hdf_data_dir , combo_query_lbc , image_row = row , n_intensity_bins = n_intensity_bins , R0 = R0 ) original_los_hist = psi_d_types . LosImage . iit_hist ( original_los , intensity_bin_edges , lat_band , log10 ) lbcc_image , iit_image , use_indices , alpha , x = apply_iit ( db_session , hdf_data_dir , combo_query_iit , lbcc_image , use_indices , image_row = row , R0 = R0 ) hist_iit = psi_d_types . IITImage . iit_hist ( iit_image , lat_band , log10 ) Plotting . Plot1d_Hist ( norm_original_hist , instrument , inst_index , intensity_bin_edges , color_list , linestyle_list , figure = 100 , xlabel = \"Intensity (log10)\" , ylabel = \"H(I)\" , title = \"Histogram: Original Image\" ) Plotting . Plot1d_Hist ( norm_lbc_hist , instrument , inst_index , intensity_bin_edges , color_list , linestyle_list , figure = 200 , xlabel = \"Intensity (log10)\" , ylabel = \"H(I)\" , title = \"Histogram: Post LBCC\" ) Plotting . Plot1d_Hist ( norm_corrected_hist , instrument , inst_index , intensity_bin_edges , color_list , linestyle_list , figure = 300 , xlabel = \"Intensity (log10)\" , ylabel = \"H(I)\" , title = \"Histogram: Post IIT\" ) 1.) db_funcs.query_hist queries database for histograms (from Histogram table) in specified date range 2.) db_funcs.query_euv_images queries database for images (from EUV_Images table) in specified date range 3.) db_funcs.query_inst_combo queries database for closest image combinations to date observed does this for both the LBC and IIT methods 3.) lbcc_funcs.apply_lbc applies Limb-Brightening Correction to images and creates LBCCImage datatype 4.) psi_d_types.LosImage.iit_hist create 1D IIT Histogram from original LOS image data 5.) apply_iit applies Inter-Instrument Transformation Correction to images and creates IITImage datatype 6.) psi_d_types.IITImage.iit_hist create 1D IIT Histogram from corrected IIT image data 7.) Plotting.Plot1d_Hist plot 1D Normalized IIT Histograms for original, LBC, and IIT data","title":"Inter-Instrument Transformation"},{"location":"ipp/iit/#inter-instrument-transformation","text":"The goal of the inter-instrument correction is to equate the intensities from one instrument to the intensities of another. The choice of which instrument to use as the \"reference instrument\" is an updatable parameter.","title":"Inter-Instrument Transformation"},{"location":"ipp/iit/#examples-of-corrected-images","text":"These images of before and after applying IIT are from the different instruments at Carrington Rotation 2108.59. These can be enlarged by clicking image titles.","title":"Examples of Corrected Images"},{"location":"ipp/iit/#aia-images","text":"Original AIA Image Corrected AIA Image Difference AIA Image","title":"AIA Images"},{"location":"ipp/iit/#euvi-a-images","text":"Original STA Image Corrected STA Image Difference STA Image","title":"EUVI-A Images"},{"location":"ipp/iit/#euvi-b-images","text":"Original STB Image Corrected STB Image Difference STB Image","title":"EUVI-B Images"},{"location":"ipp/iit/#examples-of-histograms","text":"200 Intensity Bin Histograms before and after IIT Correction. LBC Corrected Histogram IIT Corrected Histogram","title":"Examples of Histograms"},{"location":"ipp/iit/#analysis-pipeline","text":"","title":"Analysis Pipeline"},{"location":"ipp/iit/#compute-histograms-and-save-to-database","text":"This function applies the limb-brightening correction, calculates the associated IIT histogram, and saves these histograms to the database. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def create_histograms ( db_session , inst_list , lbc_query_time_min , lbc_query_time_max , hdf_data_dir , n_mu_bins = 18 , n_intensity_bins = 200 , lat_band = [ - np . pi / 64. , np . pi / 64. ], log10 = True , R0 = 1.01 ): \"\"\" function to apply LBC, create and save histograms to the database \"\"\" image_pd = db_funcs . query_euv_images ( db_session = db_session , time_min = lbc_query_time_min , time_max = lbc_query_time_max , instrument = query_instrument ) combo_query = db_funcs . query_inst_combo ( db_session , lbc_query_time_min , lbc_query_time_max , meth_name = \"LBCC Theoretic\" , instrument = instrument ) original_los , lbcc_image , mu_indices , use_indices , theoretic_query = lbcc_funcs . apply_lbc ( db_session , hdf_data_dir , combo_query , image_row = row , n_intensity_bins = n_intensity_bins , R0 = R0 ) hist = psi_d_types . LBCCImage . iit_hist ( lbcc_image , lat_band , log10 ) iit_hist = psi_d_types . create_iit_hist ( lbcc_image , method_id [ 1 ], lat_band , hist ) db_funcs . add_hist ( db_session , iit_hist ) 1.) db_funcs.query_euv_images queries database for images (from EUV_Images table) in specified date range 2.) db_funcs.query_inst_combo queries database for closest image combinations to date observed 3.) lbcc_funcs.apply_lbc applies Limb-Brightening Correction to images and creates LBCCImage datatype 4.) psi_d_types.LBCCImage.iit_hist calculates IIT histogram from LBC corrected data 5.) psi_d_types.create_iit_hist creates IIT histogram datatype 6.) db_funcs.add_hist saves histograms to database (table Histogram) associating an image_id, meth_id, and basic information with histogram","title":"Compute Histograms and Save to Database"},{"location":"ipp/iit/#calculate-and-save-correction-coefficients","text":"This function queries the database for IIT histograms, calculates correction coefficients, and saves them to the database. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def calc_iit_coefficients ( db_session , inst_list , ref_inst , calc_query_time_min , calc_query_time_max , weekday = 0 , number_of_days = 180 , n_intensity_bins = 200 , lat_band = [ - np . pi / 2.4 , np . pi / 2.4 ], create = False ): \"\"\" function to query IIT histograms, calculate IIT coefficients, and save to database \"\"\" euv_images = db_funcs . query_euv_images ( db_session , time_min = calc_query_time_min , time_max = calc_query_time_max , instrument = ref_instrument ) ref_hist_pd = db_funcs . query_hist ( db_session = db_session , meth_id = method_id [ 1 ], n_intensity_bins = n_intensity_bins , lat_band = np . array ( lat_band ) . tobytes (), time_min = calc_query_time_min - datetime . timedelta ( days = number_of_days ), time_max = calc_query_time_max + datetime . timedelta ( days = number_of_days ), instrument = ref_instrument ) rot_images = db_funcs . query_euv_images_rot ( db_session , rot_min = rot_min , rot_max = rot_max , instrument = query_instrument ) alpha_x_parameters = iit . optim_iit_linear ( norm_hist_ref , norm_hist_fit , intensity_bin_edges , init_pars = init_pars ) db_funcs . store_iit_values ( db_session , pd_hist , meth_name , meth_desc , alpha_x_parameters . x , create ) 1.) db_funcs.query_euv_images queries database for euv images for the reference instrument used to get a range of Carrington rotation for which to calculate fit coefficients 2.) db_funcs.query_hist queries database for histograms (from Histogram table) in specified date range 3.) db_funcs.query_euv_images_rot queries database for euv images by Carrington rotation range 4.) iit.optim_iit_linear use linear optimization method to calculate fit parameters hist_ref and hist_fit are the reference histogram and the instrument histogram for the fit these are determined using boolean indexing 5.) db_funcs.store_iit_values save the two fit coefficients to database using function store_iit_values creates image combination combo_id of image_ids and dates in Images_Combos table creates association between each image_id and combo_id in Image_Combo_Assoc table creates new method \u201cIIT\u201d with an associated meth_id in Meth_Defs table creates new variable definitions \"alpha and \"x\"\" with an associated var_id in Var_Defs table store variable value as float in Var_Vals table with associated combo_id, meth_id, and var_id","title":"Calculate and Save Correction Coefficients"},{"location":"ipp/iit/#apply-inter-instrument-transformation-and-plot-new-images","text":"This function queries the database for IIT coefficients, applies the correction, and plots resulting images. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def apply_iit_correction ( db_session , hdf_data_dir , iit_query_time_min , iit_query_time_max , inst_list , n_mu_bins , n_intensity_bins , n_images_plot = 1 , plot = False ): \"\"\" function to query IIT correction coefficients, apply correction, and plot resulting images \"\"\" euv_images = db_funcs . query_euv_images ( db_session , time_min = iit_query_time_min , time_max = iit_query_time_max , instrument = ref_instrument ) rot_images = db_funcs . query_euv_images_rot ( db_session , rot_min = rot_min , rot_max = rot_max , instrument = query_instrument ) combo_query_lbc = db_funcs . query_inst_combo ( db_session , iit_query_time_min , iit_query_time_max , lbc_meth_name , instrument ) combo_query_iit = db_funcs . query_inst_combo ( db_session , iit_query_time_min , iit_query_time_max , iit_meth_name , instrument ) original_los , lbcc_image , mu_indices , use_indices , theoretic_query = lbcc_funcs . apply_lbc ( db_session , hdf_data_dir , combo_query_lbc , image_row = row , n_intensity_bins = n_intensity_bins , R0 = R0 ) lbcc_image , iit_image , use_indices , alpha , x = apply_iit ( db_session , hdf_data_dir , combo_query_iit , lbcc_image , use_indices , image_row = row , R0 = R0 ) if plot : Plotting . PlotCorrectedImage ( lbcc_data , los_image = original_los , nfig = 100 + inst_index * 10 + index , title = \"Corrected LBCC Image for \" + instrument ) Plotting . PlotCorrectedImage ( corrected_iit_data , los_image = original_los , nfig = 200 + inst_index * 10 + index , title = \"Corrected IIT Image for \" + instrument ) Plotting . PlotCorrectedImage ( lbcc_data - corrected_iit_data , los_image = original_los , nfig = 300 + inst_index * 10 + index , title = \"Difference Plot for \" + instrument ) 1.) db_funcs.query_euv_images queries database for images (from EUV_Images table) in specified date range queries for reference instrument to get minimum and maximum Carrington rotation 2.) db_funcs.query_euv_images_rot queries for instrument in question based on Carrington rotation range 3.) db_funcs.query_inst_combo queries database for closest image combinations to date observed does this for both the LBC and IIT methods 4.) lbcc_funcs.apply_lbc applies Limb-Brightening Correction to images and creates LBCCImage datatype 5.) apply_iit applies Inter-Instrument Transformation Correction to images and creates IITImage datatype 6.) Plotting.PlotCorrectedImage plots LBC images, IIT corrected images, and the difference between them","title":"Apply Inter-Instrument Transformation and Plot New Images"},{"location":"ipp/iit/#apply-iit","text":"This is a sub-step that applies the Inter-Instrument Transformation Correction to individual image and returns the correct IIT Image. It is called during the third step of Inter-Instrument Transformation. 1 2 3 4 5 6 7 8 9 10 11 12 13 def apply_iit ( db_session , hdf_data_dir , inst_combo_query , lbcc_image , image_row , R0 = 1.01 ): \"\"\" function to apply IIT to a specific image, returns corrected image \"\"\" method_id_info = db_funcs . get_method_id ( db_session , meth_name , meth_desc = None , var_names = None , var_descs = None , create = False ) alpha_x_parameters = db_funcs . query_var_val ( db_session , meth_name , date_obs = lbcc_image . date_obs , inst_combo_query = inst_combo_query ) corrected_iit_data [ use_indices ] = 10 ** ( alpha * np . log10 ( lbcc_data [ use_indices ]) + x ) iit_image = psi_d_types . create_iit_image ( lbcc_image , corrected_iit_data , method_id_info [ 1 ], hdf_path ) return lbcc_image , iit_image , use_indices , alpha , x 1.) db_funcs.get_method_id queries database for method id associated with method name 2.) db_funcs.query_var_val queries database for variable values associated with specific image (from Var_Vals table) 3.) corrected_iit_data[use_indices] = 10 ** (alpha * np.log10(lbcc_data[use_indices]) + x) applies correction to image based off alpha, x, and lbcc corrected data arrays 4.) psi_d_types.create_iit_image create IIT Image datatype from corrected IIT data","title":"Apply IIT"},{"location":"ipp/iit/#generate-histogram-plots","text":"This function generates histogram plots comparing data from before and after the IIT correction. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def plot_iit_histograms ( db_session , hdf_data_dir , hist_query_time_min , hist_query_time_max , inst_list , ref_inst , n_intensity_bins = 200 , lat_band = [ - np . pi / 2.4 , np . pi / 2.4 ], R0 = 1.01 , log10 = True ): \"\"\" function to create corrected IIT histograms then plot original, LBC Corrected, and IIT Corrected histograms for comparison \"\"\" pd_hist = db_funcs . query_hist ( db_session = db_session , meth_id = method_id [ 1 ], n_intensity_bins = n_intensity_bins , lat_band = np . array ( lat_band ) . tobytes (), time_min = hist_query_time_min , time_max = hist_query_time_max ) combo_query_lbc = db_funcs . query_inst_combo ( db_session , hist_query_time_min , hist_query_time_max , meth_name = \"LBCC Theoretic\" , instrument = instrument ) combo_query_iit = db_funcs . query_inst_combo ( db_session , hist_query_time_min , hist_query_time_max , meth_name = \"IIT\" , instrument = instrument ) image_pd = db_funcs . query_euv_images ( db_session = db_session , time_min = hist_query_time_min , time_max = hist_query_time_max , instrument = query_instrument ) original_los , lbcc_image , mu_indices , use_indices , theoretic_query = lbcc_funcs . apply_lbc ( db_session , hdf_data_dir , combo_query_lbc , image_row = row , n_intensity_bins = n_intensity_bins , R0 = R0 ) original_los_hist = psi_d_types . LosImage . iit_hist ( original_los , intensity_bin_edges , lat_band , log10 ) lbcc_image , iit_image , use_indices , alpha , x = apply_iit ( db_session , hdf_data_dir , combo_query_iit , lbcc_image , use_indices , image_row = row , R0 = R0 ) hist_iit = psi_d_types . IITImage . iit_hist ( iit_image , lat_band , log10 ) Plotting . Plot1d_Hist ( norm_original_hist , instrument , inst_index , intensity_bin_edges , color_list , linestyle_list , figure = 100 , xlabel = \"Intensity (log10)\" , ylabel = \"H(I)\" , title = \"Histogram: Original Image\" ) Plotting . Plot1d_Hist ( norm_lbc_hist , instrument , inst_index , intensity_bin_edges , color_list , linestyle_list , figure = 200 , xlabel = \"Intensity (log10)\" , ylabel = \"H(I)\" , title = \"Histogram: Post LBCC\" ) Plotting . Plot1d_Hist ( norm_corrected_hist , instrument , inst_index , intensity_bin_edges , color_list , linestyle_list , figure = 300 , xlabel = \"Intensity (log10)\" , ylabel = \"H(I)\" , title = \"Histogram: Post IIT\" ) 1.) db_funcs.query_hist queries database for histograms (from Histogram table) in specified date range 2.) db_funcs.query_euv_images queries database for images (from EUV_Images table) in specified date range 3.) db_funcs.query_inst_combo queries database for closest image combinations to date observed does this for both the LBC and IIT methods 3.) lbcc_funcs.apply_lbc applies Limb-Brightening Correction to images and creates LBCCImage datatype 4.) psi_d_types.LosImage.iit_hist create 1D IIT Histogram from original LOS image data 5.) apply_iit applies Inter-Instrument Transformation Correction to images and creates IITImage datatype 6.) psi_d_types.IITImage.iit_hist create 1D IIT Histogram from corrected IIT image data 7.) Plotting.Plot1d_Hist plot 1D Normalized IIT Histograms for original, LBC, and IIT data","title":"Generate Histogram Plots"},{"location":"ipp/lbc/","text":"Limb-Brightening Correction Limb Brightening Correction (LBC) is the second step in the data pre-processing pipeline. The goal of LBC is to correct for brightening of structures that is dependent upon their distance from disk center. Examples of Corrected Images These images of before and after applying LBC are from the different instruments on April 1, 2011. These can be enlarged by clicking image titles. AIA Images Original AIA Image Corrected AIA Image Difference AIA Image EUVI-A Images Original STA Image Corrected STA Image Difference STA Image EUVI-B Images Original STB Image Corrected STB Image Difference STB Image Theoretical Analysis Pipeline Compute Histograms and Save to Database This function computes 2D Histograms from processed images for use in the LBC process. It then saves these computed histograms to the database. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 def save_histograms ( db_session , hdf_data_dir , inst_list , hist_query_time_min , hist_query_time_max , n_mu_bins = 18 , n_intensity_bins = 200 , lat_band = [ - np . pi / 64. , np . pi / 64. ], log10 = True , R0 = 1.01 ): \"\"\" function to create and save histograms to database \"\"\" query_pd = db_funcs . query_euv_images ( db_session = db_session , time_min = hist_query_time_min , time_max = hist_query_time_max , instrument = query_instrument ) temp_hist = los_temp . mu_hist ( image_intensity_bin_edges , mu_bin_edges , lat_band = lat_band , log10 = log10 ) hist_lbcc = psi_d_types . create_lbcc_hist ( hdf_path , row . image_id , method_id [ 1 ], mu_bin_edges , image_intensity_bin_edges , lat_band , temp_hist ) db_funcs . add_hist ( db_session , hist_lbcc ) 1.) db_funcs.query_euv_images queries database for images (from EUV_Images table) in specified date range 2.) los_temp.mu_hist creates histogram based on number of mu and intensity bins 3.) psi_d_types.create_lbcc_hist create histogram datatype from lbcc histogram 4.) db_funcs.add_hist saves histograms to database (table Histogram) associating an image_id, meth_id, and basic information with histogram Calculate and Save Theoretical Fit Parameters This function queries histograms from the database then calculates LBC fit parameters which are then saved in the database. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def calc_theoretic_fit ( db_session , inst_list , calc_query_time_min , calc_query_time_max , weekday = 0 , number_of_days = 180 , n_mu_bins = 18 , n_intensity_bins = 200 , lat_band = [ - np . pi / 64. , np . pi / 64. ], create = False ): \"\"\" function to calculate and save (to database) theoretic LBC fit parameters \"\"\" pd_hist = db_funcs . query_hist ( db_session = db_session , meth_id = method_id [ 1 ], n_mu_bins = n_mu_bins , n_intensity_bins = n_intensity_bins , lat_band = np . array ( lat_band ) . tobytes (), time_min = np . datetime64 ( min_date ) . astype ( datetime . datetime ), time_max = np . datetime64 ( max_date ) . astype ( datetime . datetime ), instrument = query_instrument ) optim_out_theo = optim . minimize ( lbcc . get_functional_sse , init_pars , args = ( hist_ref , hist_mat , mu_vec , intensity_bin_array , model ), method = \"BFGS\" ) db_funcs . store_lbcc_values ( db_session , pd_hist , meth_name , meth_desc , var_name , var_desc , date_index , inst_index , optim_vals = optim_vals_theo [ 0 : 6 ], results = results_theo , create = True ) 1.) db_funcs.query_hist queries database for histograms (from Histogram table) in specified date range 2.) optim.minimize use theoretical optimization method to calculate fit parameters 3.) db_funcs.store_lbcc_values save the six fit parameters to database using function store_lbcc_values creates image combination combo_id of image_ids and dates in Images_Combos table creates association between each image_id and combo_id in Image_Combo_Assoc table creates new method \u201cLBCC Theoretic\u201d with an associated meth_id in Meth_Defs table creates new variable definitions \u201cTheoVar\u201d + index with an associated var_id in Var_Defs table store variable value as float in Var_Vals table with associated combo_id, meth_id, and var_id Apply Limb-Brightening Correction and Plot Corrected Images This function queries the database for LBC fit parameters then applies them to specified images, plotting resulting images before and after the correction. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def apply_lbc_correction ( db_session , hdf_data_dir , inst_list , lbc_query_time_min , lbc_query_time_max , n_intensity_bins = 200 , R0 = 1.01 , n_images_plot = 1 , plot = False ): \"\"\" function to apply limb-brightening correction and plot images within a certain time frame \"\"\" image_pd = db_funcs . query_euv_images ( db_session = db_session , time_min = lbc_query_time_min , time_max = lbc_query_time_max , instrument = query_instrument ) combo_query = db_funcs . query_inst_combo ( db_session , lbc_query_time_min , lbc_query_time_max , meth_name , instrument ) original_los , lbcc_image , mu_indices , use_indices = apply_lbc ( db_session , hdf_data_dir , combo_query , image_row = row , n_intensity_bins = n_intensity_bins , R0 = R0 ) if plot : Plotting . PlotImage ( original_los , nfig = 100 + inst_index * 10 + index , title = \"Original LOS Image for \" + instrument ) Plotting . PlotCorrectedImage ( corrected_data = lbcc_image . lbcc_data , los_image = original_los , nfig = 200 + inst_index * 10 + index , title = \"Corrected LBCC Image for \" + instrument ) Plotting . PlotCorrectedImage ( corrected_data = original_los . data - lbcc_image . lbcc_data , los_image = original_los , nfig = 300 + inst_index * 10 + index , title = \"Difference Plot for \" + instrument ) 1.) db_funcs.query_euv_images queries database for images (from EUV_Images table) in specified date range 2.) db_funcs.query_inst_combo queries database for closest image combinations to date observed 3.) db_funcs.query_var_val queries database for variable values associated with specific image (from Var_Vals table) 4.) lbcc.get_beta_y_theoretic_continuous_1d_indices calculates 1d beta and y arrays for valid mu indices uses variable values from query in step two uses original los image to determine indices for correction 5.) corrected_lbc_data[use_indices] = 10 ** (beta1d * np.log10(original_los.data[use_indices]) + y1d) applies correction to image based off beta, y, and original data arrays 6.) Plotting.PlotImage and Plotting.PlotCorrectedImage plots original and corrected images, and difference between them Apply LBC This is a sub-step that applies the Limb-Brightening Corrrection to individual image and returns the correct LBCC Image. It is called during the third step of Limb-Brightening. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def apply_lbc ( db_session , hdf_data_dir , inst_combo_query , image_row , n_intensity_bins = 200 , R0 = 1.01 ): \"\"\" function to apply LBC to a specific image, returns corrected image \"\"\" db_sesh , meth_id , var_ids = db_funcs . get_method_id ( db_session , meth_name , meth_desc = None , var_names = None , var_descs = None , create = False ) original_los = psi_d_types . read_los_image ( hdf_path ) theoretic_query = db_funcs . query_var_val ( db_session , meth_name , date_obs = original_los . info [ 'date_string' ], inst_combo_query = inst_combo_query ) beta1d , y1d , mu_indices , use_indices = lbcc . get_beta_y_theoretic_continuous_1d_indices ( theoretic_query , los_image = original_los ) corrected_lbc_data [ use_indices ] = 10 ** ( beta1d * np . log10 ( original_los . data [ use_indices ]) + y1d ) lbcc_image = psi_d_types . create_lbcc_image ( hdf_path , corrected_lbc_data , image_id = image_row . image_id , meth_id = meth_id , intensity_bin_edges = intensity_bin_edges ) return original_los , lbcc_image , mu_indices , use_indices , theoretic_query 1.) db_funcs.get_method_id queries database for method id associated with method name 2.) psi_d_types.read_los_image reads in los image from database 3.) db_funcs.query_var_val queries database for variable values associated with specific image (from Var_Vals table) 4.) lbcc.get_beta_y_theoretic_continuous_1d_indices calculates 1d beta and y arrays for valid mu indices uses variable values from query in step two uses original los image to determine indices for correction 5.) corrected_lbc_data[use_indices] = 10 ** (beta1d * np.log10(original_los.data[use_indices]) + y1d) applies correction to image based off beta, y, and original data arrays 6.) psi_d_types.create_lbcc_image create LBCC Image datatype from corrected LBC data Generate Plots of Beta and y This function queries the database for LBC fit parameters then generates plots of Beta and y over time. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 def generate_theoretic_plots ( db_session , inst_list , plot_query_time_min , plot_query_time_max , weekday , image_out_path , year = '2011' , time_period = '6 Month' , plot_week = 0 , n_mu_bins = 18 ): \"\"\" function to generate plots of beta/y over time and beta/y v. mu \"\"\" combo_query = db_funcs . query_inst_combo ( db_session , plot_query_time_min , plot_query_time_max , meth_name , instrument ) theoretic_query [ date_index , :] = db_funcs . query_var_val ( db_session , meth_name , date_obs = np . datetime64 ( center_date ) . astype ( datetime . datetime ), inst_combo_query = inst_combo_query ) plot_beta [ mu_index , date_index ], plot_y [ mu_index , date_index ] = lbcc . get_beta_y_theoretic_based ( theoretic_query [ date_index , :], mu ) beta_y_v_mu [ index , :] = lbcc . get_beta_y_theoretic_based ( theoretic_query [ plot_week , :], mu ) 1.) db_funcs.query_inst_combo queries database for closest image combinations to date observed 2.) db_funcs.query_var_val query fit parameters from database 3.) lbcc.get_beta_y_theoretic_based(theoretic_query[date_index, :], mu) calculate beta and y correction coefficients over time using theoretic fit parameters and mu values used for plotting beta and y over time 4.) lbcc.get_beta_y_theoretic_based(theoretic_query[plot_week, :], mu) calculate beta and y correction coefficients for a specific week using theoretic fit parameters and mu values used for plotting beta and y v. mu for a specific week Generate Histogram Plots This function queries the database for histograms and LBC fit parameters then generates plots of histograms before and after the LBC correction. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def generate_histogram_plots ( db_session , hdf_data_dir , inst_list , hist_plot_query_time_min , hist_plot_query_time_max , n_hist_plots = 1 , n_mu_bins = 18 , n_intensity_bins = 200 , lat_band = [ - np . pi / 64. , np . pi / 64. ], log10 = True ): \"\"\" function to generate plots of histograms before and after limb-brightening \"\"\" pd_hist = db_funcs . query_hist ( db_session = db_session , meth_id = method_id [ 1 ], n_mu_bins = n_mu_bins , n_intensity_bins = n_intensity_bins , lat_band = np . array ( lat_band ) . tobytes (), time_min = hist_plot_query_time_min , time_max = hist_plot_query_time_max , instrument = query_instrument ) Plotting . Plot2d_Hist ( plot_hist , date_obs , instrument , intensity_bin_edges , mu_bin_edges , figure , plot_index ) original_los , lbcc_image , mu_indices , use_indices = iit_funcs . apply_lbc_correction ( db_session , hdf_data_dir , instrument , row , n_intensity_bins , R0 ) hist_lbcc = psi_d_types . create_lbcc_hist ( hdf_path , row . image_id , method_id [ 1 ], mu_bin_edges , intensity_bin_edges , lat_band , temp_hist ) Plotting . Plot_LBCC_Hists ( plot_hist , date_obs , instrument , intensity_bin_edges , mu_bin_edges , figure , plot_index ) 1.) db_funcs.query_hist queries database for histograms (from Histogram table) in specified date range 2.) Plotting.Plot2d_Hist plots 2D histogram with plot title and axes labels 3.) iit_funcs.apply_lbc_correction applies Limb-Brightening Correction to images and creates LBCCImage datatype 4.) psi_d_types.create_lbcc_hist create histogram datatype from lbcc histogram 5.) Plotting.Plot_LBCC_Hists plots original and LBC corrected 2D histograms","title":"Limb-Brightening Correction"},{"location":"ipp/lbc/#limb-brightening-correction","text":"Limb Brightening Correction (LBC) is the second step in the data pre-processing pipeline. The goal of LBC is to correct for brightening of structures that is dependent upon their distance from disk center.","title":"Limb-Brightening Correction"},{"location":"ipp/lbc/#examples-of-corrected-images","text":"These images of before and after applying LBC are from the different instruments on April 1, 2011. These can be enlarged by clicking image titles.","title":"Examples of Corrected Images"},{"location":"ipp/lbc/#aia-images","text":"Original AIA Image Corrected AIA Image Difference AIA Image","title":"AIA Images"},{"location":"ipp/lbc/#euvi-a-images","text":"Original STA Image Corrected STA Image Difference STA Image","title":"EUVI-A Images"},{"location":"ipp/lbc/#euvi-b-images","text":"Original STB Image Corrected STB Image Difference STB Image","title":"EUVI-B Images"},{"location":"ipp/lbc/#theoretical-analysis-pipeline","text":"","title":"Theoretical Analysis Pipeline"},{"location":"ipp/lbc/#compute-histograms-and-save-to-database","text":"This function computes 2D Histograms from processed images for use in the LBC process. It then saves these computed histograms to the database. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 def save_histograms ( db_session , hdf_data_dir , inst_list , hist_query_time_min , hist_query_time_max , n_mu_bins = 18 , n_intensity_bins = 200 , lat_band = [ - np . pi / 64. , np . pi / 64. ], log10 = True , R0 = 1.01 ): \"\"\" function to create and save histograms to database \"\"\" query_pd = db_funcs . query_euv_images ( db_session = db_session , time_min = hist_query_time_min , time_max = hist_query_time_max , instrument = query_instrument ) temp_hist = los_temp . mu_hist ( image_intensity_bin_edges , mu_bin_edges , lat_band = lat_band , log10 = log10 ) hist_lbcc = psi_d_types . create_lbcc_hist ( hdf_path , row . image_id , method_id [ 1 ], mu_bin_edges , image_intensity_bin_edges , lat_band , temp_hist ) db_funcs . add_hist ( db_session , hist_lbcc ) 1.) db_funcs.query_euv_images queries database for images (from EUV_Images table) in specified date range 2.) los_temp.mu_hist creates histogram based on number of mu and intensity bins 3.) psi_d_types.create_lbcc_hist create histogram datatype from lbcc histogram 4.) db_funcs.add_hist saves histograms to database (table Histogram) associating an image_id, meth_id, and basic information with histogram","title":"Compute Histograms and Save to Database"},{"location":"ipp/lbc/#calculate-and-save-theoretical-fit-parameters","text":"This function queries histograms from the database then calculates LBC fit parameters which are then saved in the database. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def calc_theoretic_fit ( db_session , inst_list , calc_query_time_min , calc_query_time_max , weekday = 0 , number_of_days = 180 , n_mu_bins = 18 , n_intensity_bins = 200 , lat_band = [ - np . pi / 64. , np . pi / 64. ], create = False ): \"\"\" function to calculate and save (to database) theoretic LBC fit parameters \"\"\" pd_hist = db_funcs . query_hist ( db_session = db_session , meth_id = method_id [ 1 ], n_mu_bins = n_mu_bins , n_intensity_bins = n_intensity_bins , lat_band = np . array ( lat_band ) . tobytes (), time_min = np . datetime64 ( min_date ) . astype ( datetime . datetime ), time_max = np . datetime64 ( max_date ) . astype ( datetime . datetime ), instrument = query_instrument ) optim_out_theo = optim . minimize ( lbcc . get_functional_sse , init_pars , args = ( hist_ref , hist_mat , mu_vec , intensity_bin_array , model ), method = \"BFGS\" ) db_funcs . store_lbcc_values ( db_session , pd_hist , meth_name , meth_desc , var_name , var_desc , date_index , inst_index , optim_vals = optim_vals_theo [ 0 : 6 ], results = results_theo , create = True ) 1.) db_funcs.query_hist queries database for histograms (from Histogram table) in specified date range 2.) optim.minimize use theoretical optimization method to calculate fit parameters 3.) db_funcs.store_lbcc_values save the six fit parameters to database using function store_lbcc_values creates image combination combo_id of image_ids and dates in Images_Combos table creates association between each image_id and combo_id in Image_Combo_Assoc table creates new method \u201cLBCC Theoretic\u201d with an associated meth_id in Meth_Defs table creates new variable definitions \u201cTheoVar\u201d + index with an associated var_id in Var_Defs table store variable value as float in Var_Vals table with associated combo_id, meth_id, and var_id","title":"Calculate and Save Theoretical Fit Parameters"},{"location":"ipp/lbc/#apply-limb-brightening-correction-and-plot-corrected-images","text":"This function queries the database for LBC fit parameters then applies them to specified images, plotting resulting images before and after the correction. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def apply_lbc_correction ( db_session , hdf_data_dir , inst_list , lbc_query_time_min , lbc_query_time_max , n_intensity_bins = 200 , R0 = 1.01 , n_images_plot = 1 , plot = False ): \"\"\" function to apply limb-brightening correction and plot images within a certain time frame \"\"\" image_pd = db_funcs . query_euv_images ( db_session = db_session , time_min = lbc_query_time_min , time_max = lbc_query_time_max , instrument = query_instrument ) combo_query = db_funcs . query_inst_combo ( db_session , lbc_query_time_min , lbc_query_time_max , meth_name , instrument ) original_los , lbcc_image , mu_indices , use_indices = apply_lbc ( db_session , hdf_data_dir , combo_query , image_row = row , n_intensity_bins = n_intensity_bins , R0 = R0 ) if plot : Plotting . PlotImage ( original_los , nfig = 100 + inst_index * 10 + index , title = \"Original LOS Image for \" + instrument ) Plotting . PlotCorrectedImage ( corrected_data = lbcc_image . lbcc_data , los_image = original_los , nfig = 200 + inst_index * 10 + index , title = \"Corrected LBCC Image for \" + instrument ) Plotting . PlotCorrectedImage ( corrected_data = original_los . data - lbcc_image . lbcc_data , los_image = original_los , nfig = 300 + inst_index * 10 + index , title = \"Difference Plot for \" + instrument ) 1.) db_funcs.query_euv_images queries database for images (from EUV_Images table) in specified date range 2.) db_funcs.query_inst_combo queries database for closest image combinations to date observed 3.) db_funcs.query_var_val queries database for variable values associated with specific image (from Var_Vals table) 4.) lbcc.get_beta_y_theoretic_continuous_1d_indices calculates 1d beta and y arrays for valid mu indices uses variable values from query in step two uses original los image to determine indices for correction 5.) corrected_lbc_data[use_indices] = 10 ** (beta1d * np.log10(original_los.data[use_indices]) + y1d) applies correction to image based off beta, y, and original data arrays 6.) Plotting.PlotImage and Plotting.PlotCorrectedImage plots original and corrected images, and difference between them","title":"Apply Limb-Brightening Correction and Plot Corrected Images"},{"location":"ipp/lbc/#apply-lbc","text":"This is a sub-step that applies the Limb-Brightening Corrrection to individual image and returns the correct LBCC Image. It is called during the third step of Limb-Brightening. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def apply_lbc ( db_session , hdf_data_dir , inst_combo_query , image_row , n_intensity_bins = 200 , R0 = 1.01 ): \"\"\" function to apply LBC to a specific image, returns corrected image \"\"\" db_sesh , meth_id , var_ids = db_funcs . get_method_id ( db_session , meth_name , meth_desc = None , var_names = None , var_descs = None , create = False ) original_los = psi_d_types . read_los_image ( hdf_path ) theoretic_query = db_funcs . query_var_val ( db_session , meth_name , date_obs = original_los . info [ 'date_string' ], inst_combo_query = inst_combo_query ) beta1d , y1d , mu_indices , use_indices = lbcc . get_beta_y_theoretic_continuous_1d_indices ( theoretic_query , los_image = original_los ) corrected_lbc_data [ use_indices ] = 10 ** ( beta1d * np . log10 ( original_los . data [ use_indices ]) + y1d ) lbcc_image = psi_d_types . create_lbcc_image ( hdf_path , corrected_lbc_data , image_id = image_row . image_id , meth_id = meth_id , intensity_bin_edges = intensity_bin_edges ) return original_los , lbcc_image , mu_indices , use_indices , theoretic_query 1.) db_funcs.get_method_id queries database for method id associated with method name 2.) psi_d_types.read_los_image reads in los image from database 3.) db_funcs.query_var_val queries database for variable values associated with specific image (from Var_Vals table) 4.) lbcc.get_beta_y_theoretic_continuous_1d_indices calculates 1d beta and y arrays for valid mu indices uses variable values from query in step two uses original los image to determine indices for correction 5.) corrected_lbc_data[use_indices] = 10 ** (beta1d * np.log10(original_los.data[use_indices]) + y1d) applies correction to image based off beta, y, and original data arrays 6.) psi_d_types.create_lbcc_image create LBCC Image datatype from corrected LBC data","title":"Apply LBC"},{"location":"ipp/lbc/#generate-plots-of-beta-and-y","text":"This function queries the database for LBC fit parameters then generates plots of Beta and y over time. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 def generate_theoretic_plots ( db_session , inst_list , plot_query_time_min , plot_query_time_max , weekday , image_out_path , year = '2011' , time_period = '6 Month' , plot_week = 0 , n_mu_bins = 18 ): \"\"\" function to generate plots of beta/y over time and beta/y v. mu \"\"\" combo_query = db_funcs . query_inst_combo ( db_session , plot_query_time_min , plot_query_time_max , meth_name , instrument ) theoretic_query [ date_index , :] = db_funcs . query_var_val ( db_session , meth_name , date_obs = np . datetime64 ( center_date ) . astype ( datetime . datetime ), inst_combo_query = inst_combo_query ) plot_beta [ mu_index , date_index ], plot_y [ mu_index , date_index ] = lbcc . get_beta_y_theoretic_based ( theoretic_query [ date_index , :], mu ) beta_y_v_mu [ index , :] = lbcc . get_beta_y_theoretic_based ( theoretic_query [ plot_week , :], mu ) 1.) db_funcs.query_inst_combo queries database for closest image combinations to date observed 2.) db_funcs.query_var_val query fit parameters from database 3.) lbcc.get_beta_y_theoretic_based(theoretic_query[date_index, :], mu) calculate beta and y correction coefficients over time using theoretic fit parameters and mu values used for plotting beta and y over time 4.) lbcc.get_beta_y_theoretic_based(theoretic_query[plot_week, :], mu) calculate beta and y correction coefficients for a specific week using theoretic fit parameters and mu values used for plotting beta and y v. mu for a specific week","title":"Generate Plots of Beta and y"},{"location":"ipp/lbc/#generate-histogram-plots","text":"This function queries the database for histograms and LBC fit parameters then generates plots of histograms before and after the LBC correction. The source code and example usage for this is found in the CHD GitHub and the generalized function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def generate_histogram_plots ( db_session , hdf_data_dir , inst_list , hist_plot_query_time_min , hist_plot_query_time_max , n_hist_plots = 1 , n_mu_bins = 18 , n_intensity_bins = 200 , lat_band = [ - np . pi / 64. , np . pi / 64. ], log10 = True ): \"\"\" function to generate plots of histograms before and after limb-brightening \"\"\" pd_hist = db_funcs . query_hist ( db_session = db_session , meth_id = method_id [ 1 ], n_mu_bins = n_mu_bins , n_intensity_bins = n_intensity_bins , lat_band = np . array ( lat_band ) . tobytes (), time_min = hist_plot_query_time_min , time_max = hist_plot_query_time_max , instrument = query_instrument ) Plotting . Plot2d_Hist ( plot_hist , date_obs , instrument , intensity_bin_edges , mu_bin_edges , figure , plot_index ) original_los , lbcc_image , mu_indices , use_indices = iit_funcs . apply_lbc_correction ( db_session , hdf_data_dir , instrument , row , n_intensity_bins , R0 ) hist_lbcc = psi_d_types . create_lbcc_hist ( hdf_path , row . image_id , method_id [ 1 ], mu_bin_edges , intensity_bin_edges , lat_band , temp_hist ) Plotting . Plot_LBCC_Hists ( plot_hist , date_obs , instrument , intensity_bin_edges , mu_bin_edges , figure , plot_index ) 1.) db_funcs.query_hist queries database for histograms (from Histogram table) in specified date range 2.) Plotting.Plot2d_Hist plots 2D histogram with plot title and axes labels 3.) iit_funcs.apply_lbc_correction applies Limb-Brightening Correction to images and creates LBCCImage datatype 4.) psi_d_types.create_lbcc_hist create histogram datatype from lbcc histogram 5.) Plotting.Plot_LBCC_Hists plots original and LBC corrected 2D histograms","title":"Generate Histogram Plots"},{"location":"ipp/psf/","text":"PSF Deconvolution Page regarding PSF deconvolution.","title":"PSF Deconvolution"},{"location":"ipp/psf/#psf-deconvolution","text":"Page regarding PSF deconvolution.","title":"PSF Deconvolution"},{"location":"map/cmb/","text":"Combining Maps Maps are combined using a minimum intensity merge method. Maps are originally created for each instrument image individually then merged. There can be some points of overlap between different instrument maps and this is resolved by taking the data with the minimum intensity from each point of overlap. Coronal Hole data is then chosen based off which data points are used to create the original EUV maps. This method ensures that resulting maps are more continuous at seams. We also use a cutoff mu value to limit limb data distortion. In merging regions of overlap, we use data with a mu value greater than the cutoff value. In areas without overlap, any data available is used (mu cutoff of 0). Combine Maps Function The combine maps function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def combine_maps ( map_list , chd_map_list = None , mu_cutoff = 0.0 , mu_cut_over = None , del_mu = None ): \"\"\" function to combine maps from a list of PsiMap objects based on a mu_cutoff and minimum intensity merge Two methods to determine best minimum intensity merge - using mu cutoff values, or using del mu value return: combined EUV map, combined CHD map \"\"\" map_list [ ii ] . data [ map_list [ ii ] . mu < mu_cutoff ] = map_list [ ii ] . no_data_val if mu_cut_over is not None : overlap [:, :, ii ] = np . logical_and ( data_array [:, :, ii ] != map_list [ 0 ] . no_data_val , data_array [:, :, jj ] != map_list [ 0 ] . no_data_val ) good_index [:, :, ii ] = np . logical_or ( np . logical_and ( overlap [:, :, ii ], mu_array [:, :, ii ] >= mu_cut_over ), np . logical_and ( data_array [:, :, ii ] != map_list [ 0 ] . no_data_val , mu_array [:, :, ii ] >= mu_cutoff )) data_array [ np . logical_not ( good_index )] = float_info . max elif del_mu is not None : good_index [:, :, ii ] = mu_array [:, :, ii ] > ( max_mu - del_mu ) data_array [ np . logical_not ( good_index )] = float_info . max data_array [ data_array == map_list [ 0 ] . no_data_val ] = float_info . max map_index = np . argmin ( data_array , axis = 2 ) keep_data = data_array [ row_index , col_index , map_index ] keep_chd = chd_array [ row_index , col_index , map_index ] chd_map = psi_d_types . PsiMap ( keep_chd , map_list [ 0 ] . x , map_list [ 0 ] . y , mu = keep_mu , origin_image = keep_image , no_data_val = map_list [ 0 ] . no_data_val ) euv_map = psi_d_types . PsiMap ( keep_data , map_list [ 0 ] . x , map_list [ 0 ] . y , mu = keep_mu , origin_image = keep_image , no_data_val = map_list [ 0 ] . no_data_val ) return euv_map , chd_map 1.) map_list[ii].data[map_list[ii].mu < mu_cutoff] = map_list[ii].no_data_val for all pixels with mu < mu_cutoff, set intensity to no_data_val 2.1) np.logical_or(np.logical_and(overlap[:, :, ii], mu_array[:, :, ii] >= mu_cut_over), np.logical_and( data_array[:, :, ii] != map_list[0].no_data_val, mu_array[:, :, ii] >= mu_cutoff)) to determine \"good indices\" based of Caplan et. al. 2016: check for overlap, in areas of overlap choose data where mu > mu_cut_over in areas of no overlap, choose data where mu > mu_cutoff 2.2) mu_array[:, :, ii] > (max_mu - del_mu) determines \"good indices\" based off the value of del_mu 3.) data_array[np.logical_not(good_index)] = float_info.max, data_array[data_array == map_list[0].no_data_val] = float_info.max make poor mu pixels unusable to merge, make no_data_vals unusable to merge 4.) map_index = np.argmin(data_array, axis=2) find minimum intensity of remaining pixels 5.) keep_data = data_array[row_index, col_index, map_index], keep_data = data_array[row_index, col_index, map_index] choose data to use for the EUV and CHD map 6.) psi_d_types.PsiMap create new PsiMap object for both EUV and CHD combined maps","title":"Combining Maps"},{"location":"map/cmb/#combining-maps","text":"Maps are combined using a minimum intensity merge method. Maps are originally created for each instrument image individually then merged. There can be some points of overlap between different instrument maps and this is resolved by taking the data with the minimum intensity from each point of overlap. Coronal Hole data is then chosen based off which data points are used to create the original EUV maps. This method ensures that resulting maps are more continuous at seams. We also use a cutoff mu value to limit limb data distortion. In merging regions of overlap, we use data with a mu value greater than the cutoff value. In areas without overlap, any data available is used (mu cutoff of 0).","title":"Combining Maps"},{"location":"map/cmb/#combine-maps-function","text":"The combine maps function can be found here . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def combine_maps ( map_list , chd_map_list = None , mu_cutoff = 0.0 , mu_cut_over = None , del_mu = None ): \"\"\" function to combine maps from a list of PsiMap objects based on a mu_cutoff and minimum intensity merge Two methods to determine best minimum intensity merge - using mu cutoff values, or using del mu value return: combined EUV map, combined CHD map \"\"\" map_list [ ii ] . data [ map_list [ ii ] . mu < mu_cutoff ] = map_list [ ii ] . no_data_val if mu_cut_over is not None : overlap [:, :, ii ] = np . logical_and ( data_array [:, :, ii ] != map_list [ 0 ] . no_data_val , data_array [:, :, jj ] != map_list [ 0 ] . no_data_val ) good_index [:, :, ii ] = np . logical_or ( np . logical_and ( overlap [:, :, ii ], mu_array [:, :, ii ] >= mu_cut_over ), np . logical_and ( data_array [:, :, ii ] != map_list [ 0 ] . no_data_val , mu_array [:, :, ii ] >= mu_cutoff )) data_array [ np . logical_not ( good_index )] = float_info . max elif del_mu is not None : good_index [:, :, ii ] = mu_array [:, :, ii ] > ( max_mu - del_mu ) data_array [ np . logical_not ( good_index )] = float_info . max data_array [ data_array == map_list [ 0 ] . no_data_val ] = float_info . max map_index = np . argmin ( data_array , axis = 2 ) keep_data = data_array [ row_index , col_index , map_index ] keep_chd = chd_array [ row_index , col_index , map_index ] chd_map = psi_d_types . PsiMap ( keep_chd , map_list [ 0 ] . x , map_list [ 0 ] . y , mu = keep_mu , origin_image = keep_image , no_data_val = map_list [ 0 ] . no_data_val ) euv_map = psi_d_types . PsiMap ( keep_data , map_list [ 0 ] . x , map_list [ 0 ] . y , mu = keep_mu , origin_image = keep_image , no_data_val = map_list [ 0 ] . no_data_val ) return euv_map , chd_map 1.) map_list[ii].data[map_list[ii].mu < mu_cutoff] = map_list[ii].no_data_val for all pixels with mu < mu_cutoff, set intensity to no_data_val 2.1) np.logical_or(np.logical_and(overlap[:, :, ii], mu_array[:, :, ii] >= mu_cut_over), np.logical_and( data_array[:, :, ii] != map_list[0].no_data_val, mu_array[:, :, ii] >= mu_cutoff)) to determine \"good indices\" based of Caplan et. al. 2016: check for overlap, in areas of overlap choose data where mu > mu_cut_over in areas of no overlap, choose data where mu > mu_cutoff 2.2) mu_array[:, :, ii] > (max_mu - del_mu) determines \"good indices\" based off the value of del_mu 3.) data_array[np.logical_not(good_index)] = float_info.max, data_array[data_array == map_list[0].no_data_val] = float_info.max make poor mu pixels unusable to merge, make no_data_vals unusable to merge 4.) map_index = np.argmin(data_array, axis=2) find minimum intensity of remaining pixels 5.) keep_data = data_array[row_index, col_index, map_index], keep_data = data_array[row_index, col_index, map_index] choose data to use for the EUV and CHD map 6.) psi_d_types.PsiMap create new PsiMap object for both EUV and CHD combined maps","title":"Combine Maps Function"},{"location":"map/int/","text":"Interpolation Currently use linear interpolation, working on other methods utilizing astropy package functionality.","title":"Interpolation"},{"location":"map/int/#interpolation","text":"Currently use linear interpolation, working on other methods utilizing astropy package functionality.","title":"Interpolation"},{"location":"map/map/","text":"Mapping Pipeline After the calculation of the image pre-processing parameters (LBC and IIT), the mapping process undergoes five main steps through which EUV Images are converted to EUV and CHD Maps. 1.) Selecting Images 2.) Apply Pre-Processing Corrections a.) generate moving average dates b.) query for image combos associated with dates c.) apply LBC d.) apply IIT 3.) Coronal Hole Detection 4.) Create Single Instrument Maps 5.) Combine Maps and Save to the Database Mapping Pipeline Functions Select Images The first step in map creation is querying the database for all EUV Images in the relevant time frame and creating a methods dataframe. These functions are database functions and the full code can be found here . 1 2 query_pd = db_funcs . query_euv_images ( db_session = db_session , time_min = query_time_min , time_max = query_time_max ) methods_list = db_funcs . generate_methdf ( query_pd ) 1.) db_funcs.query_euv_images queries the database for EUV Images between time_min and time_max 2.) db_funcs.generate_methdf generates an empty pandas dataframe to later store method information columns hold associated method and variable information Apply Pre-Processing Corrections Limb-Brightening and Inter-Instrument Transformation Corrections are applied to images. Due to memory and storage issues, the rest of the mapping pipeline is applied to images based off date to limit the amount of data stored in memory. Dates for Processing This function creates an array of moving average dates which are looped through to apply corrections. 1 2 3 4 5 6 7 def get_dates ( query_time_min , query_time_max , map_freq ): \"\"\" function to create moving average dates based on hourly frequency of map creation \"\"\" map_frequency = int (( query_time_max - query_time_min ) . seconds / 3600 / map_freq ) moving_avg_centers = np . array ([ np . datetime64 ( str ( query_time_min )) + ii * np . timedelta64 ( map_freq , 'h' ) for ii in range ( map_frequency + 1 )]) return moving_avg_centers 1.) int((query_time_max - query_time_min).seconds / 3600 / map_freq) convert the map_freq integer to hours 2.) np.array(...) create moving average centers array based upon map frequency Query for Image Combos This function creates lists of combo queries for each instrument. It returns lists for LBC and IIT combo queries. 1 2 3 4 5 6 7 8 9 10 def get_inst_combos ( db_session , inst_list , time_min , time_max ): \"\"\" function to create instrument based lists of combo queries for image pre-processing \"\"\" for inst_index , instrument in enumerate ( inst_list ): lbc_combo = db_funcs . query_inst_combo ( db_session , time_min - datetime . timedelta ( days = 180 ), time_max + datetime . timedelta ( days = 180 ), meth_name = 'LBCC' , instrument = instrument ) iit_combo = db_funcs . query_inst_combo ( db_session , time_min - datetime . timedelta ( days = 180 ), time_max + datetime . timedelta ( days = 180 ), meth_name = 'IIT' , instrument = instrument ) lbc_combo_query [ inst_index ] = lbc_combo iit_combo_query [ inst_index ] = iit_combo return lbc_combo_query , iit_combo_query 1.) db_funcs.query_inst_combo queries database for image combinations for specific instrument within the 180 day range does this for both the LBC and IIT methods 2.) lbc_combo_query[inst_index] = lbc_combo add the combo query to the combo query list at the inst_index does this for both the LBC and IIT methods Apply Image Corrections This function applies the image pre-processing corrections to images of the center date in question. It returns a list of processed IIT Images and reference values for Coronal Hole Detection. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def apply_ipp ( db_session , center_date , query_pd , inst_list , hdf_data_dir , lbc_combo_query , iit_combo_query , methods_list , n_intensity_bins = 200 , R0 = 1.01 ): \"\"\" function to apply image pre-processing (limb-brightening, inter-instrument transformation) corrections to EUV images for creation of maps \"\"\" ref_alpha , ref_x = db_funcs . query_var_val ( db_session , meth_name = 'IIT' , date_obs = date_time , inst_combo_query = iit_combo_query [ sta_ind ]) for inst_ind , instrument in enumerate ( inst_list ): los_list [ inst_ind ], lbcc_image , mu_indices , use_ind , theoretic_query = lbcc_funcs . apply_lbc ( db_session , hdf_data_dir , lbc_combo_query [ inst_ind ], image_row = image_row , n_intensity_bins = n_intensity_bins , R0 = R0 ) lbcc_image , iit_list [ inst_ind ], use_indices [ inst_ind ], alpha , x = iit_funcs . apply_iit ( db_session , iit_combo_query [ inst_ind ], lbcc_image , use_ind , los_list [ inst_ind ], R0 = R0 ) ipp_method = { 'meth_name' : ( \"LBCC\" , \"IIT\" ), 'meth_description' :[ \"LBCC Theoretic Fit Method\" , \"IIT Fit Method\" ] , 'var_name' : ( \"LBCC\" , \"IIT\" ), 'var_description' : ( \" \" , \" \" )} methods_list [ inst_ind ] = methods_list [ inst_ind ] . append ( pd . DataFrame ( data = ipp_method ), sort = False ) return date_pd , los_list , iit_list , use_indices , methods_list , ref_alpha , ref_x 1.) db_funcs.query_var_val this is a database function to query variable values ref_alpha and ref_x are the IIT values for the STA Image at this date; these values are used to calculate threshold values for CH Detection 2.) lbcc_funcs.apply_lbc applies Limb-Brightening Correction to images and creates LBCCImage datatype 3.) iit_funcs.apply_iit applies Inter-Instrument Transformation Correction to images and creates IITImage datatype which is added to the iit_list 4.) methods_list[inst_ind].append add the LBC and IIT Correction methods to the methods dataframe Coronal Hole Detection This function applies the Fortran Coronal Hole Detection algorithm and returns a list of CHD Images for mapping. 1 2 3 4 5 6 7 8 9 10 11 def chd ( iit_list , los_list , use_indices , inst_list , thresh1 , thresh2 , ref_alpha , ref_x , nc , iters ): \"\"\" function to apply CHD algorithm and create list of CHD Images from a list of IIT Images \"\"\" for inst_ind , instrument in enumerate ( inst_list ): t1 = thresh1 * ref_alpha + ref_x t2 = thresh2 * ref_alpha + ref_x ezseg_output , iters_used = ezsegwrapper . ezseg ( np . log10 ( image_data ), use_chd , nx , ny , t1 , t2 , nc , iters ) chd_image_list [ inst_ind ] = datatypes . create_chd_image ( los_list [ inst_ind ], chd_result ) return chd_image_list 1.) t1 = thresh1 * ref_alpha + ref_x re-calculate threshold 1 and 2 values based off the EUVI-A IIT values 2.) ezsegwrapper.ezseg call the python wrapper function for the CH Detection algorithm 3.) datatypes.create_chd_image create CHD Image datatype and add to the CHD Image list for mapping Single Maps This function creates single instrument maps from both IIT Images and CHD Images. This mapping is done through linear interpolation onto a Carrington map. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def create_singles_maps ( inst_list , date_pd , iit_list , chd_image_list , methods_list , map_x = None , map_y = None , R0 = 1.01 ): \"\"\" function to map single instrument images to a Carrington map \"\"\" for inst_ind , instrument in enumerate ( inst_list ): map_list [ inst_ind ] = iit_list [ inst_ind ] . interp_to_map ( R0 = R0 , map_x = map_x , map_y = map_y , image_num = image_row . image_id ) chd_map_list [ inst_ind ] = chd_image_list [ inst_ind ] . interp_to_map ( R0 = R0 , map_x = map_x , map_y = map_y , image_num = image_row . image_id ) interp_method = { 'meth_name' : ( \"Im2Map_Lin_Interp_1\" ,), 'meth_description' :[ \"Use SciPy.RegularGridInterpolator() to linearly interpolate from an Image to a Map\" ] * 1 , 'var_name' : ( \"R0\" ,), 'var_description' : ( \"Solar radii\" ,), 'var_val' : ( R0 ,)} methods_list [ inst_ind ] = methods_list [ inst_ind ] . append ( pd . DataFrame ( data = interp_method ), sort = False ) map_list [ inst_ind ] . append_method_info ( methods_list [ inst_ind ]) chd_map_list [ inst_ind ] . append_method_info ( methods_list [ inst_ind ]) return map_list , chd_map_list , methods_list , image_info , map_info 1.) iit_list[inst_ind].interp_to_map, chd_image_list[inst_ind].interp_to_map interpolate IIT corrected, CHD image to Carrington map using linear interpolation 2.) methods_list[inst_ind].append append linear interpolation mapping method to the methods list 3.) map_list[inst_ind].append_method_info, chd_map_list[inst_ind].append_method_info append method information to the both the EUV and CHD map lists Combine Maps This function creates combined EUV and CHD maps from individual instruments maps. Then saves method, map parameter values, and maps to the database. Maps are combined using a Minimum Intensity Merge. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def create_combined_maps ( db_session , map_data_dir , map_list , chd_map_list , methods_list , image_info , map_info , mu_cut_over = None , del_mu = None , mu_cutoff = 0.0 ): \"\"\" function to create combined EUV and CHD maps and save to database with associated method information \"\"\" if del_mu is not None : euv_combined , chd_combined = combine_maps ( euv_maps , chd_maps , del_mu = del_mu , mu_cutoff = mu_cutoff ) combined_method = { 'meth_name' : ( \"Min-Int-Merge_1\" , \"Min-Int-Merge_1\" ), 'meth_description' :[ \"Minimum intensity merge: using del mu\" ] * 2 , 'var_name' : ( \"mu_cutoff\" , \"del_mu\" ), 'var_description' : ( \"lower mu cutoff value\" , \"max acceptable mu range\" ), 'var_val' : ( mu_cutoff , del_mu )} else : euv_combined , chd_combined = combine_maps ( euv_maps , chd_maps , mu_cut_over = mu_cut_over , mu_cutoff = mu_cutoff ) combined_method = { 'meth_name' : ( \"Min-Int-Merge_2\" , \"Min-Int-Merge_2\" ), 'meth_description' :[ \"Minimum intensity merge: based on Caplan et. al.\" ] * 2 , 'var_name' : ( \"mu_cutoff\" , \"mu_cut_over\" ), 'var_description' : ( \"lower mu cutoff value\" , \"mu cutoff value in areas of overlap\" ), 'var_val' : ( mu_cutoff , mu_cut_over )} euv_combined . append_method_info ( methods_list ) euv_combined . append_method_info ( pd . DataFrame ( data = combined_method )) euv_combined . append_image_info ( image_info ) euv_combined . append_map_info ( map_info ) chd_combined . append_method_info ( methods_list ) chd_combined . append_method_info ( pd . DataFrame ( data = combined_method )) chd_combined . append_image_info ( image_info ) chd_combined . append_map_info ( map_info ) Plotting . PlotMap ( euv_combined , nfig = \"EUV Combined map for: \" + str ( euv_combined . image_info . date_obs [ 0 ]), title = \"Minimum Intensity Merge Map \\n Date: \" + str ( euv_combined . image_info . date_obs [ 0 ])) Plotting . PlotMap ( euv_combined , nfig = \"EUV/CHD Combined map for: \" + str ( euv_combined . image_info . date_obs [ 0 ]), title = \"Minimum Intensity EUV/CHD Merge Map \\n Date: \" + str ( euv_combined . image_info . date_obs [ 0 ])) Plotting . PlotMap ( chd_combined , nfig = \"EUV/CHD Combined map for: \" + str ( chd_combined . image_info . date_obs [ 0 ]), title = \"Minimum Intensity EUV/CHD Merge Map \\n Date: \" + str ( chd_combined . image_info . date_obs [ 0 ]), map_type = 'CHD' ) euv_combined . write_to_file ( map_data_dir , map_type = 'synoptic_euv' , filename = None , db_session = db_session ) chd_combined . write_to_file ( map_data_dir , map_type = 'synoptic_chd' , filename = None , db_session = db_session ) return euv_combined , chd_combined 1.) combine_maps function that combines EUV and CHD maps using a minimum intensity merge there are currently two implemented methods for the minimum intensity merge depending on initial input parameters 2.) euv_combined.append_method_info, euv_combined.append_image_info, euv_combined.append_map_info append methods list and combination method information to the both the EUV and CHD combined maps appends image and map info to combined maps, used for database storage 3.) Plotting.PlotMap plot the combined EUV and CHD maps 4.) euv_combined.write_to_file, chd_combined.write_to_file PSI Map function that writes the map to file and saves to the database using function add_map_dbase_record generates filename for map based off base path and map type creates method combination of LBC, IIT, Interpolation, and Minimum Intensity Merge creates Image Combination associated with each method stores map variable values (R0, mu_cutoff, del_mu/mu_cut_over) in database Var Vals Map table stores map information and filename in EUV Maps table","title":"Mapping Pipeline"},{"location":"map/map/#mapping-pipeline","text":"After the calculation of the image pre-processing parameters (LBC and IIT), the mapping process undergoes five main steps through which EUV Images are converted to EUV and CHD Maps. 1.) Selecting Images 2.) Apply Pre-Processing Corrections a.) generate moving average dates b.) query for image combos associated with dates c.) apply LBC d.) apply IIT 3.) Coronal Hole Detection 4.) Create Single Instrument Maps 5.) Combine Maps and Save to the Database","title":"Mapping Pipeline"},{"location":"map/map/#mapping-pipeline-functions","text":"","title":"Mapping Pipeline Functions"},{"location":"map/map/#select-images","text":"The first step in map creation is querying the database for all EUV Images in the relevant time frame and creating a methods dataframe. These functions are database functions and the full code can be found here . 1 2 query_pd = db_funcs . query_euv_images ( db_session = db_session , time_min = query_time_min , time_max = query_time_max ) methods_list = db_funcs . generate_methdf ( query_pd ) 1.) db_funcs.query_euv_images queries the database for EUV Images between time_min and time_max 2.) db_funcs.generate_methdf generates an empty pandas dataframe to later store method information columns hold associated method and variable information","title":"Select Images"},{"location":"map/map/#apply-pre-processing-corrections","text":"Limb-Brightening and Inter-Instrument Transformation Corrections are applied to images. Due to memory and storage issues, the rest of the mapping pipeline is applied to images based off date to limit the amount of data stored in memory.","title":"Apply Pre-Processing Corrections"},{"location":"map/map/#dates-for-processing","text":"This function creates an array of moving average dates which are looped through to apply corrections. 1 2 3 4 5 6 7 def get_dates ( query_time_min , query_time_max , map_freq ): \"\"\" function to create moving average dates based on hourly frequency of map creation \"\"\" map_frequency = int (( query_time_max - query_time_min ) . seconds / 3600 / map_freq ) moving_avg_centers = np . array ([ np . datetime64 ( str ( query_time_min )) + ii * np . timedelta64 ( map_freq , 'h' ) for ii in range ( map_frequency + 1 )]) return moving_avg_centers 1.) int((query_time_max - query_time_min).seconds / 3600 / map_freq) convert the map_freq integer to hours 2.) np.array(...) create moving average centers array based upon map frequency","title":"Dates for Processing"},{"location":"map/map/#query-for-image-combos","text":"This function creates lists of combo queries for each instrument. It returns lists for LBC and IIT combo queries. 1 2 3 4 5 6 7 8 9 10 def get_inst_combos ( db_session , inst_list , time_min , time_max ): \"\"\" function to create instrument based lists of combo queries for image pre-processing \"\"\" for inst_index , instrument in enumerate ( inst_list ): lbc_combo = db_funcs . query_inst_combo ( db_session , time_min - datetime . timedelta ( days = 180 ), time_max + datetime . timedelta ( days = 180 ), meth_name = 'LBCC' , instrument = instrument ) iit_combo = db_funcs . query_inst_combo ( db_session , time_min - datetime . timedelta ( days = 180 ), time_max + datetime . timedelta ( days = 180 ), meth_name = 'IIT' , instrument = instrument ) lbc_combo_query [ inst_index ] = lbc_combo iit_combo_query [ inst_index ] = iit_combo return lbc_combo_query , iit_combo_query 1.) db_funcs.query_inst_combo queries database for image combinations for specific instrument within the 180 day range does this for both the LBC and IIT methods 2.) lbc_combo_query[inst_index] = lbc_combo add the combo query to the combo query list at the inst_index does this for both the LBC and IIT methods","title":"Query for Image Combos"},{"location":"map/map/#apply-image-corrections","text":"This function applies the image pre-processing corrections to images of the center date in question. It returns a list of processed IIT Images and reference values for Coronal Hole Detection. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def apply_ipp ( db_session , center_date , query_pd , inst_list , hdf_data_dir , lbc_combo_query , iit_combo_query , methods_list , n_intensity_bins = 200 , R0 = 1.01 ): \"\"\" function to apply image pre-processing (limb-brightening, inter-instrument transformation) corrections to EUV images for creation of maps \"\"\" ref_alpha , ref_x = db_funcs . query_var_val ( db_session , meth_name = 'IIT' , date_obs = date_time , inst_combo_query = iit_combo_query [ sta_ind ]) for inst_ind , instrument in enumerate ( inst_list ): los_list [ inst_ind ], lbcc_image , mu_indices , use_ind , theoretic_query = lbcc_funcs . apply_lbc ( db_session , hdf_data_dir , lbc_combo_query [ inst_ind ], image_row = image_row , n_intensity_bins = n_intensity_bins , R0 = R0 ) lbcc_image , iit_list [ inst_ind ], use_indices [ inst_ind ], alpha , x = iit_funcs . apply_iit ( db_session , iit_combo_query [ inst_ind ], lbcc_image , use_ind , los_list [ inst_ind ], R0 = R0 ) ipp_method = { 'meth_name' : ( \"LBCC\" , \"IIT\" ), 'meth_description' :[ \"LBCC Theoretic Fit Method\" , \"IIT Fit Method\" ] , 'var_name' : ( \"LBCC\" , \"IIT\" ), 'var_description' : ( \" \" , \" \" )} methods_list [ inst_ind ] = methods_list [ inst_ind ] . append ( pd . DataFrame ( data = ipp_method ), sort = False ) return date_pd , los_list , iit_list , use_indices , methods_list , ref_alpha , ref_x 1.) db_funcs.query_var_val this is a database function to query variable values ref_alpha and ref_x are the IIT values for the STA Image at this date; these values are used to calculate threshold values for CH Detection 2.) lbcc_funcs.apply_lbc applies Limb-Brightening Correction to images and creates LBCCImage datatype 3.) iit_funcs.apply_iit applies Inter-Instrument Transformation Correction to images and creates IITImage datatype which is added to the iit_list 4.) methods_list[inst_ind].append add the LBC and IIT Correction methods to the methods dataframe","title":"Apply Image Corrections"},{"location":"map/map/#coronal-hole-detection","text":"This function applies the Fortran Coronal Hole Detection algorithm and returns a list of CHD Images for mapping. 1 2 3 4 5 6 7 8 9 10 11 def chd ( iit_list , los_list , use_indices , inst_list , thresh1 , thresh2 , ref_alpha , ref_x , nc , iters ): \"\"\" function to apply CHD algorithm and create list of CHD Images from a list of IIT Images \"\"\" for inst_ind , instrument in enumerate ( inst_list ): t1 = thresh1 * ref_alpha + ref_x t2 = thresh2 * ref_alpha + ref_x ezseg_output , iters_used = ezsegwrapper . ezseg ( np . log10 ( image_data ), use_chd , nx , ny , t1 , t2 , nc , iters ) chd_image_list [ inst_ind ] = datatypes . create_chd_image ( los_list [ inst_ind ], chd_result ) return chd_image_list 1.) t1 = thresh1 * ref_alpha + ref_x re-calculate threshold 1 and 2 values based off the EUVI-A IIT values 2.) ezsegwrapper.ezseg call the python wrapper function for the CH Detection algorithm 3.) datatypes.create_chd_image create CHD Image datatype and add to the CHD Image list for mapping","title":"Coronal Hole Detection"},{"location":"map/map/#single-maps","text":"This function creates single instrument maps from both IIT Images and CHD Images. This mapping is done through linear interpolation onto a Carrington map. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def create_singles_maps ( inst_list , date_pd , iit_list , chd_image_list , methods_list , map_x = None , map_y = None , R0 = 1.01 ): \"\"\" function to map single instrument images to a Carrington map \"\"\" for inst_ind , instrument in enumerate ( inst_list ): map_list [ inst_ind ] = iit_list [ inst_ind ] . interp_to_map ( R0 = R0 , map_x = map_x , map_y = map_y , image_num = image_row . image_id ) chd_map_list [ inst_ind ] = chd_image_list [ inst_ind ] . interp_to_map ( R0 = R0 , map_x = map_x , map_y = map_y , image_num = image_row . image_id ) interp_method = { 'meth_name' : ( \"Im2Map_Lin_Interp_1\" ,), 'meth_description' :[ \"Use SciPy.RegularGridInterpolator() to linearly interpolate from an Image to a Map\" ] * 1 , 'var_name' : ( \"R0\" ,), 'var_description' : ( \"Solar radii\" ,), 'var_val' : ( R0 ,)} methods_list [ inst_ind ] = methods_list [ inst_ind ] . append ( pd . DataFrame ( data = interp_method ), sort = False ) map_list [ inst_ind ] . append_method_info ( methods_list [ inst_ind ]) chd_map_list [ inst_ind ] . append_method_info ( methods_list [ inst_ind ]) return map_list , chd_map_list , methods_list , image_info , map_info 1.) iit_list[inst_ind].interp_to_map, chd_image_list[inst_ind].interp_to_map interpolate IIT corrected, CHD image to Carrington map using linear interpolation 2.) methods_list[inst_ind].append append linear interpolation mapping method to the methods list 3.) map_list[inst_ind].append_method_info, chd_map_list[inst_ind].append_method_info append method information to the both the EUV and CHD map lists","title":"Single Maps"},{"location":"map/map/#combine-maps","text":"This function creates combined EUV and CHD maps from individual instruments maps. Then saves method, map parameter values, and maps to the database. Maps are combined using a Minimum Intensity Merge. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def create_combined_maps ( db_session , map_data_dir , map_list , chd_map_list , methods_list , image_info , map_info , mu_cut_over = None , del_mu = None , mu_cutoff = 0.0 ): \"\"\" function to create combined EUV and CHD maps and save to database with associated method information \"\"\" if del_mu is not None : euv_combined , chd_combined = combine_maps ( euv_maps , chd_maps , del_mu = del_mu , mu_cutoff = mu_cutoff ) combined_method = { 'meth_name' : ( \"Min-Int-Merge_1\" , \"Min-Int-Merge_1\" ), 'meth_description' :[ \"Minimum intensity merge: using del mu\" ] * 2 , 'var_name' : ( \"mu_cutoff\" , \"del_mu\" ), 'var_description' : ( \"lower mu cutoff value\" , \"max acceptable mu range\" ), 'var_val' : ( mu_cutoff , del_mu )} else : euv_combined , chd_combined = combine_maps ( euv_maps , chd_maps , mu_cut_over = mu_cut_over , mu_cutoff = mu_cutoff ) combined_method = { 'meth_name' : ( \"Min-Int-Merge_2\" , \"Min-Int-Merge_2\" ), 'meth_description' :[ \"Minimum intensity merge: based on Caplan et. al.\" ] * 2 , 'var_name' : ( \"mu_cutoff\" , \"mu_cut_over\" ), 'var_description' : ( \"lower mu cutoff value\" , \"mu cutoff value in areas of overlap\" ), 'var_val' : ( mu_cutoff , mu_cut_over )} euv_combined . append_method_info ( methods_list ) euv_combined . append_method_info ( pd . DataFrame ( data = combined_method )) euv_combined . append_image_info ( image_info ) euv_combined . append_map_info ( map_info ) chd_combined . append_method_info ( methods_list ) chd_combined . append_method_info ( pd . DataFrame ( data = combined_method )) chd_combined . append_image_info ( image_info ) chd_combined . append_map_info ( map_info ) Plotting . PlotMap ( euv_combined , nfig = \"EUV Combined map for: \" + str ( euv_combined . image_info . date_obs [ 0 ]), title = \"Minimum Intensity Merge Map \\n Date: \" + str ( euv_combined . image_info . date_obs [ 0 ])) Plotting . PlotMap ( euv_combined , nfig = \"EUV/CHD Combined map for: \" + str ( euv_combined . image_info . date_obs [ 0 ]), title = \"Minimum Intensity EUV/CHD Merge Map \\n Date: \" + str ( euv_combined . image_info . date_obs [ 0 ])) Plotting . PlotMap ( chd_combined , nfig = \"EUV/CHD Combined map for: \" + str ( chd_combined . image_info . date_obs [ 0 ]), title = \"Minimum Intensity EUV/CHD Merge Map \\n Date: \" + str ( chd_combined . image_info . date_obs [ 0 ]), map_type = 'CHD' ) euv_combined . write_to_file ( map_data_dir , map_type = 'synoptic_euv' , filename = None , db_session = db_session ) chd_combined . write_to_file ( map_data_dir , map_type = 'synoptic_chd' , filename = None , db_session = db_session ) return euv_combined , chd_combined 1.) combine_maps function that combines EUV and CHD maps using a minimum intensity merge there are currently two implemented methods for the minimum intensity merge depending on initial input parameters 2.) euv_combined.append_method_info, euv_combined.append_image_info, euv_combined.append_map_info append methods list and combination method information to the both the EUV and CHD combined maps appends image and map info to combined maps, used for database storage 3.) Plotting.PlotMap plot the combined EUV and CHD maps 4.) euv_combined.write_to_file, chd_combined.write_to_file PSI Map function that writes the map to file and saves to the database using function add_map_dbase_record generates filename for map based off base path and map type creates method combination of LBC, IIT, Interpolation, and Minimum Intensity Merge creates Image Combination associated with each method stores map variable values (R0, mu_cutoff, del_mu/mu_cut_over) in database Var Vals Map table stores map information and filename in EUV Maps table","title":"Combine Maps"},{"location":"ml/chd/","text":"Techniques to detect coronal holes using neural networks and K-means algorithm Where we are Fortran region growing algorithm CNN Supervised Learning Algorithm advantages: floating point GOAL: unsupervised algorithm CNN - Supervised Learning to Detect CH labeled image data based on Fortran two-threshold detection algorithm U-Net Convolutional Network built using tensorflow Example Images Test Data Training Loss K-Means Clustering unsupervised learning method centroid based clustering algorithm to generate CH/non-CH groups Advantages: unsupervised * no requirement of segmentation masks or labeling easy to create multiple cluster types (find AR as well) * can basically regenerate EUV map \u2014 use for prediction Concerns/Questions: * can this be trained on a lot of images?? * can be trained on a lot of data points but thus far that seems like training on one image K-Nearest Neighbor: * this would allow us to add pixels to a class -- perhaps track a CH over time Example Images Basic Clustering v. CHD Two Clusters Five Clusters CHD Clustering Two Clusters Five Clusters Predicted CHD Two Clusters Five Clusters","title":"Machine Learning Detection"},{"location":"ml/chd/#techniques-to-detect-coronal-holes-using-neural-networks-and-k-means-algorithm","text":"","title":"Techniques to detect coronal holes using neural networks and K-means algorithm"},{"location":"ml/chd/#where-we-are","text":"Fortran region growing algorithm CNN Supervised Learning Algorithm advantages: floating point GOAL: unsupervised algorithm","title":"Where we are"},{"location":"ml/chd/#cnn-supervised-learning-to-detect-ch","text":"labeled image data based on Fortran two-threshold detection algorithm U-Net Convolutional Network built using tensorflow","title":"CNN - Supervised Learning to Detect CH"},{"location":"ml/chd/#example-images","text":"","title":"Example Images"},{"location":"ml/chd/#test-data","text":"","title":"Test Data"},{"location":"ml/chd/#training-loss","text":"","title":"Training Loss"},{"location":"ml/chd/#k-means-clustering","text":"unsupervised learning method centroid based clustering algorithm to generate CH/non-CH groups Advantages: unsupervised * no requirement of segmentation masks or labeling easy to create multiple cluster types (find AR as well) * can basically regenerate EUV map \u2014 use for prediction Concerns/Questions: * can this be trained on a lot of images?? * can be trained on a lot of data points but thus far that seems like training on one image K-Nearest Neighbor: * this would allow us to add pixels to a class -- perhaps track a CH over time","title":"K-Means Clustering"},{"location":"ml/chd/#example-images_1","text":"","title":"Example Images"},{"location":"ml/chd/#basic-clustering-v-chd","text":"Two Clusters Five Clusters","title":"Basic Clustering v. CHD"},{"location":"ml/chd/#chd-clustering","text":"Two Clusters Five Clusters","title":"CHD Clustering"},{"location":"ml/chd/#predicted-chd","text":"Two Clusters Five Clusters","title":"Predicted CHD"},{"location":"ml/pred/","text":"CH-Net Future Frame Prediction Where we are autoencoder can generate decently good images doesn't hold any scientific purpose necessarily but was more to help me understand how the generator type functions work created a basic GAN to generate new CHD Maps able to create maps (still some issues with input data) but is very blurry GOAL: generate future EUV/CHD maps based on previous maps requires changing how the training step is set up to rather than look at random data, look at previous eight images https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2020SW002478 use supervised component: since we have a lot of data we can chekc the future prediction on the accurate image we have Autoencoder reduces signal noise generates images Reconstructed Images CHD Reconstruction","title":"CH-Net Future Frame Predictions"},{"location":"ml/pred/#ch-net-future-frame-prediction","text":"","title":"CH-Net Future Frame Prediction"},{"location":"ml/pred/#where-we-are","text":"autoencoder can generate decently good images doesn't hold any scientific purpose necessarily but was more to help me understand how the generator type functions work created a basic GAN to generate new CHD Maps able to create maps (still some issues with input data) but is very blurry GOAL: generate future EUV/CHD maps based on previous maps requires changing how the training step is set up to rather than look at random data, look at previous eight images https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2020SW002478 use supervised component: since we have a lot of data we can chekc the future prediction on the accurate image we have","title":"Where we are"},{"location":"ml/pred/#autoencoder","text":"reduces signal noise generates images Reconstructed Images CHD Reconstruction","title":"Autoencoder"},{"location":"ml/tracking_v1/","text":"Coronal Hole Tracking Algorithm Projections The current images are in latitude longitude coordinates with the poles placed at the top and bottom of the image. In this current projection, clustering coronal holes at the poles is challenging due to the distortion near the poles. In order to overcome this issue, we suggest a coordinate mapping placing the poles at the equator. This transformation can be done in \"projection.py\" module. Input: image dimensions (n_{\\theta}, n_{\\phi}) (n_{\\theta}, n_{\\phi}) , where \\theta \\in [0, \\pi] \\theta \\in [0, \\pi] and \\phi \\in [0, 2\\pi] \\phi \\in [0, 2\\pi] . Step 1: Convert to Cartesian coordinates (x,y,z). x = \\rho \\sin(\\theta)\\cos(\\phi) x = \\rho \\sin(\\theta)\\cos(\\phi) , y = \\rho \\sin(\\theta)\\sin(\\phi) y = \\rho \\sin(\\theta)\\sin(\\phi) , z = \\rho \\cos(\\theta) z = \\rho \\cos(\\theta) , where \\rho = 1 \\rho = 1 . Step 2: Rotate about the x axis with \\alpha = \\pi/2 \\alpha = \\pi/2 . Step 3: map back to spherical coordinates ( \\theta \\theta , \\phi \\phi ). \\theta = \\arccos(\\frac{z}{\\rho}) = \\arccos(\\sin(\\theta)\\sin(\\phi)) \\theta = \\arccos(\\frac{z}{\\rho}) = \\arccos(\\sin(\\theta)\\sin(\\phi)) , \\phi = \\arctan(\\frac{y}{x}) = \\arctan(\\frac{-\\cos(\\theta)}{\\sin(\\theta)\\cos(\\phi)}) \\phi = \\arctan(\\frac{y}{x}) = \\arctan(\\frac{-\\cos(\\theta)}{\\sin(\\theta)\\cos(\\phi)}) . Step 4: find contours - save coronal holes. Step 5: convert to Cartesian coordinates. Step 6: Rotate about the x axis with \\alpha = -\\pi/2 \\alpha = -\\pi/2 . Step 7: map back to spherical coordinates ( \\theta \\theta , \\phi \\phi ). Step 8: Save current coronal hole contour coordinates. Centroid Tracking Track coronal holes between frames based on the distance to the previous list of coronal hole centers. Features saved to object: centroid pixel location (x,y) centroid physical location (phi, theta) contour - pixel list. contour pixel area. contour physical area. bounding rectangle - straight. January 18th notes - updates the current tracking algortihm logic: input = coronal hole detection image. step 1: convert to grayscale. step 2: convert grayscaled image to polar projection. step 3: find contours + save ch id + color assigned + match to previous frame. step 4: map back to lat-lon projection as rbg image. step 5: save the new contour pixels location. step 6: compute contour features (center, area, bounding box). January 19th notes - TODO List. Add periodicity to the tracking algorithm otherwise a coronal hole center will be falsified. See figure below: Print/save all features to a log file. Match coronal holes based on previous 5 frames! Projection Analysis. In lat-lon projection: z < -1/2 or z > 1/2. x = \\rho \\sin(\\theta)\\cos(\\phi) x = \\rho \\sin(\\theta)\\cos(\\phi) , y = \\rho \\sin(\\theta)\\sin(\\phi) y = \\rho \\sin(\\theta)\\sin(\\phi) , z = \\rho \\cos(\\theta) z = \\rho \\cos(\\theta) , where \\rho = 1 \\rho = 1 . \\theta \\theta < \\pi/4 \\pi/4 or \\theta \\theta > 3\\pi/4 3\\pi/4 . In polar projection: z < -1/2 or z > 1/2. [1 0 0, 0 0 -1, 0 1 0] \\cdot \\cdot [ \\rho \\sin(\\theta)\\cos(\\phi) \\rho \\sin(\\theta)\\cos(\\phi) , \\rho \\sin(\\theta)\\sin(\\phi) \\rho \\sin(\\theta)\\sin(\\phi) , \\rho \\cos(\\theta) \\rho \\cos(\\theta) ] = [ \\rho \\sin(\\theta)\\cos(\\phi) \\rho \\sin(\\theta)\\cos(\\phi) , - \\rho \\cos(\\theta) \\rho \\cos(\\theta) , \\rho \\sin(\\theta)\\sin(\\phi) \\rho \\sin(\\theta)\\sin(\\phi) ]. Then, \\rho \\sin(\\theta)\\sin(\\phi) \\rho \\sin(\\theta)\\sin(\\phi) < -0.5 or \\rho \\sin(\\theta)\\sin(\\phi) \\rho \\sin(\\theta)\\sin(\\phi) > 0.5","title":"Polar Projection"},{"location":"ml/tracking_v1/#coronal-hole-tracking-algorithm","text":"","title":"Coronal Hole Tracking Algorithm"},{"location":"ml/tracking_v1/#projections","text":"The current images are in latitude longitude coordinates with the poles placed at the top and bottom of the image. In this current projection, clustering coronal holes at the poles is challenging due to the distortion near the poles. In order to overcome this issue, we suggest a coordinate mapping placing the poles at the equator. This transformation can be done in \"projection.py\" module. Input: image dimensions (n_{\\theta}, n_{\\phi}) (n_{\\theta}, n_{\\phi}) , where \\theta \\in [0, \\pi] \\theta \\in [0, \\pi] and \\phi \\in [0, 2\\pi] \\phi \\in [0, 2\\pi] . Step 1: Convert to Cartesian coordinates (x,y,z). x = \\rho \\sin(\\theta)\\cos(\\phi) x = \\rho \\sin(\\theta)\\cos(\\phi) , y = \\rho \\sin(\\theta)\\sin(\\phi) y = \\rho \\sin(\\theta)\\sin(\\phi) , z = \\rho \\cos(\\theta) z = \\rho \\cos(\\theta) , where \\rho = 1 \\rho = 1 . Step 2: Rotate about the x axis with \\alpha = \\pi/2 \\alpha = \\pi/2 . Step 3: map back to spherical coordinates ( \\theta \\theta , \\phi \\phi ). \\theta = \\arccos(\\frac{z}{\\rho}) = \\arccos(\\sin(\\theta)\\sin(\\phi)) \\theta = \\arccos(\\frac{z}{\\rho}) = \\arccos(\\sin(\\theta)\\sin(\\phi)) , \\phi = \\arctan(\\frac{y}{x}) = \\arctan(\\frac{-\\cos(\\theta)}{\\sin(\\theta)\\cos(\\phi)}) \\phi = \\arctan(\\frac{y}{x}) = \\arctan(\\frac{-\\cos(\\theta)}{\\sin(\\theta)\\cos(\\phi)}) . Step 4: find contours - save coronal holes. Step 5: convert to Cartesian coordinates. Step 6: Rotate about the x axis with \\alpha = -\\pi/2 \\alpha = -\\pi/2 . Step 7: map back to spherical coordinates ( \\theta \\theta , \\phi \\phi ). Step 8: Save current coronal hole contour coordinates.","title":"Projections"},{"location":"ml/tracking_v1/#centroid-tracking","text":"Track coronal holes between frames based on the distance to the previous list of coronal hole centers.","title":"Centroid Tracking"},{"location":"ml/tracking_v1/#features-saved-to-object","text":"centroid pixel location (x,y) centroid physical location (phi, theta) contour - pixel list. contour pixel area. contour physical area. bounding rectangle - straight.","title":"Features saved to object:"},{"location":"ml/tracking_v1/#january-18th-notes-updates","text":"the current tracking algortihm logic: input = coronal hole detection image. step 1: convert to grayscale. step 2: convert grayscaled image to polar projection. step 3: find contours + save ch id + color assigned + match to previous frame. step 4: map back to lat-lon projection as rbg image. step 5: save the new contour pixels location. step 6: compute contour features (center, area, bounding box).","title":"January 18th notes - updates"},{"location":"ml/tracking_v1/#january-19th-notes-todo-list","text":"Add periodicity to the tracking algorithm otherwise a coronal hole center will be falsified. See figure below: Print/save all features to a log file. Match coronal holes based on previous 5 frames!","title":"January 19th notes - TODO List."},{"location":"ml/tracking_v1/#projection-analysis","text":"In lat-lon projection: z < -1/2 or z > 1/2. x = \\rho \\sin(\\theta)\\cos(\\phi) x = \\rho \\sin(\\theta)\\cos(\\phi) , y = \\rho \\sin(\\theta)\\sin(\\phi) y = \\rho \\sin(\\theta)\\sin(\\phi) , z = \\rho \\cos(\\theta) z = \\rho \\cos(\\theta) , where \\rho = 1 \\rho = 1 . \\theta \\theta < \\pi/4 \\pi/4 or \\theta \\theta > 3\\pi/4 3\\pi/4 . In polar projection: z < -1/2 or z > 1/2. [1 0 0, 0 0 -1, 0 1 0] \\cdot \\cdot [ \\rho \\sin(\\theta)\\cos(\\phi) \\rho \\sin(\\theta)\\cos(\\phi) , \\rho \\sin(\\theta)\\sin(\\phi) \\rho \\sin(\\theta)\\sin(\\phi) , \\rho \\cos(\\theta) \\rho \\cos(\\theta) ] = [ \\rho \\sin(\\theta)\\cos(\\phi) \\rho \\sin(\\theta)\\cos(\\phi) , - \\rho \\cos(\\theta) \\rho \\cos(\\theta) , \\rho \\sin(\\theta)\\sin(\\phi) \\rho \\sin(\\theta)\\sin(\\phi) ]. Then, \\rho \\sin(\\theta)\\sin(\\phi) \\rho \\sin(\\theta)\\sin(\\phi) < -0.5 or \\rho \\sin(\\theta)\\sin(\\phi) \\rho \\sin(\\theta)\\sin(\\phi) > 0.5","title":"Projection Analysis."},{"location":"ml/tracking_v2/","text":"Dilation and Erosion - Proposed New Tracking Algorithm Introduction Mathematical Morphology Definitions Let A A be the original set and B B be the structuring element (also referred to as the kernel). Binary Dilation and Erosion Erosion : A \\ominus B = \\{z| B_{z} \\subseteq A\\} A \\ominus B = \\{z| B_{z} \\subseteq A\\} , s.t. A \\ominus B \\subseteq A A \\ominus B \\subseteq A . Dilation : A \\oplus B = \\{z| B_{z} \\cap A \\subseteq A\\} A \\oplus B = \\{z| B_{z} \\cap A \\subseteq A\\} , s.t. A \\oplus B \\supseteq A A \\oplus B \\supseteq A . For erosion, the structuring element center is marked True if there is a full overlap with structuring element, whereas with dilation, shifted elements have any overlap with original set A A (in our case A A is the input image). Opening : (A \\ominus B) \\oplus B (A \\ominus B) \\oplus B Closing : (A \\oplus B) \\ominus B (A \\oplus B) \\ominus B Opening, meaning, erode then dilate, and closing is of opposite order. Opening removes small objects from the foreground of an image, placing them in the background, while closing removes small holes in the foreground, changing small islands of background into foreground. Both are tools in image processing to remove noise or in our case - \"false positives\". Greyscale Dilation and Erosion Denoting an image by f(x) f(x) and the structuring function by B B , the grayscale dilation of f f by B B is given by Erosion : f \\ominus B = \\inf_{(k, l) \\in B}{f_{i+k, j+l}} f \\ominus B = \\inf_{(k, l) \\in B}{f_{i+k, j+l}} Dilation : f \\oplus B = \\sup_{(k, l) \\in B}{f_{i+k, j+l}} f \\oplus B = \\sup_{(k, l) \\in B}{f_{i+k, j+l}} Example code using Scipy library in Python. The Coronal Hole Tracking Algorithm Pseudocode Step 1 Input image in lat-lon projection - greyscaled or binary image (greyscale if we use Tamar's CNN results). Therefore, dilation and erosion are done using greyscale formula (inf/sup). Step 2 Apply a latitude weighted dilation. The structuring element depends on the pixel's latitude since in high and low latitude there is high distortion in lat-lon projection. The structuring element B B width depends on \\theta \\theta . The proposed function is as follows: w(\\theta) = \\begin{array}{cc} \\{ & \\begin{array}{cc} n_{p} & 0 \\leq \\theta \\leq \\alpha \\\\ \\frac{\\gamma}{\\sin(\\theta)} & \\alpha < \\theta < \\beta \\\\ n_{p} & \\beta \\leq \\theta \\leq \\pi \\end{array} \\} \\end{array} w(\\theta) = \\begin{array}{cc} \\{ & \\begin{array}{cc} n_{p} & 0 \\leq \\theta \\leq \\alpha \\\\ \\frac{\\gamma}{\\sin(\\theta)} & \\alpha < \\theta < \\beta \\\\ n_{p} & \\beta \\leq \\theta \\leq \\pi \\end{array} \\} \\end{array} where \\alpha = \\arcsin(\\frac{\\gamma}{n_{p}}) \\alpha = \\arcsin(\\frac{\\gamma}{n_{p}}) and \\beta = \\pi - \\alpha \\beta = \\pi - \\alpha (from symmetry). Step 3 Find contours using a binary threshold. Remove coronal holes that are too small. Force periodicity. Step 4 Classify coronal hole unique ID and color. Match to previous coronal holes by centroid location. Future research steps: find a way to classify merges of two coronal holes based on previous pixel location. Step 5 Compute coronal hole features: bounding box coordinates centroid (calculated in cartesian coordinates then map back to spherical). coronal hole area bounding box area etc...","title":"Dilation and Erosion"},{"location":"ml/tracking_v2/#dilation-and-erosion-proposed-new-tracking-algorithm","text":"","title":"Dilation and Erosion - Proposed New Tracking Algorithm"},{"location":"ml/tracking_v2/#introduction","text":"","title":"Introduction"},{"location":"ml/tracking_v2/#mathematical-morphology-definitions","text":"Let A A be the original set and B B be the structuring element (also referred to as the kernel).","title":"Mathematical Morphology Definitions"},{"location":"ml/tracking_v2/#binary-dilation-and-erosion","text":"Erosion : A \\ominus B = \\{z| B_{z} \\subseteq A\\} A \\ominus B = \\{z| B_{z} \\subseteq A\\} , s.t. A \\ominus B \\subseteq A A \\ominus B \\subseteq A . Dilation : A \\oplus B = \\{z| B_{z} \\cap A \\subseteq A\\} A \\oplus B = \\{z| B_{z} \\cap A \\subseteq A\\} , s.t. A \\oplus B \\supseteq A A \\oplus B \\supseteq A . For erosion, the structuring element center is marked True if there is a full overlap with structuring element, whereas with dilation, shifted elements have any overlap with original set A A (in our case A A is the input image). Opening : (A \\ominus B) \\oplus B (A \\ominus B) \\oplus B Closing : (A \\oplus B) \\ominus B (A \\oplus B) \\ominus B Opening, meaning, erode then dilate, and closing is of opposite order. Opening removes small objects from the foreground of an image, placing them in the background, while closing removes small holes in the foreground, changing small islands of background into foreground. Both are tools in image processing to remove noise or in our case - \"false positives\".","title":"Binary Dilation and Erosion"},{"location":"ml/tracking_v2/#greyscale-dilation-and-erosion","text":"Denoting an image by f(x) f(x) and the structuring function by B B , the grayscale dilation of f f by B B is given by Erosion : f \\ominus B = \\inf_{(k, l) \\in B}{f_{i+k, j+l}} f \\ominus B = \\inf_{(k, l) \\in B}{f_{i+k, j+l}} Dilation : f \\oplus B = \\sup_{(k, l) \\in B}{f_{i+k, j+l}} f \\oplus B = \\sup_{(k, l) \\in B}{f_{i+k, j+l}} Example code using Scipy library in Python.","title":"Greyscale Dilation and Erosion"},{"location":"ml/tracking_v2/#the-coronal-hole-tracking-algorithm-pseudocode","text":"","title":"The Coronal Hole Tracking Algorithm Pseudocode"},{"location":"ml/tracking_v2/#step-1","text":"Input image in lat-lon projection - greyscaled or binary image (greyscale if we use Tamar's CNN results). Therefore, dilation and erosion are done using greyscale formula (inf/sup).","title":"Step 1"},{"location":"ml/tracking_v2/#step-2","text":"Apply a latitude weighted dilation. The structuring element depends on the pixel's latitude since in high and low latitude there is high distortion in lat-lon projection. The structuring element B B width depends on \\theta \\theta . The proposed function is as follows: w(\\theta) = \\begin{array}{cc} \\{ & \\begin{array}{cc} n_{p} & 0 \\leq \\theta \\leq \\alpha \\\\ \\frac{\\gamma}{\\sin(\\theta)} & \\alpha < \\theta < \\beta \\\\ n_{p} & \\beta \\leq \\theta \\leq \\pi \\end{array} \\} \\end{array} w(\\theta) = \\begin{array}{cc} \\{ & \\begin{array}{cc} n_{p} & 0 \\leq \\theta \\leq \\alpha \\\\ \\frac{\\gamma}{\\sin(\\theta)} & \\alpha < \\theta < \\beta \\\\ n_{p} & \\beta \\leq \\theta \\leq \\pi \\end{array} \\} \\end{array} where \\alpha = \\arcsin(\\frac{\\gamma}{n_{p}}) \\alpha = \\arcsin(\\frac{\\gamma}{n_{p}}) and \\beta = \\pi - \\alpha \\beta = \\pi - \\alpha (from symmetry).","title":"Step 2"},{"location":"ml/tracking_v2/#step-3","text":"Find contours using a binary threshold. Remove coronal holes that are too small. Force periodicity.","title":"Step 3"},{"location":"ml/tracking_v2/#step-4","text":"Classify coronal hole unique ID and color. Match to previous coronal holes by centroid location. Future research steps: find a way to classify merges of two coronal holes based on previous pixel location.","title":"Step 4"},{"location":"ml/tracking_v2/#step-5","text":"Compute coronal hole features: bounding box coordinates centroid (calculated in cartesian coordinates then map back to spherical). coronal hole area bounding box area etc...","title":"Step 5"}]}